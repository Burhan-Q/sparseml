{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSNEB-3orJ9C"
   },
   "source": [
    "# **Text Classification: Sparse Transfer Learning with the CLI**\n",
    "\n",
    "In this example, you will fine-tune a 90% pruned BERT model onto some text classification datasets using SparseML's CLI.\n",
    "\n",
    "### **Sparse Transfer Learning Overview**\n",
    "\n",
    "Sparse Transfer Learning is very similiar to typical fine-tuning you are used to when training models. However, with Sparse Transfer Learning, we start the training process from a pre-sparsified checkpoint and maintain the sparsity structure while the fine tuning occurs. At the end, you will have a sparse model trained on your dataset, ready to be deployed with DeepSparse for GPU-class performance on CPUs!\n",
    "\n",
    "### **Pre-Sparsified BERT**\n",
    "SparseZoo, Neural Magic's open source repository of pre-sparsified models, contains a 90% pruned version of BERT, which has been sparsified on the upstream Wikipedia and BookCorpus datasets with the\n",
    "masked language modeling objective. [Check out the model card](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fobert-base%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned90-none). We will use this model as the starting point for the transfer learning process.\n",
    "\n",
    "\n",
    "***Let's dive in!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0WybTbssU0g"
   },
   "source": [
    "## **Installation**\n",
    "\n",
    "Install SparseML via `pip`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Nr5jM2zoqzuG"
   },
   "outputs": [],
   "source": [
    "%pip uninstall torch torchvision -y\n",
    "%pip install sparseml[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SlYUvD61ppy"
   },
   "source": [
    "If you are running on Google Colab, restart the runtime after this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "BT_fq1tk2O4Q"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.text_classification --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG_qKQcXsfgW"
   },
   "source": [
    "## **Sparse Transfer Learning with MNLI**\n",
    "\n",
    "SparseML's CLI enables you to kick-off training workflows with various utilities like dataset loading, checkpoint saving, \n",
    "metric reporting, and logging handled for you. All we have to do is pass a `model_name_or_path` (the starting checkpoint), a `task` (the GLUE task to train on), and a `recipe` (a YAML file specifying the sparsity related parameters) and we are up and running. The `recipes` is critical for instructing the training script how to modify the training process with sparsity related algorithms. For Sparse Transfer Learning, we will use a `recipe` that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jM1QJxiMu5T8"
   },
   "source": [
    "### **Run Transfer Learning**\n",
    "\n",
    "For MNLI, there is a pre-made transfer learning recipe available in [SparseZoo](https://sparsezoo.neuralmagic.com/models/nlp%2Ftext_classification%2Fobert-base%2Fpytorch%2Fhuggingface%2Fmnli%2Fpruned90_quant-none). As such, we kick off transfer learning with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz8o5CNlsNo4"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.train.text_classification \\\n",
    "  --task_name mnli \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none \\\n",
    "  --distill_teacher zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/base-none \\\n",
    "  --output_dir sparse_quantized_bert-text_classification_mnli \\\n",
    "  --do_train --do_eval --max_seq_length 128 --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 8 --per_device_eval_batch_size 32 --gradient_accumulation_steps 4 --preprocessing_num_workers 32 \\\n",
    "  --seed 5114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HFYMN3yq6fJ"
   },
   "source": [
    "Let's discuss the key arguments:\n",
    "- `--task_name mnli` instructs SparseML to download and fine-tune onto the MNLI dataset. You can pass any GLUE task as the task name and SparseML automatically downloads the dataset from the Hugging Face hub.\n",
    "\n",
    "- `--model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none` specifies the starting checkpoint for the fine tuning. Here, we passed a SparseZoo stub identifying the 90% pruned version of BERT trained with masked language modeling on the Wikipedia and BookCorpus datasets. SparseML downloads the checkpoint from the Zoo when the script starts.\n",
    "\n",
    "- `--recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none` specifies the recipe to be applied by SparseML. Here, we passed a SparseZoo stub identifying the transfer learning recipe for the MNLI dataset. SparseML downloads the recipe from the Zoo when the script starts. See below for the details of what this recipe looks like.\n",
    "\n",
    "- `--distill_teacher zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/base-none` is an optional argument that specifies a model to use for as a teacher to apply distillation during the training process. We passed a SparseZoo stub identifying a dense BERT model trained on MNLI. SparseML downloads the teacher from the Zoo when the script starts.\n",
    "\n",
    "The script downloads the starting checkpoint, the teacher model, and transfer learning recipe from SparseZoo as well as the MNLI \n",
    "dataset and trains the model for 12 epochs, converging to ~82.5% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJnGKuXnwhWH"
   },
   "source": [
    "#### **Transfer Learning Recipe**\n",
    "\n",
    "Here's what the transfer learning recipe for the MNLI dataset looks like.\n",
    "\n",
    "The \"Modifiers\" are the important items that encode how SparseML should modify the training process for Sparse Transfer Learning:\n",
    "- `ConstantPruningModifier` tells SparseML to pin weights at 0 over all epochs, maintaining the sparsity structure of the network\n",
    "- `QuantizationModifier` tells SparseML to quanitze the weights with quantization aware training over the last 5 epochs\n",
    "- `DistillationModifier` tells SparseML how to apply distillation to the model, including the layer and some hyperparameters\n",
    "\n",
    "SparseML parses the modifiers and updates the training process to implement the algorithms and hyperparameters specified in the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5Z_a3HOwrpu"
   },
   "outputs": [],
   "source": [
    "from sparsezoo import Model\n",
    "transfer_stub = \"zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none\"\n",
    "download_dir = \"./transfer_recipe-mnli\"\n",
    "zoo_model = Model(transfer_stub, download_path=download_dir)\n",
    "recipe_path = zoo_model.recipes.default.path\n",
    "print(recipe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oeu1PvyYxR0V"
   },
   "outputs": [],
   "source": [
    "%cat ./transfer_recipe-mnli/recipe/recipe_original.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFbppO99xGeT"
   },
   "source": [
    "### **Export to ONNX**\n",
    "\n",
    "Once you have trained your model, export to ONNX in order to deploy with DeepSparse. The artifacts of the training process \n",
    "are saved to your local filesystem. \n",
    "\n",
    "Run the following to convert your PyTorch checkpoint to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHKEUvPOx-su"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.export_onnx \\\n",
    "  --model_path sparse_quantized_bert-text_classification_mnli \\\n",
    "  --task text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3JKl3xIyDIe"
   },
   "source": [
    "The script above creates a `deployment` folder in your local directory, which has all of the files needed for deployment with DeepSparse including the `model.onnx`, `config.json`, and `tokenizer.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MKZlYCD878I"
   },
   "source": [
    "## **Sparse Transfer Learning with QQP**\n",
    "\n",
    "SparseML's CLI enables you to kick-off training workflows with various utilities like dataset loading, checkpoint saving, \n",
    "metric reporting, and logging handled for you. All we have to do is pass a `model_name_or_path` (the starting checkpoint), a `task` (the GLUE task to train on), and a `recipe` (a YAML file specifying the sparsity related parameters) and we are up and running. The `recipes` is critical for instructing the training script how to modify the training process with sparsity related algorithms. For Sparse Transfer Learning, we will use a `recipe` that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evYNtvbG878K"
   },
   "source": [
    "### **Run Transfer Learning**\n",
    "\n",
    "For QQP, there is a pre-made transfer learning recipe available in [SparseZoo](https://sparsezoo.neuralmagic.com/models/nlp%2Ftext_classification%2Fobert-base%2Fpytorch%2Fhuggingface%2Fqqp%2Fpruned90_quant-none). As such, we kick off transfer learning with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ml32ao9b878K"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.train.text_classification \\\n",
    "  --task_name qqp \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none \\\n",
    "  --distill_teacher zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/base-none \\\n",
    "  --output_dir obert_base_pruned90_quant_qqp \\\n",
    "  --do_train --do_eval --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 32 \\\n",
    "  --max_seq_length 128 \\\n",
    "  --seed 10194"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwVLySAf878K"
   },
   "source": [
    "Let's discuss the key arguments:\n",
    "- `--task_name qqp` instructs SparseML to download and fine-tune onto the QQP dataset. You can pass any GLUE task as the task name and SparseML automatically downloads the dataset from the Hugging Face hub.\n",
    "\n",
    "- `--model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none` specifies the starting checkpoint for the fine tuning. Here, we passed a SparseZoo stub identifying the 90% pruned version of BERT trained with masked language modeling on the Wikipedia and BookCorpus datasets. SparseML downloads the checkpoint from the Zoo when the script starts.\n",
    "\n",
    "- `--recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none` specifies the recipe to be applied by SparseML. Here, we passed a SparseZoo stub identifying the transfer learning recipe for the QQP dataset. SparseML downloads the recipe from the Zoo when the script starts. See below for the details of what this recipe looks like.\n",
    "\n",
    "- `--distill_teacher zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/base-none` is an optional argument that specifies a model to use for as a teacher to apply distillation during the training process. We passed a SparseZoo stub identifying a dense BERT model trained on QQP. SparseML downloads the teacher from the Zoo when the script starts.\n",
    "\n",
    "The script downloads the starting checkpoint, the teacher model, and transfer learning recipe from SparseZoo as well as the QQP \n",
    "dataset and trains the model for 3 epochs, converging to ~91% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8fcas2k878K"
   },
   "source": [
    "#### **Transfer Learning Recipe**\n",
    "\n",
    "Here's what the transfer learning recipe for the QQP dataset looks like.\n",
    "\n",
    "The \"Modifiers\" are the important items that encode how SparseML should modify the training process for Sparse Transfer Learning:\n",
    "- `ConstantPruningModifier` tells SparseML to pin weights at 0 over all epochs, maintaining the sparsity structure of the network\n",
    "- `QuantizationModifier` tells SparseML to quanitze the weights with quantization aware training over the last 5 epochs\n",
    "- `DistillationModifier` tells SparseML how to apply distillation to the model, including the layer and some hyperparameters\n",
    "\n",
    "SparseML parses the modifiers and updates the training process to implement the algorithms and hyperparameters specified in the recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-iYXqXc878K"
   },
   "outputs": [],
   "source": [
    "from sparsezoo import Model\n",
    "transfer_stub = \"zoo:nlp/text_classification/obert-base/pytorch/huggingface/qqp/pruned90_quant-none\"\n",
    "download_dir = \"./transfer_recipe-qqp\"\n",
    "zoo_model = Model(transfer_stub, download_path=download_dir)\n",
    "recipe_path = zoo_model.recipes.default.path\n",
    "print(recipe_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnyFTkeS878L"
   },
   "outputs": [],
   "source": [
    "%cat ./transfer_recipe-qqp/recipe/recipe_original.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dptcuSSU878L"
   },
   "source": [
    "### **Export to ONNX**\n",
    "\n",
    "Once you have trained your model, export to ONNX in order to deploy with DeepSparse. The artifacts of the training process \n",
    "are saved to your local filesystem. \n",
    "\n",
    "Run the following to convert your PyTorch checkpoint to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOxZQVJR878L"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.export_onnx \\\n",
    "  --model_path obert_base_pruned90_quant_qqp \\\n",
    "  --task text_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD4zFVYF878L"
   },
   "source": [
    "The script above creates a `deployment` folder in your local directory, which has all of the files needed for deployment with DeepSparse including the `model.onnx`, `config.json`, and `tokenizer.json` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCE4HKWvyWRu"
   },
   "source": [
    "## **Sparse Transfer Learning with a Custom Dataset: Single Input Classification**\n",
    "\n",
    "Beyond the built-in GLUE tasks, we can also use datasets for single input multi-class classification problems. The datasets can either be passed as Hugging Face hub model identifiers or via local CSV/JSON files.\n",
    "\n",
    "Let's try to transfer onto the [TweetEval Emotion Dataset](https://huggingface.co/datasets/tweet_eval). This dataset \n",
    "contains single sentences with 4 labels representing the emotion of the tweet (`0=anger, 1=joy, 2=optimism, 3=sadness`.\n",
    "\n",
    "For simplicity, we will perform the fine-tuning without distillation. Although the transfer learning recipe contains distillation modifiers, we can turn them off by setting `--distill_teacher disable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5ab0yh6yspE"
   },
   "source": [
    "### **Using a Hugging Face Dataset**\n",
    "\n",
    "Let's walk through how to pass a Hugging Face dataset identifier to the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q41afk_22DcU"
   },
   "source": [
    "#### **Inspecting TweetEval Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfKp5RaMybmt"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "emotion = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "print(emotion)\n",
    "pprint(emotion[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdpvTT654LAy"
   },
   "source": [
    "#### **Kick off Training**\n",
    "\n",
    "To configure the training script to use the emotion dataset, we replaced the `--task_name` argument with `--dataset_name tweet_eval --dataset_config_name emotion --input_column_names text --label_column_name label`. Since the TweetEval dataset contains multiple subsets (e.g. there is a subset\n",
    "that classifies text into an emoji), we pass the `--dataset_config_name` to specify the subset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPHR_b0AADIG"
   },
   "source": [
    "Run the following to kick of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMxgkhMRz-x3"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.train.text_classification \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none \\\n",
    "  --recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}' \\\n",
    "  --distill_teacher disable \\\n",
    "  --dataset_name tweet_eval --dataset_config_name emotion \\\n",
    "  --input_column_names \"text\" --label_column_name \"label\" \\\n",
    "  --output_dir sparse_quantized_bert-text_classification_tweet_eval_emotion \\\n",
    "  --do_train --do_eval --max_seq_length 128 --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 8 --per_device_eval_batch_size 32 --gradient_accumulation_steps 4 --preprocessing_num_workers 32 \\\n",
    "  --seed 5114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NfB9uhi0CBX"
   },
   "source": [
    "You will notice that we used the same recipe as we did in the SST2 case (identified by the SparseZoo stub `zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none`). \n",
    "\n",
    "Since the TweetEval Emotion dataset is a single sentence multi-class classification problem, we used the transfer learning recipe from the sentiment analysis task (a single sentence binary classification problem) as the starting point.\n",
    "\n",
    "\n",
    "\n",
    "To update a recipe, you can download the YAML file from SparseZoo, make updates to the YAML directly, and pass the local path to SparseML. In this case, we used `--recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}'` to modify a recipe on the fly, updating to only run for 11 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79KVF5TB11xD"
   },
   "source": [
    "### **Using Local CSV/JSON Files**\n",
    "\n",
    "Let's walk through how to pass a CSV/JSON dataset to the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZF7myQc23uc"
   },
   "source": [
    "#### **Save Dataset as a CSV File**\n",
    "\n",
    "For this example, we use Hugging Face `datasets` to create a CSV file for Tweet Eval dataset that can be passed to SparseML's CLI but you can use any framework you want to create the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWsAD8b_2tQ2"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "emotion = load_dataset(\"tweet_eval\", \"emotion\")\n",
    "print(emotion)\n",
    "pprint(emotion[\"train\"][0])\n",
    "emotion[\"train\"].to_csv(\"./emotion-train.csv\")\n",
    "emotion[\"validation\"].to_csv(\"./emotion-validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTwUYc913uDB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5O2haE73_my"
   },
   "source": [
    "We can see that the data is a CSV file with text and label as the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2kMBxV134ii"
   },
   "outputs": [],
   "source": [
    "!head ./emotion-train.csv --lines=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JdeJqQC3-SE"
   },
   "outputs": [],
   "source": [
    "!head ./emotion-validation.csv --lines=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D70YemqF3_D2"
   },
   "source": [
    "#### **Kick off Training**\n",
    "\n",
    "To use the local files with the CLI, pass `--train_file ./emotion-train.csv --validation_file ./emotion-validation.csv  --input_column_names text --label_column_name label`.\n",
    "\n",
    "Run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqDbMuS74YaP"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.text_classification \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none \\\n",
    "  --recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}' \\\n",
    "  --distill_teacher disable \\\n",
    "  --train_file ./emotion-train.csv --validation_file ./emotion-validation.csv \\\n",
    "  --input_column_names \"text\" --label_column_name \"label\" \\\n",
    "  --output_dir sparse_quantized_bert-text_classification_tweet_eval_emotion-csv \\\n",
    "  --do_train --do_eval --max_seq_length 128 --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 8 --per_device_eval_batch_size 32 --gradient_accumulation_steps 4 --preprocessing_num_workers 32 \\\n",
    "  --seed 5114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD8n5jsHBWWk"
   },
   "source": [
    "## **Sparse Transfer Learning with a Custom Dataset: Multiple Input Classification**\n",
    "\n",
    "Beyond the built-in GLUE tasks, we can also use datasets for multiple input multi-class classification problems. The datasets can either be passed as Hugging Face hub model identifiers or via local CSV/JSON files.\n",
    "\n",
    "Let's try to transfer onto the [SICK (Sentences Involving Compositional Knowldedge) Dataset](https://huggingface.co/datasets/sick),\n",
    "which includes 10,000 pairs of sentences with entailment relationships (`0=entailment, 1=neural, 2=contradiction`).\n",
    "\n",
    "For simplicity, we will perform the fine-tuning without distillation. Although the transfer learning recipe contains distillation modifiers, we can turn them off by setting `--distill_teacher disable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NLhnHUzBWWk"
   },
   "source": [
    "### **Using a Hugging Face Dataset**\n",
    "\n",
    "Let's walk through how to pass a Hugging Face dataset identifier to the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgicc719BWWk"
   },
   "source": [
    "#### **Inspecting SICK Dataset**\n",
    "\n",
    "`Sentence_A` and `Sentence_B` are the input columns. `label` is the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2c7Fu7bBWWk"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "\n",
    "sick = load_dataset(\"sick\")\n",
    "print(sick)\n",
    "pprint(sick[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S3mA4qNBWWk"
   },
   "source": [
    "#### **Kick off Training**\n",
    "\n",
    "To configure the training script to use the SICK dataset, we replaced the `--task_name` argument with `--dataset_name sick --input_column_names 'sentence_A,sentence_B' --label_column_name label`.\n",
    "\n",
    "\n",
    "Run the following to kick of the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw-RwNbNBWWl"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.text_classification \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none \\\n",
    "  --distill_teacher disable \\\n",
    "  --dataset_name sick --input_column_names 'sentence_A,sentence_B' --label_column_name 'label' \\\n",
    "  --output_dir sparse_quantized_bert-text_classification_sick \\\n",
    "  --do_train --do_eval --max_seq_length 128 --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 8 --per_device_eval_batch_size 32 --gradient_accumulation_steps 4 --preprocessing_num_workers 32 \\\n",
    "  --seed 5114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYb01VaaBWWl"
   },
   "source": [
    "You will notice that we used the same recipe as we did in the MNLI case (identified by the SparseZoo stub `zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none`). \n",
    "\n",
    "Since the MNLI dataset is a multi sentence multi-class classification problem (similiarly, it is an entailment problem), we used the transfer learning recipe from the sentiment analysis task (a single sentence binary classification problem) as the starting point.\n",
    "\n",
    "To update a recipe, you can download the YAML file from SparseZoo, make updates to the YAML directly, and pass the local path to SparseML. Alternative, you can use `--recipe_args` to update on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AQ9KT8NBWWl"
   },
   "source": [
    "### **Using Local CSV/JSON Files**\n",
    "\n",
    "Let's walk through how to pass a CSV/JSON dataset to the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hXT1k5HBWWl"
   },
   "source": [
    "#### **Save Dataset as a CSV File**\n",
    "\n",
    "For this example, we use Hugging Face `datasets` to create a CSV file for SICK dataset that can be passed to SparseML's CLI but you can use any framework you want to create the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-5RKR4nBWWl"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "sick = load_dataset(\"sick\")\n",
    "print(sick)\n",
    "print(sick[\"train\"][0])\n",
    "sick[\"train\"].to_csv(\"./sick-train.csv\")\n",
    "sick[\"validation\"].to_csv(\"./sick-validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMLeALtQBWWl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Db2uVqWOBWWl"
   },
   "source": [
    "We can see that the data is a CSV file with `sentence_A` and `sentence_B` as the input columns and `label` as the label column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmVitGcrBWWl"
   },
   "outputs": [],
   "source": [
    "!head ./sick-train.csv --lines=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLGJ1t6lBWWl"
   },
   "outputs": [],
   "source": [
    "!head ./sick-validation.csv --lines=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4SWCS6ABWWl"
   },
   "source": [
    "#### **Kick off Training**\n",
    "\n",
    "To use the local files with the CLI, pass `--train_file ./sick-train.csv --validation_file ./sick-validation.csv  --input_column_names 'sentence_A,sentence_B' --label_column_name label`.\n",
    "\n",
    "Run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lwd3Mx5VBWWl"
   },
   "outputs": [],
   "source": [
    "!sparseml.transformers.train.text_classification \\\n",
    "  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n",
    "  --recipe zoo:nlp/text_classification/obert-base/pytorch/huggingface/mnli/pruned90_quant-none \\\n",
    "  --distill_teacher disable \\\n",
    "  --train_file ./sick-train.csv --validation_file ./sick-validation.csv --input_column_names 'sentence_A,sentence_B' --label_column_name 'label' \\\n",
    "  --output_dir sparse_quantized_bert-text_classification_sick-csv \\\n",
    "  --do_train --do_eval --max_seq_length 128 --evaluation_strategy epoch --logging_steps 1000 --save_steps 1000 \\\n",
    "  --per_device_train_batch_size 8 --per_device_eval_batch_size 32 --gradient_accumulation_steps 4 --preprocessing_num_workers 32 \\\n",
    "  --seed 5114"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOBmRO5NF6M1oD16Su3xyWt",
   "provenance": [
    {
     "file_id": "1-s8e-LoRXMtdptr7BzuGn7OecJUTS62u",
     "timestamp": 1677468586176
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
