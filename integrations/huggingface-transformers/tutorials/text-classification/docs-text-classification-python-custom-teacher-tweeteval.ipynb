{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zawa0sifXr2wIl9tbF7ySJ7xYY0dtTzI","timestamp":1677345946788}],"toc_visible":true,"authorship_tag":"ABX9TyNUQDzGJY5h/6OPOEWVMurw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Text Classification: Sparse Transfer Learning with the Python API**\n","\n","In this example, you will fine-tune a 90% pruned BERT model onto the TweetEval Emotions dataset using SparseML's Hugging Face Integration.\n","\n","### **Sparse Transfer Learning Overview**\n","\n","Sparse Transfer Learning is very similiar to typical fine-tuning you are used to when training models. However, with Sparse Transfer Learning, we start the training process from a pre-sparsified checkpoint and maintain the sparsity structure while the fine tuning occurs. At the end, you will have a sparse model trained on your dataset, ready to be deployed with DeepSparse for GPU-class performance on CPUs!\n","\n","### **Pre-Sparsified BERT**\n","SparseZoo, Neural Magic's open source repository of pre-sparsified models, contains a 90% pruned version of BERT, which has been sparsified on the upstream Wikipedia and BookCorpus datasets with the\n","masked language modeling objective. [Check out the model card](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fobert-base%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned90-none). We will use this model as the starting point for the transfer learning process.\n","\n","\n","***Let's dive in!***"],"metadata":{"id":"kSNEB-3orJ9C"}},{"cell_type":"markdown","source":["## **Installation**\n","\n","Install SparseML via `pip`.\n","\n"],"metadata":{"id":"Y0WybTbssU0g"}},{"cell_type":"code","source":["%pip uninstall torch -y\n","%pip install sparseml[torch]"],"metadata":{"collapsed":true,"id":"NjynYRLMIONF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you are running on Google Colab, restart the runtime after this step."],"metadata":{"id":"_jY0SKdXFGO3"}},{"cell_type":"code","source":["import sparseml\n","from sparsezoo import Model\n","from sparseml.transformers.utils import SparseAutoModel\n","from sparseml.transformers.sparsification import Trainer, TrainingArguments\n","import numpy as np\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    AutoConfig, \n","    AutoTokenizer, \n","    EvalPrediction, \n","    default_data_collator\n",")\n","from datasets import load_dataset, load_metric"],"metadata":{"id":"XXj0S5Jdq2M-","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 1: Load a Dataset**\n","\n","SparseML is integrated with Hugging Face, so we can use the `datasets` class to load datasets from the Hugging Face hub or from local files. \n","\n","[TweetEval Emotions](https://huggingface.co/datasets/tweet_eval/viewer/emotion/train)"],"metadata":{"id":"2rS2Q5kxFcW3"}},{"cell_type":"code","source":["# load_dataset from HF hub\n","dataset = load_dataset(\"tweet_eval\", \"emotion\")\n","\n","# alternatively, load from a csv\n","dataset[\"train\"].to_csv(\"tweet_eval_emotion-train.csv\")\n","dataset[\"validation\"].to_csv(\"tweet_eval_emotion-validation.csv\")\n","\n","data_files = {\n","  \"train\": \"tweet_eval_emotion-train.csv\",\n","  \"validation\": \"tweet_eval_emotion-validation.csv\"\n","}\n","dataset_from_json = load_dataset(\"csv\", data_files=data_files)"],"metadata":{"id":"CkvbT1i9p87z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head tweet_eval_emotion-train.csv --lines=5"],"metadata":{"id":"nGfpSK3CI15n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset_from_json)"],"metadata":{"id":"fnKtfIU6IzOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# configs for below\n","INPUT_COL_1 = \"text\"\n","INPUT_COL_2 = None\n","LABEL_COL = \"label\"\n","NUM_LABELS = len(dataset_from_json[\"train\"].unique(LABEL_COL))"],"metadata":{"id":"7ti52fgQqdSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 2: Setup Evaluation Metric**\n","\n","Tweet Eval emotion is a simple multi-class classification problem (we are predicting one of 4 emotions). We will use the `accuracy` function as the evaluation metric. We can use the native Hugging Face `compute_metrics` function (which will be passed to the `Trainer` class below)."],"metadata":{"id":"3BfXUE9HHFoq"}},{"cell_type":"code","source":["metric = load_metric(\"accuracy\")\n","\n","# setup metrics function\n","def compute_metrics(p: EvalPrediction):\n","  preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","  preds = np.argmax(preds, axis=1)\n","  result = metric.compute(predictions=preds, references=p.label_ids)\n","  if len(result) > 1:\n","      result[\"combined_score\"] = np.mean(list(result.values())).item()\n","  return result\n","\n","print(dataset[\"train\"])"],"metadata":{"id":"tPIVsdAzIgf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 3: Train the Teacher**\n","\n","To support the sparse transfer learning process, we will first train a dense teacher model from scratch - which we can then distill during the sparse transfer learning process.\n","\n","Although we will be fine-tuning BERT, we can use a different model as the teacher. In this case, we will use `roberta-base` as the teacher, training with native Hugging Face objects."],"metadata":{"id":"zERYC_SL4J2A"}},{"cell_type":"code","source":["TEACHER = \"roberta-base\"\n","teacher_config = AutoConfig.from_pretrained(TEACHER, num_labels=NUM_LABELS)\n","teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER)\n","teacher = AutoModelForSequenceClassification.from_pretrained(TEACHER, config=teacher_config)"],"metadata":{"id":"XqaExx7Hp5Zl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Teacher Training: Tokenize the Dataset**\n","\n","Run the RoBERTa tokenizer on the dataset."],"metadata":{"id":"Qcb_MA2-JuUZ"}},{"cell_type":"code","source":["MAX_LEN = 128\n","def teacher_preprocess_fn(examples):\n","  args = None\n","  if INPUT_COL_2 is None:\n","    args = (examples[INPUT_COL_1], )\n","  else:\n","    args = (examples[INPUT_COL_1], examples[INPUT_COL_2])\n","  result = teacher_tokenizer(*args, \n","                   padding=\"max_length\", \n","                   max_length=min(teacher_tokenizer.model_max_length, MAX_LEN), \n","                   truncation=True)\n","  return result\n","\n","# tokenize the dataset\n","teacher_tokenized_dataset = dataset_from_json.map(\n","    teacher_preprocess_fn,\n","    batched=True,\n","    desc=\"Running teacher tokenizer on dataset\"\n",")"],"metadata":{"id":"OT0ogPOCrPaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Teacher Training: Fine-Tune the Teacher**\n","\n","We use the native Hugging Face `Trainer` (which we import as `HFTrainer`) to train the model. Check out the [Hugging Face documentation](https://huggingface.co/docs/transformers/main_classes/trainer) for more details on the `Trainer` as needed.\n"],"metadata":{"id":"pTb5lf-kJ_st"}},{"cell_type":"code","source":["from transformers import Trainer as HFTrainer\n","from transformers import TrainingArguments as HFTrainingArguments\n","\n","# setup trainer arguments\n","teacher_training_args = HFTrainingArguments(\n","    output_dir=\"./teacher_training\",\n","    do_train=True,\n","    do_eval=True,\n","    num_train_epochs=6.0,\n","    learning_rate=5e-6,\n","    lr_scheduler_type=\"linear\",\n","    logging_strategy=\"epoch\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=1,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32)\n","\n","# initialize trainer\n","teacher_trainer = HFTrainer(\n","    model=teacher,\n","    args=teacher_training_args,\n","    train_dataset=teacher_tokenized_dataset[\"train\"],\n","    eval_dataset=teacher_tokenized_dataset[\"validation\"],\n","    tokenizer=teacher_tokenizer,\n","    data_collator=default_data_collator,\n","    compute_metrics=compute_metrics)\n","\n","# run training\n","%rm -rf ./teacher_training\n","teacher_trainer.train(resume_from_checkpoint=False)"],"metadata":{"id":"oVrGtQwbrTlO","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 4: Sparse Transfer Learning**\n","\n","Now that we have the teacher trained, we can sparse transfer learn from the pre-sparsified version of BERT with distillation support. "],"metadata":{"id":"8bcO7Kclv5uX"}},{"cell_type":"markdown","source":["First, we need to select a sparse checkpoint to begin the training process. In this case, we are fine-tuning a 90% pruned version of BERT onto the TweetEval Emotion dataset. This model is available in SparseZoo, identified by the following stub:\n","```\n","zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none\n","```\n","\n","We also need to create/select a sparsification recipe for usage in the training process. Recipes are YAML files that encode the sparsity related algorithms and parameters to be applied by SparseML. For Sparse Transfer Learning, we use a recipe that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. In SparseZoo, there is a transfer recipe which was used to fine-tune BERT onto the SST2 task (which is also a single sequence classification problem). Since the TweetEval Emotion task is a similiar problem to SST2, we will use the SST2 recipe, which is identified by the following stub:\n","\n","```\n","zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\n","```"],"metadata":{"id":"1GEhYi53HoAH"}},{"cell_type":"markdown","source":["Use the `sparsezoo` python client to download the models and recipe using their SparseZoo stubs."],"metadata":{"id":"U_iyuuB4Wq7N"}},{"cell_type":"code","source":["# downloads 90% pruned upstream BERT trained on MLM objective\n","model_stub = \"zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none\" \n","model_path = Model(model_stub, download_path=\"./model\").training.path\n","\n","# downloads transfer recipe for SST2 (pruned90_quant)\n","transfer_stub = \"zoo:nlp/sentiment_analysis/obert-base/pytorch/huggingface/sst2/pruned90_quant-none\"\n","recipe_path = Model(transfer_stub, download_path=\"./transfer_recipe\").recipes.default.path"],"metadata":{"id":"Ykg8fEN2Q5o_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the upstream model (trained on Wikipedia BookCorpus) and  configuration files have been downloaded to the local directory."],"metadata":{"id":"RLe8iEWxV_zz"}},{"cell_type":"code","source":["%ls ./model/training"],"metadata":{"id":"0NTVj1kPRSCW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that a transfer learning recipe has been downloaded. The `ConstantPruningModifier` instructs SparseML to maintain the sparsity structure of the network as the model trains and the `QuantizationModifier` instructs SparseML to run Quantization Aware Training at the end of training."],"metadata":{"id":"orjvrvdCWEUi"}},{"cell_type":"code","source":["%cat ./transfer_recipe/recipe/recipe_original.md"],"metadata":{"id":"eUYg-7eBRT5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will set up the Hugging Face `tokenizer, config, and model`. These are all native Hugging Face objects, so check out the Hugging Face docs for more details on `AutoModel`, `AutoConfig`, and `AutoTokenizer` as needed. We instantiate these classes by passing the local path to the directory containing the `pytorch_model.bin`, `tokenizer.json`, and `config.json` files from the SparseZoo download."],"metadata":{"id":"CCTxNPD4XxHs"}},{"cell_type":"code","source":["# tokenizer\n","config = AutoConfig.from_pretrained(model_path, num_labels=NUM_LABELS)\n","\n","# initialize config\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","# initialize model using familiar HF AutoModel\n","model_kwargs = {\"config\": config}\n","model_kwargs[\"state_dict\"], s_delayed = SparseAutoModel._loadable_state_dict(model_path)\n","model = AutoModelForSequenceClassification.from_pretrained(model_path, **model_kwargs,)\n","\n","# prints metrics on sparsity profile\n","SparseAutoModel.log_model_load(model, model_path, \"student\", s_delayed)"],"metadata":{"id":"dhN1oGcTQ9RE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Sparse Transfer Learning: Tokenize The Dataset**\n","\n","Next, tokenize the dataset. \n","\n","Since we are using RoBERTa as the teacher model, which has a different tokenizer than BERT, we have to add results from teacher_tokenizer to results with `'distill_teacher': id`. SparseML parses the `result` and will send the tokens to the correct model during training.\n","\n","Note that if the teacher and student share a tokenizer, we can skip adding the teacher tokens and SparseML will pass the single set of tokens to each model during Training.\n"],"metadata":{"id":"Ny-5BtSsYRKH"}},{"cell_type":"code","source":["MAX_LEN = 128\n","def preprocess_fn(examples):\n","  args = None\n","  if INPUT_COL_2 is None:\n","    args = (examples[INPUT_COL_1], )\n","  else:\n","    args = (examples[INPUT_COL_1], examples[INPUT_COL_2])\n","  result = tokenizer(*args, \n","                   padding=\"max_length\", \n","                   max_length=min(tokenizer.model_max_length, MAX_LEN), \n","                   truncation=True)\n","  \n","  teacher_result = teacher_tokenizer(*args, \n","                   padding=\"max_length\", \n","                   max_length=min(teacher_tokenizer.model_max_length, MAX_LEN), \n","                   truncation=True)\n","  \n","  teacher_result = {\n","      f\"distill_teacher:{tokenizer_key}\": value\n","      for tokenizer_key, value in teacher_result.items()\n","  }\n","  result.update(teacher_result)\n","  return result\n","\n","# tokenize the dataset\n","tokenized_dataset = dataset_from_json.map(\n","    preprocess_fn,\n","    batched=True,\n","    desc=\"Running tokenizer on dataset\"\n",")"],"metadata":{"id":"2EUuFSTzRAvp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Sparse Transfer Learning: Fine-Tune the Model**\n","\n","SparseML has a custom `Trainer` class that inherits from the [Hugging Face `Trainer` Class](https://huggingface.co/docs/transformers/main_classes/trainer). As such, the SparseML `Trainer` has all of the existing functionality of the HF trainer. However, in addition, we can supply a `recipe` and (optionally) a `teacher`. \n","\n","\n","As we saw above, the `recipe` encodes the sparsity related algorithms and hyperparameters of the training process in a YAML file. The SparseML `Trainer` parses the `recipe` and adjusts the training workflow to apply the algorithms in the recipe. We use the `recipe_args` function to modify the recipe slightly (training for fewer epochs than used for SST2).\n","\n","The `teacher` is an optional argument that instructs SparseML to apply model distillation to support the training process. We are not using a teacher here, so setting to `disable` turns off distillation."],"metadata":{"id":"g5jB7s1bM4Qh"}},{"cell_type":"code","source":["# setup trainer arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./training_output\",\n","    do_train=True,\n","    do_eval=True,\n","    resume_from_checkpoint=False,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    save_total_limit=1,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    fp16=False)\n","\n","# initialize trainer\n","trainer = Trainer(\n","    model=model,\n","    model_state_path=model_path,\n","    recipe=recipe_path,\n","    recipe_args='{\"num_epochs\": 10.0, \"qat_start_epoch\": 7.0, \"observer_epoch\": 9.0}',\n","    teacher=teacher,\n","    metadata_args=[\"per_device_train_batch_size\",\"per_device_eval_batch_size\",\"fp16\"],\n","    args=training_args,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=default_data_collator,\n","    compute_metrics=compute_metrics)\n","\n","# step 5: run training\n","%rm -rf training_output\n","train_result = trainer.train()\n","trainer.save_model()\n","trainer.save_state()\n","trainer.save_optimizer_and_scheduler(training_args.output_dir)"],"metadata":{"id":"34IXj1n6RCgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!sparseml.transformers.export_onnx \\\n","  --model_path training_output \\\n","  --task text_classification"],"metadata":{"id":"-rhWjiHBeR7M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Optional: Deploy with DeepSparse**"],"metadata":{"id":"QicVADU5NW94"}},{"cell_type":"code","source":["%pip install deepsparse"],"metadata":{"id":"-XubpXohO_8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from deepsparse import Pipeline\n","\n","pipeline = Pipeline.create(\"text_classification\", model_path=\"./deployment\")"],"metadata":{"id":"m_USM8mCPETg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = pipeline(\"@user Get Donovan out of your soccer booth. He's awful. He's bitter. He makes me want to mute the tv. #horrid\")\n","print(prediction) # label 0 is anger"],"metadata":{"id":"YFzCkPotPXx6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = pipeline(\"@user Welcome to #MPSVT! We are delighted to have you! #grateful #MPSVT #relationships\")\n","print(prediction) # label 1 is joy"],"metadata":{"id":"g1oJZlFdPlvj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = pipeline(\"“The #optimist proclaims that we live in the best of all possible worlds; and the #pessimist fears this is true.” ~ James Branch Cabell\")\n","print(prediction) # label 2 is optimism"],"metadata":{"id":"TS-OCQJ_PpoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = pipeline(\"In need of a change! #restless\")\n","print(prediction) # label 3 is sadness"],"metadata":{"id":"qowh540KPGwh"},"execution_count":null,"outputs":[]}]}