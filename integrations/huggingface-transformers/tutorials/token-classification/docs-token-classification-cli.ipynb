{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtgLCRS8Afk3kBqptkH/m/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"cd180a53de6c43f990dc9ca40cfa0a0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d016be59a2df419f85e42ef31528ebc5","IPY_MODEL_185901f51bca44e28f2641bd3861b605","IPY_MODEL_fe5ef19ff9bb4187b64e3820bcd34bf2"],"layout":"IPY_MODEL_f8be44439c6944819df5225ae9208352"}},"d016be59a2df419f85e42ef31528ebc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0356a277d9541649c199a99cf1c6848","placeholder":"​","style":"IPY_MODEL_4945c933b4284d2ab12a162d136a48ac","value":"downloading...: 100%"}},"185901f51bca44e28f2641bd3861b605":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a04a81b13f44fca8922cc08bd1bf4bd","max":3928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4bc89d442c7646298b10e9a9942454d3","value":3928}},"fe5ef19ff9bb4187b64e3820bcd34bf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ed027447669498bb6f6bffe08dd4591","placeholder":"​","style":"IPY_MODEL_d93d7ffb522f4c02a41926a60d028cf3","value":" 3.84k/3.84k [00:00&lt;00:00, 127kB/s]"}},"f8be44439c6944819df5225ae9208352":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0356a277d9541649c199a99cf1c6848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4945c933b4284d2ab12a162d136a48ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a04a81b13f44fca8922cc08bd1bf4bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bc89d442c7646298b10e9a9942454d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ed027447669498bb6f6bffe08dd4591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d93d7ffb522f4c02a41926a60d028cf3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c29be55b40643939ddd26251b603123":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55078d52a7634f93b84daff12d290a10","IPY_MODEL_55310d85ac654669b61e0d9336737e19","IPY_MODEL_7bdfa88ddd3b4b6184e5fcb6dbd847f6"],"layout":"IPY_MODEL_345147d057a4496c8580c6a86fcec84c"}},"55078d52a7634f93b84daff12d290a10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e28dcae41b424ecda81120e75e358568","placeholder":"​","style":"IPY_MODEL_4a635058419f4fdc9c5f158e8abfd790","value":"100%"}},"55310d85ac654669b61e0d9336737e19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dab204cce0364612992df6bed8eb4927","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_37a67a2da4444248a5c9814f2ce74b57","value":3}},"7bdfa88ddd3b4b6184e5fcb6dbd847f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edc9264d8f144162b4115afc447be79d","placeholder":"​","style":"IPY_MODEL_c6e94b28bbd441b8a9dc71bfeb5a3f7c","value":" 3/3 [00:00&lt;00:00, 120.69it/s]"}},"345147d057a4496c8580c6a86fcec84c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e28dcae41b424ecda81120e75e358568":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a635058419f4fdc9c5f158e8abfd790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dab204cce0364612992df6bed8eb4927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37a67a2da4444248a5c9814f2ce74b57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edc9264d8f144162b4115afc447be79d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6e94b28bbd441b8a9dc71bfeb5a3f7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6880dacd711e456b90863a9e45256cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff9496df1512493f98175ae0e001aa74","IPY_MODEL_487da3389e6d4c2eba4f48321e03d9d2","IPY_MODEL_7d9e0318357444d387d3a06adc3930f4"],"layout":"IPY_MODEL_dec5be615e2942d5982a74a24511db0d"}},"ff9496df1512493f98175ae0e001aa74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef1627aaa0ae440eb317b06a8b323169","placeholder":"​","style":"IPY_MODEL_450b54b75e1b406ca98147dfec5d8739","value":"Downloading: "}},"487da3389e6d4c2eba4f48321e03d9d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffe3c1a5fa9a4362af9738bad0de2476","max":2543,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6524f663bd14a6592e7d3a00c96f8a7","value":2543}},"7d9e0318357444d387d3a06adc3930f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db0e6f474ee7423ca165bba2a0b1f5f1","placeholder":"​","style":"IPY_MODEL_bfde372330124e16a48b7191fe550b3e","value":" 7.46k/? [00:00&lt;00:00, 159kB/s]"}},"dec5be615e2942d5982a74a24511db0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef1627aaa0ae440eb317b06a8b323169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"450b54b75e1b406ca98147dfec5d8739":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffe3c1a5fa9a4362af9738bad0de2476":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6524f663bd14a6592e7d3a00c96f8a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db0e6f474ee7423ca165bba2a0b1f5f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfde372330124e16a48b7191fe550b3e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81b7a846b47b4787b34e955a65529800":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85093bd416794c4aabb35de03fba2b2b","IPY_MODEL_80870f0bd73746d38354ca84e9a5fa85","IPY_MODEL_a169314502264369a49e0d9da0a02084"],"layout":"IPY_MODEL_8ffa7906e82a45b5b5e205eee38765c6"}},"85093bd416794c4aabb35de03fba2b2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4208e423e4f4adf9c19f4f387797530","placeholder":"​","style":"IPY_MODEL_b16c5d633b8048b58ed682489460adfe","value":"Downloading: "}},"80870f0bd73746d38354ca84e9a5fa85":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a075a57bb8e740b58d99893081b1d822","max":1656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1860ae1731da41daaf1e7bd51a6a6cc6","value":1656}},"a169314502264369a49e0d9da0a02084":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6169d57096a349828d311ab521f945e5","placeholder":"​","style":"IPY_MODEL_75ed8d60feeb44419c597042a5495e02","value":" 4.28k/? [00:00&lt;00:00, 73.5kB/s]"}},"8ffa7906e82a45b5b5e205eee38765c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4208e423e4f4adf9c19f4f387797530":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b16c5d633b8048b58ed682489460adfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a075a57bb8e740b58d99893081b1d822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1860ae1731da41daaf1e7bd51a6a6cc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6169d57096a349828d311ab521f945e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75ed8d60feeb44419c597042a5495e02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a263a4135b7546628725076a6b5352b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4a5290a069e45ed97d6679cc8c62cbe","IPY_MODEL_d9aca31175e0491fa0a4b5890c797a0e","IPY_MODEL_061ed736b47645ce9de3cf50f22d4ec2"],"layout":"IPY_MODEL_e08b4c3cc69942fda4a08fbba2ab5240"}},"c4a5290a069e45ed97d6679cc8c62cbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0672861fc623435cbff1198d33fa0f0a","placeholder":"​","style":"IPY_MODEL_f9eadc0f5fb54f909439e950929d0749","value":"100%"}},"d9aca31175e0491fa0a4b5890c797a0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c966a37610d4b2c89240f9108bb6a4c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7808bf219ff14a29a0dd94bf9f505109","value":3}},"061ed736b47645ce9de3cf50f22d4ec2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09d0be7ea80d46659517ebf9b8064fed","placeholder":"​","style":"IPY_MODEL_4e45ac3e718e4c7192155b2ed8877cbd","value":" 3/3 [00:01&lt;00:00,  1.88it/s]"}},"e08b4c3cc69942fda4a08fbba2ab5240":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0672861fc623435cbff1198d33fa0f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9eadc0f5fb54f909439e950929d0749":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c966a37610d4b2c89240f9108bb6a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7808bf219ff14a29a0dd94bf9f505109":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09d0be7ea80d46659517ebf9b8064fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e45ac3e718e4c7192155b2ed8877cbd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac8bcb41ce974ff59905c34af75bef5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0e0b88f75caf4ffa8d4ae0f73419741c","IPY_MODEL_fc4ba734543d4e7d8955f282d2cfbb9d","IPY_MODEL_2fc1181b6e61445496cdf9c6b489d114"],"layout":"IPY_MODEL_8090370735ef43dfb29754acf9df7c89"}},"0e0b88f75caf4ffa8d4ae0f73419741c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb27888ed99746c69119ba74aba2e41f","placeholder":"​","style":"IPY_MODEL_c70774152d6245968fd908441095a0f6","value":"Downloading: "}},"fc4ba734543d4e7d8955f282d2cfbb9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_afecd61c44bb4ce2b922b9364618f729","max":185319,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86c172af4bb1408aaebc49fdcabd5c70","value":185319}},"2fc1181b6e61445496cdf9c6b489d114":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4d029ab010849c89245219e9cbda60c","placeholder":"​","style":"IPY_MODEL_21f54924b12c4242b6ed6876268dc877","value":" 494k/? [00:00&lt;00:00, 3.67MB/s]"}},"8090370735ef43dfb29754acf9df7c89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb27888ed99746c69119ba74aba2e41f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c70774152d6245968fd908441095a0f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afecd61c44bb4ce2b922b9364618f729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86c172af4bb1408aaebc49fdcabd5c70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4d029ab010849c89245219e9cbda60c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21f54924b12c4242b6ed6876268dc877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64ebd881cb3440f482a807572e31bf6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_690bad25d05941c8a3dfe27c3044eaeb","IPY_MODEL_4a905779335643c2b4e2d6142f2a4835","IPY_MODEL_69a7bc5ffb114b5484473046686f90d7"],"layout":"IPY_MODEL_eca5a049a27d4352bacdb19ab4fb3146"}},"690bad25d05941c8a3dfe27c3044eaeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80ff91fc779a44ebadf358231143412c","placeholder":"​","style":"IPY_MODEL_a5621ed281b048cc899befddd178874b","value":"Downloading: "}},"4a905779335643c2b4e2d6142f2a4835":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f35767a649b342df97c218b2d54fb490","max":39129,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3a8365fe9e545c2b8065f1b9dea8378","value":39129}},"69a7bc5ffb114b5484473046686f90d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28ad90a9016241f691c2219f536052a8","placeholder":"​","style":"IPY_MODEL_26b2bc910f444c13b912f037725553bd","value":" 115k/? [00:00&lt;00:00, 3.98MB/s]"}},"eca5a049a27d4352bacdb19ab4fb3146":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80ff91fc779a44ebadf358231143412c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5621ed281b048cc899befddd178874b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f35767a649b342df97c218b2d54fb490":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3a8365fe9e545c2b8065f1b9dea8378":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28ad90a9016241f691c2219f536052a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26b2bc910f444c13b912f037725553bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ee40c89dae24138bb25e5f5e93b80cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9253957b8cdd4e1fa7c51785757fe96e","IPY_MODEL_51302c2791d143bca60c7c3b0c86f5b2","IPY_MODEL_3f15e52c6e0942469e5e561e312c7b5c"],"layout":"IPY_MODEL_7476aa4f6e0a4446a89c87ff45a0fffe"}},"9253957b8cdd4e1fa7c51785757fe96e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd893678bdfb481fad9c61f5ddd16748","placeholder":"​","style":"IPY_MODEL_3b8366c7f9914b258344468e97b4d34d","value":"Downloading: "}},"51302c2791d143bca60c7c3b0c86f5b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8dd3e95f28345129d9288cb0878045e","max":66855,"min":0,"orientation":"horizontal","style":"IPY_MODEL_995891608836427ab6e61674bef5d724","value":66855}},"3f15e52c6e0942469e5e561e312c7b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc4ed9d966af42259e8029c0df1c794a","placeholder":"​","style":"IPY_MODEL_32cf63affc2d4298b9b4d3972963bd13","value":" 192k/? [00:00&lt;00:00, 2.25MB/s]"}},"7476aa4f6e0a4446a89c87ff45a0fffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd893678bdfb481fad9c61f5ddd16748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b8366c7f9914b258344468e97b4d34d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8dd3e95f28345129d9288cb0878045e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"995891608836427ab6e61674bef5d724":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc4ed9d966af42259e8029c0df1c794a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32cf63affc2d4298b9b4d3972963bd13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1af200224bda494bad8da9e2cc4ca504":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ecef0219858049b1b0c811c5fd3c86d4","IPY_MODEL_74c3344fa2b842de858403bd4b5d06d8","IPY_MODEL_c73af13f02e240e5b8ce92bad181f789"],"layout":"IPY_MODEL_19ffbc21d61f4706940ebe7e6194a529"}},"ecef0219858049b1b0c811c5fd3c86d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db14b30671b0411896de0c8ce4a76795","placeholder":"​","style":"IPY_MODEL_bc21d55a8a25496ba0d8bbf1f1ceb09d","value":"100%"}},"74c3344fa2b842de858403bd4b5d06d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c164a3aa954368ae5ee3e678194df0","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a398edb570014886a91053161d693e6a","value":3}},"c73af13f02e240e5b8ce92bad181f789":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a61c79b62d3541b2806c1a46d101f355","placeholder":"​","style":"IPY_MODEL_17cfd690e62b43eea69f0e320766e25a","value":" 3/3 [00:00&lt;00:00, 84.02it/s]"}},"19ffbc21d61f4706940ebe7e6194a529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db14b30671b0411896de0c8ce4a76795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc21d55a8a25496ba0d8bbf1f1ceb09d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57c164a3aa954368ae5ee3e678194df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a398edb570014886a91053161d693e6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a61c79b62d3541b2806c1a46d101f355":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17cfd690e62b43eea69f0e320766e25a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77006c838c6a47adb1293236af3b8fd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e66d098a65ff4a2594bee1bddd3b50e3","IPY_MODEL_4541d6c4fe6d4f26b270edebb2c7af35","IPY_MODEL_691f058d424a4d3ba8157371ddfb9ed1"],"layout":"IPY_MODEL_83a01e23257f422d98f28c66f1d23073"}},"e66d098a65ff4a2594bee1bddd3b50e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45c164e4f6c5497bbaea6a25fe1fb157","placeholder":"​","style":"IPY_MODEL_05a2ebc3456b401a8686e5169300e285","value":""}},"4541d6c4fe6d4f26b270edebb2c7af35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_d143d580479041a0b16557db8db71a05","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cce9162195154284a4228e79fb46761b","value":1}},"691f058d424a4d3ba8157371ddfb9ed1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7298574fc40e471d8ceec45544e0def4","placeholder":"​","style":"IPY_MODEL_801c0908cbd24a70a0aadf2090db55a4","value":" 3366/0 [00:00&lt;00:00, 4889.25 examples/s]"}},"83a01e23257f422d98f28c66f1d23073":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"45c164e4f6c5497bbaea6a25fe1fb157":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05a2ebc3456b401a8686e5169300e285":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d143d580479041a0b16557db8db71a05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"cce9162195154284a4228e79fb46761b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7298574fc40e471d8ceec45544e0def4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"801c0908cbd24a70a0aadf2090db55a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b39760b61ee4d0d8fcafcecbe12d80c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed23286acc1d4584a8b201e9eecc5265","IPY_MODEL_0013ac7ecbe34414bcdd773c464b2766","IPY_MODEL_080ff58291e34abf879342522713427d"],"layout":"IPY_MODEL_72ed9d77c9fe4a36a8791c7b50359458"}},"ed23286acc1d4584a8b201e9eecc5265":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df9f1c95575444c4be0d36c22c200200","placeholder":"​","style":"IPY_MODEL_d9798285725e4dd5a71e22bc9e258cfc","value":""}},"0013ac7ecbe34414bcdd773c464b2766":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a591ea88d2a4d9b914762b86e33c26f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6bac88d818e249bea2256d1adffcbf1a","value":1}},"080ff58291e34abf879342522713427d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13951194bcbe4fb9a065ea78ab7735a4","placeholder":"​","style":"IPY_MODEL_172229ba4cf2495884a045c3352cc69c","value":" 930/0 [00:00&lt;00:00, 4816.11 examples/s]"}},"72ed9d77c9fe4a36a8791c7b50359458":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"df9f1c95575444c4be0d36c22c200200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9798285725e4dd5a71e22bc9e258cfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a591ea88d2a4d9b914762b86e33c26f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6bac88d818e249bea2256d1adffcbf1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13951194bcbe4fb9a065ea78ab7735a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"172229ba4cf2495884a045c3352cc69c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b237fe5563c04b71b8291dfe35ec7ac9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f7ef6640b164ff7b0527259019b4a51","IPY_MODEL_a3ef606c8cd84903bae6c7133e957194","IPY_MODEL_44281307e86d41dd854fc259dcfeff4d"],"layout":"IPY_MODEL_4cbf672bc0e04e3a9aa222836d8f5eb9"}},"2f7ef6640b164ff7b0527259019b4a51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48ecf62ea2854352970f9e3c9d50ce89","placeholder":"​","style":"IPY_MODEL_7046a842da2e42b182b22ee439e928af","value":""}},"a3ef606c8cd84903bae6c7133e957194":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a0bd154a9a645698c07e04ff677cc6c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b1ed0df0a55478e8663d15d1f90d160","value":1}},"44281307e86d41dd854fc259dcfeff4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eb095863589408fba52ed0b46a9e67d","placeholder":"​","style":"IPY_MODEL_2ffa201c6b9246c880620fb8cc8c2795","value":" 877/0 [00:00&lt;00:00, 4437.57 examples/s]"}},"4cbf672bc0e04e3a9aa222836d8f5eb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"48ecf62ea2854352970f9e3c9d50ce89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7046a842da2e42b182b22ee439e928af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2a0bd154a9a645698c07e04ff677cc6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"0b1ed0df0a55478e8663d15d1f90d160":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7eb095863589408fba52ed0b46a9e67d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ffa201c6b9246c880620fb8cc8c2795":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89b8bb0958254052b3fe599ac2287b65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56574416d833471faa2cea8f0875c123","IPY_MODEL_40386bc96a94416f975411360e0903a1","IPY_MODEL_70c694a2c23147a081cad425ca7bd059"],"layout":"IPY_MODEL_8750dacd32f14b3d9d94415fa3d46fe8"}},"56574416d833471faa2cea8f0875c123":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da53e40a339f403fa6ae4d566fa42915","placeholder":"​","style":"IPY_MODEL_8a975ab450014840b7ddd03af1254ac0","value":"100%"}},"40386bc96a94416f975411360e0903a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_982fcf605f6a459e8c608e36ae62f863","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bbca7bcb3c41dcb8287f1560f50898","value":3}},"70c694a2c23147a081cad425ca7bd059":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6396bdfcfe914452985cfb76013aa7b5","placeholder":"​","style":"IPY_MODEL_28a1df55df2946e184ca6daf256c712b","value":" 3/3 [00:00&lt;00:00, 97.83it/s]"}},"8750dacd32f14b3d9d94415fa3d46fe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da53e40a339f403fa6ae4d566fa42915":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a975ab450014840b7ddd03af1254ac0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"982fcf605f6a459e8c608e36ae62f863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93bbca7bcb3c41dcb8287f1560f50898":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6396bdfcfe914452985cfb76013aa7b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a1df55df2946e184ca6daf256c712b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3dfc9c9954247beababb1e5a86b3c7a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec06b0ccebaf4eaa954f0affc8976a5b","IPY_MODEL_d2639c3fc91a47e98d3df86f7d5a3d33","IPY_MODEL_a0bbd8e91e6b48f1a4e95a68ce1e0c60"],"layout":"IPY_MODEL_2e8671b57210445fb53401024d578056"}},"ec06b0ccebaf4eaa954f0affc8976a5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc72c3a3d254023ba1c124ee111c068","placeholder":"​","style":"IPY_MODEL_cff2d3918ad34edb90751646afc0bebe","value":"Creating json from Arrow format: 100%"}},"d2639c3fc91a47e98d3df86f7d5a3d33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebf993cd61c143dbbaa2871aaa1847d3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7cf2f3c7cbbe451681897ed80e011d3c","value":1}},"a0bbd8e91e6b48f1a4e95a68ce1e0c60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39c54b7ebd504452a8dc98a8f0e0d036","placeholder":"​","style":"IPY_MODEL_ccbea320a6e04e248e3ded82a04a9dc4","value":" 1/1 [00:00&lt;00:00, 10.05ba/s]"}},"2e8671b57210445fb53401024d578056":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bc72c3a3d254023ba1c124ee111c068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cff2d3918ad34edb90751646afc0bebe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebf993cd61c143dbbaa2871aaa1847d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cf2f3c7cbbe451681897ed80e011d3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"39c54b7ebd504452a8dc98a8f0e0d036":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccbea320a6e04e248e3ded82a04a9dc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27c66986ebff4a0c8509cb13ebc7d46d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea8f89b213a2415882565a05a97e6049","IPY_MODEL_fdab62c990e246888a059a5c3381d53f","IPY_MODEL_ca88778193694d1daaa8aff3daa11794"],"layout":"IPY_MODEL_774a33562150432f8aa0ee4e436f5ca9"}},"ea8f89b213a2415882565a05a97e6049":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d7deba052de41f8aabd4d057dc912d5","placeholder":"​","style":"IPY_MODEL_19abac1675b34fb7a15983e0449dc417","value":"Creating json from Arrow format: 100%"}},"fdab62c990e246888a059a5c3381d53f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6331398b17e46bdbf4a5bc13a2071dc","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_126c7f21c8d347f3980ed29483af5878","value":1}},"ca88778193694d1daaa8aff3daa11794":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32ea139c34c24e8f82522b990e0415ab","placeholder":"​","style":"IPY_MODEL_68deab1ef9d24e1ebf703e5da74fb35e","value":" 1/1 [00:00&lt;00:00, 22.41ba/s]"}},"774a33562150432f8aa0ee4e436f5ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d7deba052de41f8aabd4d057dc912d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19abac1675b34fb7a15983e0449dc417":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6331398b17e46bdbf4a5bc13a2071dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"126c7f21c8d347f3980ed29483af5878":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32ea139c34c24e8f82522b990e0415ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68deab1ef9d24e1ebf703e5da74fb35e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Token Classification: Sparse Transfer Learning with the CLI**\n","\n","In this example, you will fine-tune a 90% pruned BERT model onto some token classification datasets using SparseML's CLI.\n","\n","### **Sparse Transfer Learning Overview**\n","\n","Sparse Transfer Learning is very similiar to typical fine-tuning you are used to when training models. However, with Sparse Transfer Learning, we start the training process from a pre-sparsified checkpoint and maintain the sparsity structure while the fine tuning occurs. At the end, you will have a sparse model trained on your dataset, ready to be deployed with DeepSparse for GPU-class performance on CPUs!\n","\n","### **Pre-Sparsified BERT**\n","SparseZoo, Neural Magic's open source repository of pre-sparsified models, contains a 90% pruned version of BERT, which has been sparsified on the upstream Wikipedia and BookCorpus datasets with the\n","masked language modeling objective. [Check out the model card](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fobert-base%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned90-none). We will use this model as the starting point for the transfer learning process.\n","\n","\n","***Let's dive in!***"],"metadata":{"id":"kSNEB-3orJ9C"}},{"cell_type":"markdown","source":["## **Installation**\n","\n","Install SparseML via `pip`.\n","\n"],"metadata":{"id":"Y0WybTbssU0g"}},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"Nr5jM2zoqzuG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677529939345,"user_tz":300,"elapsed":83179,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"fdf6a039-e62f-4d1c-ef68-ec4f4b600f7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 1.13.1+cu116\n","Uninstalling torch-1.13.1+cu116:\n","  Successfully uninstalled torch-1.13.1+cu116\n","Found existing installation: torchvision 0.14.1+cu116\n","Uninstalling torchvision-0.14.1+cu116:\n","  Successfully uninstalled torchvision-0.14.1+cu116\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sparseml[torch]\n","  Downloading sparseml-1.4.0-py3-none-any.whl (904 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.8/904.8 KB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter>=1.0.0\n","  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n","Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (4.64.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (1.7.3)\n","Collecting onnx<=1.12.0,>=1.5.0\n","  Downloading onnx-1.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting toposort>=1.0\n","  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n","Requirement already satisfied: pydantic>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (1.10.5)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (3.5.3)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (2.25.1)\n","Collecting merge-args>=0.1.0\n","  Downloading merge_args-0.1.5-py2.py3-none-any.whl (6.0 kB)\n","Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (1.3.5)\n","Collecting click~=8.0.0\n","  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy<=1.21.6,>=1.0.0\n","  Downloading numpy-1.21.6-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (6.0)\n","Requirement already satisfied: progressbar2>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (3.38.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (23.0)\n","Requirement already satisfied: setuptools<=59.5.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (57.4.0)\n","Collecting GPUtil>=1.4.0\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (5.4.8)\n","Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (0.18.3)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (3.19.6)\n","Collecting sparsezoo~=1.4.0\n","  Downloading sparsezoo-1.4.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (1.0.2)\n","Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from sparseml[torch]) (7.7.1)\n","Collecting gputils\n","  Downloading gputils-1.0.6-py3-none-any.whl (3.8 kB)\n","Collecting torch<=1.12.1,>=1.1.0\n","  Downloading torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.7.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (7.9.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (0.2.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (5.3.4)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (3.6.2)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.0.0->sparseml[torch]) (3.0.5)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (6.3.0)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (6.1.0)\n","Collecting qtconsole\n","  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from jupyter>=1.0.0->sparseml[torch]) (5.6.1)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (8.4.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.0.0->sparseml[torch]) (4.38.0)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx<=1.12.0,>=1.5.0->sparseml[torch]) (4.5.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25.0->sparseml[torch]) (2022.7.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from progressbar2>=3.0.0->sparseml[torch]) (1.15.0)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from progressbar2>=3.0.0->sparseml[torch]) (3.5.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->sparseml[torch]) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->sparseml[torch]) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->sparseml[torch]) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.0.0->sparseml[torch]) (2022.12.7)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2023.2.3)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (3.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (2.9.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.15.0->sparseml[torch]) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->sparseml[torch]) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24.2->sparseml[torch]) (1.2.0)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (6.2)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->sparseml[torch]) (6.1.12)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (2.6.1)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (2.0.10)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (4.4.2)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (5.7.3)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (21.3.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (2.11.3)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (23.2.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (0.16.0)\n","Requirement already satisfied: Send2Trash>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (1.8.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (0.13.3)\n","Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.8/dist-packages (from notebook->jupyter>=1.0.0->sparseml[torch]) (5.2.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.8.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.7.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (6.0.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (1.5.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.4)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.6.0)\n","Collecting qtpy>=2.0.1\n","  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook->jupyter>=1.0.0->sparseml[torch]) (2.0.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.6.1->notebook->jupyter>=1.0.0->sparseml[torch]) (3.0.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (4.3.3)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (2.16.2)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->sparseml[torch]) (0.2.6)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->sparseml[torch]) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->sparseml[torch]) (21.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->sparseml[torch]) (0.5.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (5.12.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (22.2.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (0.19.3)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->sparseml[torch]) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->sparseml[torch]) (2.21)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->sparseml[torch]) (3.14.0)\n","Building wheels for collected packages: GPUtil\n","  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=5a9d1406ea8a273c5b36189c00734bee6bc1dea4cec2d3ed041a017b528d85e3\n","  Stored in directory: /root/.cache/pip/wheels/ba/03/bb/7a97840eb54479b328672e15a536e49dc60da200fb21564d53\n","Successfully built GPUtil\n","Installing collected packages: toposort, merge-args, GPUtil, torch, qtpy, numpy, jedi, click, onnx, sparsezoo, gputils, qtconsole, jupyter, sparseml\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.22.4\n","    Uninstalling numpy-1.22.4:\n","      Successfully uninstalled numpy-1.22.4\n","  Attempting uninstall: click\n","    Found existing installation: click 7.1.2\n","    Uninstalling click-7.1.2:\n","      Successfully uninstalled click-7.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.11 requires torchvision>=0.8.2, which is not installed.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.12.1 which is incompatible.\n","flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.0.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed GPUtil-1.4.0 click-8.0.4 gputils-1.0.6 jedi-0.18.2 jupyter-1.0.0 merge-args-0.1.5 numpy-1.21.6 onnx-1.12.0 qtconsole-5.4.0 qtpy-2.3.0 sparseml-1.4.0 sparsezoo-1.4.0 toposort-1.10 torch-1.12.1\n"]}],"source":["%pip uninstall torch torchvision -y\n","%pip install sparseml[torch]"]},{"cell_type":"markdown","source":["If you are running on Google Colab, restart the runtime after this step."],"metadata":{"id":"1SlYUvD61ppy"}},{"cell_type":"code","source":["!sparseml.transformers.text_classification --help"],"metadata":{"collapsed":true,"id":"BT_fq1tk2O4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Sparse Transfer Learning with Conll2003**\n","\n","SparseML's CLI enables you to kick-off training workflows with various utilities like dataset loading, checkpoint saving, \n","metric reporting, and logging handled for you. All we have to do is pass a `model_name_or_path` (the starting checkpoint), a `task` (the GLUE task to train on), and a `recipe` (a YAML file specifying the sparsity related parameters) and we are up and running. The `recipes` is critical for instructing the training script how to modify the training process with sparsity related algorithms. For Sparse Transfer Learning, we will use a `recipe` that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. "],"metadata":{"id":"vG_qKQcXsfgW"}},{"cell_type":"markdown","source":["### **Run Transfer Learning**\n","\n","For Conll2003, there is a pre-made transfer learning recipe available in [SparseZoo](https://sparsezoo.neuralmagic.com/models/nlp%2Ftoken_classification%2Fobert-base%2Fpytorch%2Fhuggingface%2Fconll2003%2Fpruned90_quant-none). As such, we kick off transfer learning with the following:"],"metadata":{"id":"jM1QJxiMu5T8"}},{"cell_type":"code","source":["!sparseml.transformers.train.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --distill_teacher zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/base-none \\\n","  --dataset_name conll2003 \\\n","  --output_dir sparse_bert-token_classification_conll2003 \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1 \\\n","  --max_train_samples 2000"],"metadata":{"id":"hz8o5CNlsNo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's discuss the key arguments:\n","- `--dataset_name conll2003` instructs SparseML to download and fine-tune onto the Conll2003 dataset. The script automatically downloads the dataset from the Hugging Face hub.\n","\n","- `--model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none` specifies the starting checkpoint for the fine tuning. Here, we passed a SparseZoo stub identifying the 90% pruned version of BERT trained with masked language modeling on the Wikipedia and BookCorpus datasets. SparseML downloads the checkpoint from the Zoo when the script starts.\n","\n","- `--recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none` specifies the recipe to be applied by SparseML. Here, we passed a SparseZoo stub identifying the transfer learning recipe for the Conll2003 dataset. SparseML downloads the recipe from the Zoo when the script starts. See below for the details of what this recipe looks like.\n","\n","- `--distill_teacher zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/base-none` is an optional argument that specifies a model to use for as a teacher to apply distillation during the training process. We passed a SparseZoo stub identifying a dense BERT model trained on Conll2003. SparseML downloads the teacher from the Zoo when the script starts.\n","\n","The script downloads the starting checkpoint, the teacher model, and transfer learning recipe from SparseZoo as well as the Conll2003 dataset and trains the model for 13 epochs, converging to ~98.5% accuracy on the validation set. The final model is quantized with 90% of weights pruned!"],"metadata":{"id":"-HFYMN3yq6fJ"}},{"cell_type":"markdown","source":["#### **Transfer Learning Recipe**\n","\n","Here's what the transfer learning recipe for the Conll2003 dataset looks like.\n","\n","The \"Modifiers\" are the important items that encode how SparseML should modify the training process for Sparse Transfer Learning:\n","- `ConstantPruningModifier` tells SparseML to pin weights at 0 over all epochs, maintaining the sparsity structure of the network\n","- `QuantizationModifier` tells SparseML to quanitze the weights with quantization aware training over the last 5 epochs\n","- `DistillationModifier` tells SparseML how to apply distillation to the model, including the layer and some hyperparameters\n","\n","SparseML parses the modifiers and updates the training process to implement the algorithms and hyperparameters specified in the recipes."],"metadata":{"id":"ZJnGKuXnwhWH"}},{"cell_type":"code","source":["from sparsezoo import Model\n","transfer_stub = \"zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none\"\n","download_dir = \"./transfer_recipe\"\n","zoo_model = Model(transfer_stub, download_path=download_dir)\n","recipe_path = zoo_model.recipes.default.path\n","print(recipe_path)"],"metadata":{"id":"D5Z_a3HOwrpu","colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["cd180a53de6c43f990dc9ca40cfa0a0c","d016be59a2df419f85e42ef31528ebc5","185901f51bca44e28f2641bd3861b605","fe5ef19ff9bb4187b64e3820bcd34bf2","f8be44439c6944819df5225ae9208352","c0356a277d9541649c199a99cf1c6848","4945c933b4284d2ab12a162d136a48ac","7a04a81b13f44fca8922cc08bd1bf4bd","4bc89d442c7646298b10e9a9942454d3","1ed027447669498bb6f6bffe08dd4591","d93d7ffb522f4c02a41926a60d028cf3"]},"executionInfo":{"status":"ok","timestamp":1677525924382,"user_tz":300,"elapsed":1217,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"093b8f25-3549-4bc1-b62b-52ac6ad9525b"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["downloading...:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd180a53de6c43f990dc9ca40cfa0a0c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["./transfer_recipe/recipe/recipe_original.md\n"]}]},{"cell_type":"code","source":["%cat ./transfer_recipe/recipe/recipe_original.md"],"metadata":{"id":"Oeu1PvyYxR0V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677525925001,"user_tz":300,"elapsed":621,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"3f3f4d4b-b34e-40f9-8604-a7a346325c07"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["<!--\n","Copyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\");\n","you may not use this file except in compliance with the License.\n","You may obtain a copy of the License at\n","\n","   http://www.apache.org/licenses/LICENSE-2.0\n","\n","Unless required by applicable law or agreed to in writing,\n","software distributed under the License is distributed on an \"AS IS\" BASIS,\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","See the License for the specific language governing permissions and\n","limitations under the License.\n","-->\n","\n","---\n","version: 1.1.0\n","\n","# General Variables\n","num_epochs: 13\n","init_lr: 1.5e-4 \n","final_lr: 0\n","\n","qat_start_epoch: 8.0\n","observer_epoch: 12.0\n","quantize_embeddings: 1\n","\n","distill_hardness: 1.0\n","distill_temperature: 2.0\n","\n","# Modifiers:\n","\n","training_modifiers:\n","  - !EpochRangeModifier\n","      end_epoch: eval(num_epochs)\n","      start_epoch: 0.0\n","\n","  - !LearningRateFunctionModifier\n","      start_epoch: 0\n","      end_epoch: eval(num_epochs)\n","      lr_func: linear\n","      init_lr: eval(init_lr)\n","      final_lr: eval(final_lr)\n","    \n","quantization_modifiers:\n","\n","  - !QuantizationModifier\n","      start_epoch: eval(qat_start_epoch)\n","      disable_quantization_observer_epoch: eval(observer_epoch)\n","      freeze_bn_stats_epoch: eval(observer_epoch)\n","      quantize_embeddings: eval(quantize_embeddings)\n","      quantize_linear_activations: 0\n","      exclude_module_types: ['LayerNorm']\n","      submodules:\n","        - bert.embeddings\n","        - bert.encoder\n","        - classifier\n","\n","\n","distillation_modifiers:\n","  - !DistillationModifier\n","     hardness: eval(distill_hardness)\n","     temperature: eval(distill_temperature)\n","     distill_output_keys: [logits]\n","\n","constant_modifiers:\n","\n","  - !ConstantPruningModifier\n","      start_epoch: 0.0\n","      params: __ALL_PRUNABLE__\n","---\n","\n","# oBERT 90% unstructured pruned quantized on CoNNL-2003\n","\n","This recipe defines the hyperparams necessary to transfer the oBERT 90% unstructured pruned (upstream) and then quantize on the named entity recognition task [CoNLL-2003](https://arxiv.org/abs/cs/0306050v1).\n","Users are encouraged to experiment with the training length and initial learning rate to either expedite training or to produce a more accurate model.\n","This can be done by either editing the recipe or supplying the --recipe_args argument to the training commands.\n","For example, the following appended to the training commands will change the number of epochs and the initial learning rate:\n","```bash\n","--recipe_args '{\"num_epochs\":8,\"init_lr\":0.0001}'\n","```\n","\n","## Training\n","\n","To set up the training environment, [install SparseML](https://github.com/neuralmagic/sparseml#installation).\n","\n","The following command is used to transfer to the CoNNL-2003 NER task in the training environment on a single GPU.\n","It achieves an accuracy of 98.55% on the validation set.\n","\n","```bash\n","sparseml.transformers.train.token_classification \\\n","  --output_dir sparse_bert-token_classification_connl2003 \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --distill_teacher zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/base-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --dataset_name conll2003 --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1 \n","```\n","\n","## Evaluation\n","\n","The model could be evaluated with the following command:\n","\n","```bash\n","sparseml.transformers.train.token_classification \\\n","  --output_dir sparse_bert-token_classification_connl2003_eval \\\n","  --model_name_or_path zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --dataset_name conll2003 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_eval\n","```\n"]}]},{"cell_type":"markdown","source":["### **Export to ONNX**\n","\n","Once you have trained your model, export to ONNX in order to deploy with DeepSparse. The artifacts of the training process \n","are saved to your local filesystem. \n","\n","Run the following to convert your PyTorch checkpoint to ONNX:"],"metadata":{"id":"bFbppO99xGeT"}},{"cell_type":"code","source":["!sparseml.transformers.export_onnx \\\n","  --model_path ./sparse_bert-token_classification_conll2003 \\\n","  --task text_classification"],"metadata":{"id":"OHKEUvPOx-su","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677526630287,"user_tz":300,"elapsed":66378,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"afb7b74e-984e-43d7-fcd0-318abd5244e0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-02-27 19:36:06.968975: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-02-27 19:36:07.989614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-27 19:36:07.989766: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n","2023-02-27 19:36:07.989789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","2023-02-27 19:36:09 sparseml.transformers.export INFO     Attempting onnx export for model at ./sparse_bert-token_classification_connl2003 for task text-classification\n","2023-02-27 19:36:09 sparseml.transformers.utils.model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied ./sparse_bert-token_classification_connl2003\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sparse_bert-token_classification_connl2003 and are newly initialized: ['encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'classifier.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'classifier.weight', 'encoder.layer.3.attention.self.query.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.key.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2023-02-27 19:36:11 sparseml.transformers.utils.model INFO     Delayed load of model ./sparse_bert-token_classification_connl2003 detected. Will print out model information once SparseML recipes have loaded\n","2023-02-27 19:36:11 sparseml.transformers.export INFO     loaded model, config, and tokenizer from ./sparse_bert-token_classification_connl2003\n","2023-02-27 19:36:11 sparseml.transformers.sparsification.trainer INFO     Loaded 1 SparseML checkpoint recipe stage(s) from ./sparse_bert-token_classification_connl2003/recipe.yaml to replicate model sparse state\n","2023-02-27 19:36:16 sparseml.transformers.sparsification.trainer INFO     Applied structure from 1 previous recipe stage(s) to model and finalized (recipes saved with model_path)\n","All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./sparse_bert-token_classification_connl2003 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","2023-02-27 19:36:17 sparseml.transformers.sparsification.trainer WARNING  Missing keys found when reloading model state for SparseML recipe:['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n","2023-02-27 19:36:17 sparseml.transformers.sparsification.trainer INFO     Reloaded 1768 model params for SparseML Recipe from ./sparse_bert-token_classification_connl2003\n","2023-02-27 19:36:18 sparseml.transformers.utils.model INFO     Loaded model from ./sparse_bert-token_classification_connl2003 with 109489161 total params. Of those there are 85531392 prunable params which have 89.37208691751445 avg sparsity.\n","2023-02-27 19:36:20 sparseml.transformers.utils.model INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 109489161, \"sparse\": 76441958, \"sparsity_percent\": 69.81691822444415, \"prunable\": 85531392, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.37208691751445, \"quantizable\": 85615113, \"quantized\": 85024521, \"quantized_percent\": 99.31017786544298}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9312150478363037, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": true}, \"bert.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": true}, \"bert.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": true}, \"bert.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9295670986175537, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8711106777191162, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": true}, \"bert.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": true}, \"bert.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": true}, \"bert.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8685166835784912, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": true}, \"bert.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8958756923675537, \"quantized\": true}, \"bert.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": true}, \"bert.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8924729824066162, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8780839443206787, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": true}, \"bert.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": true}, \"bert.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": true}, \"bert.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066188812256, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442287445068, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": true}, \"bert.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": true}, \"bert.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": true}, \"bert.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8733994960784912, \"quantized\": true}, \"bert.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": true}, \"bert.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": true}, \"bert.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8541886806488037, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": true}, \"bert.pooler.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.module.weight\": {\"numel\": 6912, \"sparsity\": 0.0, \"quantized\": true}}}\n","2023-02-27 19:36:20 sparseml.transformers.sparsification.trainer INFO     Reloaded model state after SparseML recipe structure modifications from ./sparse_bert-token_classification_connl2003\n","2023-02-27 19:36:20 sparseml.transformers.export INFO     Applied an unstaged recipe to the model at ./sparse_bert-token_classification_connl2003\n","2023-02-27 19:36:20 sparseml.transformers.export INFO     Created sample inputs for the ONNX export process: {'input_ids': 'torch.int64: [1, 384]', 'attention_mask': 'torch.int64: [1, 384]', 'token_type_ids': 'torch.int64: [1, 384]'}\n","/usr/local/lib/python3.8/dist-packages/torch/onnx/utils.py:439: UserWarning: It is recommended that constant folding be turned off ('do_constant_folding=False') when exporting the model in training-amenable mode, i.e. with 'training=TrainingMode.TRAIN' or 'training=TrainingMode.PRESERVE' (when model is in training mode). Otherwise, some learnable model parameters may not translate correctly in the exported ONNX model because constant folding mutates model parameters. Please consider turning off constant folding or setting the training=TrainingMode.EVAL.\n","  warnings.warn(\n","2023-02-27 19:36:55 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 3 QAT embedding ops to UINT8\n","2023-02-27 19:36:56 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 24 quantizable MatMul ops to QLinearMatMul\n","2023-02-27 19:37:02 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 72 quantizable MatMul ops with weight and bias to MatMulInteger and Add\n","2023-02-27 19:37:02 sparseml.pytorch.sparsification.quantization.quantize_qat_export INFO     Converted 1 quantizable Gemm ops with weight and bias to MatMulInteger and Add\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     ONNX exported to ./sparse_bert-token_classification_connl2003/model.onnx\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     0 sample inputs/outputs exported\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     Saved model.onnx in the deployment folder at ./deployment/model.onnx\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     Saved tokenizer.json in the deployment folder at ./deployment/tokenizer.json\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     Saved tokenizer_config.json in the deployment folder at ./deployment/tokenizer_config.json\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     Saved config.json in the deployment folder at ./deployment/config.json\n","2023-02-27 19:37:07 sparseml.transformers.export INFO     Created deployment folder at ./deployment with files: ['model.onnx', 'config.json', 'tokenizer_config.json', 'tokenizer.json']\n"]}]},{"cell_type":"markdown","source":["The script above creates a `deployment` folder in your local directory, which has all of the files needed for deployment with DeepSparse including the `model.onnx`, `config.json`, and `tokenizer.json` files."],"metadata":{"id":"B3JKl3xIyDIe"}},{"cell_type":"markdown","source":["## **Sparse Transfer Learning with a Custom Dataset**\n","\n","Beyond the Conll2003 dataset, we can also use a dataset from the Hugging Face Hub or pass via local files. Let's try an example of each for the sentiment analysis using [WNUT 17](wnut_17), which is also a NER task.\n","\n","For simplicity, we will perform the fine-tuning without distillation. Although the transfer learning recipe contains distillation\n","modifiers, by setting `--distill_teacher disable` we instruct SparseML to skip distillation."],"metadata":{"id":"bCE4HKWvyWRu"}},{"cell_type":"markdown","source":["### **Using a Hugging Face Dataset**\n","\n","Let's walk through how to pass a Hugging Face dataset identifier to the CLI."],"metadata":{"id":"n5ab0yh6yspE"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from pprint import pprint\n","\n","wnut_17 = load_dataset(\"wnut_17\")\n","print(wnut_17)\n","print(wnut_17[\"train\"][0])"],"metadata":{"id":"hfKp5RaMybmt","colab":{"base_uri":"https://localhost:8080/","height":355,"referenced_widgets":["1c29be55b40643939ddd26251b603123","55078d52a7634f93b84daff12d290a10","55310d85ac654669b61e0d9336737e19","7bdfa88ddd3b4b6184e5fcb6dbd847f6","345147d057a4496c8580c6a86fcec84c","e28dcae41b424ecda81120e75e358568","4a635058419f4fdc9c5f158e8abfd790","dab204cce0364612992df6bed8eb4927","37a67a2da4444248a5c9814f2ce74b57","edc9264d8f144162b4115afc447be79d","c6e94b28bbd441b8a9dc71bfeb5a3f7c"]},"executionInfo":{"status":"ok","timestamp":1677527252522,"user_tz":300,"elapsed":1352,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"66cfa4de-652e-4a37-c6a7-bf92e978d011"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Reusing dataset wnut_17 (/root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c29be55b40643939ddd26251b603123"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 3394\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 1009\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 1287\n","    })\n","})\n","{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}]},{"cell_type":"markdown","source":["#### **Kick off Training**\n","\n","To use this dataset with the CLI, we can replace the `--dataset_name conll2003` argument with `--dataset_name wnut_17 --input_column_names tokens --label_column_name label`. SparseML will then download the dataset from the Hugging Face hub and run training as before."],"metadata":{"id":"pdpvTT654LAy"}},{"cell_type":"code","source":["!sparseml.transformers.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}' \\\n","  --distill_teacher disable \\\n","  --dataset_name wnut_17 --text_column_name tokens --label_column_name ner_tags \\\n","  --output_dir sparse_bert-token_classification_wnut_17 \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1"],"metadata":{"id":"GMxgkhMRz-x3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will notice that we used the same recipe as we did in the Conll2003 case (identified by the SparseZoo stub `zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none`). Since the WNUT and Conll2003 tasks are similiar, we chose to start with the same hyperparameters as we used in Conll2003 training.\n","\n","\n","To update a recipe, you can download the YAML file from SparseZoo, make updates to the YAML directly, and pass the local path to SparseML. In this case, we used `--recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}'` to modify a recipe on the fly, updating to only run for 11 epochs."],"metadata":{"id":"7NfB9uhi0CBX"}},{"cell_type":"markdown","source":["### **Using Local CSV/JSON Files**\n","\n","Let's walk through how to pass a CSV/JSON dataset to the CLI."],"metadata":{"id":"79KVF5TB11xD"}},{"cell_type":"markdown","source":["#### **Save Dataset as a CSV File**\n","\n","For this example, we use Hugging Face `datasets` to create a JSON file for WNUT 17 that can be passed to SparseML's CLI."],"metadata":{"id":"eZF7myQc23uc"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from pprint import pprint\n","\n","dataset = load_dataset(\"wnut_17\")\n","print(dataset)\n","print(dataset[\"train\"][0])"],"metadata":{"id":"zWsAD8b_2tQ2","colab":{"base_uri":"https://localhost:8080/","height":597,"referenced_widgets":["6880dacd711e456b90863a9e45256cdb","ff9496df1512493f98175ae0e001aa74","487da3389e6d4c2eba4f48321e03d9d2","7d9e0318357444d387d3a06adc3930f4","dec5be615e2942d5982a74a24511db0d","ef1627aaa0ae440eb317b06a8b323169","450b54b75e1b406ca98147dfec5d8739","ffe3c1a5fa9a4362af9738bad0de2476","b6524f663bd14a6592e7d3a00c96f8a7","db0e6f474ee7423ca165bba2a0b1f5f1","bfde372330124e16a48b7191fe550b3e","81b7a846b47b4787b34e955a65529800","85093bd416794c4aabb35de03fba2b2b","80870f0bd73746d38354ca84e9a5fa85","a169314502264369a49e0d9da0a02084","8ffa7906e82a45b5b5e205eee38765c6","b4208e423e4f4adf9c19f4f387797530","b16c5d633b8048b58ed682489460adfe","a075a57bb8e740b58d99893081b1d822","1860ae1731da41daaf1e7bd51a6a6cc6","6169d57096a349828d311ab521f945e5","75ed8d60feeb44419c597042a5495e02","a263a4135b7546628725076a6b5352b7","c4a5290a069e45ed97d6679cc8c62cbe","d9aca31175e0491fa0a4b5890c797a0e","061ed736b47645ce9de3cf50f22d4ec2","e08b4c3cc69942fda4a08fbba2ab5240","0672861fc623435cbff1198d33fa0f0a","f9eadc0f5fb54f909439e950929d0749","3c966a37610d4b2c89240f9108bb6a4c","7808bf219ff14a29a0dd94bf9f505109","09d0be7ea80d46659517ebf9b8064fed","4e45ac3e718e4c7192155b2ed8877cbd","ac8bcb41ce974ff59905c34af75bef5e","0e0b88f75caf4ffa8d4ae0f73419741c","fc4ba734543d4e7d8955f282d2cfbb9d","2fc1181b6e61445496cdf9c6b489d114","8090370735ef43dfb29754acf9df7c89","cb27888ed99746c69119ba74aba2e41f","c70774152d6245968fd908441095a0f6","afecd61c44bb4ce2b922b9364618f729","86c172af4bb1408aaebc49fdcabd5c70","e4d029ab010849c89245219e9cbda60c","21f54924b12c4242b6ed6876268dc877","64ebd881cb3440f482a807572e31bf6f","690bad25d05941c8a3dfe27c3044eaeb","4a905779335643c2b4e2d6142f2a4835","69a7bc5ffb114b5484473046686f90d7","eca5a049a27d4352bacdb19ab4fb3146","80ff91fc779a44ebadf358231143412c","a5621ed281b048cc899befddd178874b","f35767a649b342df97c218b2d54fb490","c3a8365fe9e545c2b8065f1b9dea8378","28ad90a9016241f691c2219f536052a8","26b2bc910f444c13b912f037725553bd","5ee40c89dae24138bb25e5f5e93b80cc","9253957b8cdd4e1fa7c51785757fe96e","51302c2791d143bca60c7c3b0c86f5b2","3f15e52c6e0942469e5e561e312c7b5c","7476aa4f6e0a4446a89c87ff45a0fffe","cd893678bdfb481fad9c61f5ddd16748","3b8366c7f9914b258344468e97b4d34d","a8dd3e95f28345129d9288cb0878045e","995891608836427ab6e61674bef5d724","bc4ed9d966af42259e8029c0df1c794a","32cf63affc2d4298b9b4d3972963bd13","1af200224bda494bad8da9e2cc4ca504","ecef0219858049b1b0c811c5fd3c86d4","74c3344fa2b842de858403bd4b5d06d8","c73af13f02e240e5b8ce92bad181f789","19ffbc21d61f4706940ebe7e6194a529","db14b30671b0411896de0c8ce4a76795","bc21d55a8a25496ba0d8bbf1f1ceb09d","57c164a3aa954368ae5ee3e678194df0","a398edb570014886a91053161d693e6a","a61c79b62d3541b2806c1a46d101f355","17cfd690e62b43eea69f0e320766e25a","77006c838c6a47adb1293236af3b8fd8","e66d098a65ff4a2594bee1bddd3b50e3","4541d6c4fe6d4f26b270edebb2c7af35","691f058d424a4d3ba8157371ddfb9ed1","83a01e23257f422d98f28c66f1d23073","45c164e4f6c5497bbaea6a25fe1fb157","05a2ebc3456b401a8686e5169300e285","d143d580479041a0b16557db8db71a05","cce9162195154284a4228e79fb46761b","7298574fc40e471d8ceec45544e0def4","801c0908cbd24a70a0aadf2090db55a4","1b39760b61ee4d0d8fcafcecbe12d80c","ed23286acc1d4584a8b201e9eecc5265","0013ac7ecbe34414bcdd773c464b2766","080ff58291e34abf879342522713427d","72ed9d77c9fe4a36a8791c7b50359458","df9f1c95575444c4be0d36c22c200200","d9798285725e4dd5a71e22bc9e258cfc","3a591ea88d2a4d9b914762b86e33c26f","6bac88d818e249bea2256d1adffcbf1a","13951194bcbe4fb9a065ea78ab7735a4","172229ba4cf2495884a045c3352cc69c","b237fe5563c04b71b8291dfe35ec7ac9","2f7ef6640b164ff7b0527259019b4a51","a3ef606c8cd84903bae6c7133e957194","44281307e86d41dd854fc259dcfeff4d","4cbf672bc0e04e3a9aa222836d8f5eb9","48ecf62ea2854352970f9e3c9d50ce89","7046a842da2e42b182b22ee439e928af","2a0bd154a9a645698c07e04ff677cc6c","0b1ed0df0a55478e8663d15d1f90d160","7eb095863589408fba52ed0b46a9e67d","2ffa201c6b9246c880620fb8cc8c2795","89b8bb0958254052b3fe599ac2287b65","56574416d833471faa2cea8f0875c123","40386bc96a94416f975411360e0903a1","70c694a2c23147a081cad425ca7bd059","8750dacd32f14b3d9d94415fa3d46fe8","da53e40a339f403fa6ae4d566fa42915","8a975ab450014840b7ddd03af1254ac0","982fcf605f6a459e8c608e36ae62f863","93bbca7bcb3c41dcb8287f1560f50898","6396bdfcfe914452985cfb76013aa7b5","28a1df55df2946e184ca6daf256c712b"]},"executionInfo":{"status":"ok","timestamp":1677530028368,"user_tz":300,"elapsed":7963,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"1ce2fe97-428d-4154-e4a3-69f7055fa5f9"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6880dacd711e456b90863a9e45256cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.66k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b7a846b47b4787b34e955a65529800"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset wnut_17/wnut_17 (download: 782.18 KiB, generated: 1.66 MiB, post-processed: Unknown size, total: 2.43 MiB) to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a263a4135b7546628725076a6b5352b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/185k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8bcb41ce974ff59905c34af75bef5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ebd881cb3440f482a807572e31bf6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/66.9k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee40c89dae24138bb25e5f5e93b80cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1af200224bda494bad8da9e2cc4ca504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77006c838c6a47adb1293236af3b8fd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b39760b61ee4d0d8fcafcecbe12d80c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b237fe5563c04b71b8291dfe35ec7ac9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset wnut_17 downloaded and prepared to /root/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b8bb0958254052b3fe599ac2287b65"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 3394\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 1009\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'ner_tags'],\n","        num_rows: 1287\n","    })\n","})\n","{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}]},{"cell_type":"code","source":["label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n","print(label_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7baLI9-ZeIcI","executionInfo":{"status":"ok","timestamp":1677530028368,"user_tz":300,"elapsed":6,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"513981f1-0294-46c0-e316-84039ec321c0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']\n"]}]},{"cell_type":"markdown","source":["For the NER task, the input in our JSON files should be labeled"],"metadata":{"id":"yi3374Mxft5s"}},{"cell_type":"code","source":["named_labels = []\n","for i in range(len(dataset[\"train\"])):\n","  named_labels_i = [label_list[label_idx] for label_idx in dataset[\"train\"][i][\"ner_tags\"]]\n","  named_labels.append(named_labels_i)\n","\n","eval_named_labels = []\n","for i in range(len(dataset[\"validation\"])):\n","  named_labels_i = [label_list[label_idx] for label_idx in dataset[\"validation\"][i][\"ner_tags\"]]\n","  eval_named_labels.append(named_labels_i)\n","\n","dataset[\"train\"] = dataset[\"train\"].add_column(\"named_ner_tags\", named_labels)\n","dataset[\"validation\"] = dataset[\"validation\"].add_column(\"named_ner_tags\", eval_named_labels)\n","dataset[\"train\"] = dataset[\"train\"].remove_columns(\"ner_tags\")\n","dataset[\"validation\"] = dataset[\"validation\"].remove_columns(\"ner_tags\")"],"metadata":{"id":"zBZCHbeDfjPh","executionInfo":{"status":"ok","timestamp":1677530028942,"user_tz":300,"elapsed":578,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["dataset[\"train\"].to_json(\"./wnut_17-train.json\")\n","dataset[\"validation\"].to_json(\"./wnut_17-validation.json\")"],"metadata":{"id":"CTwUYc913uDB","colab":{"base_uri":"https://localhost:8080/","height":99,"referenced_widgets":["b3dfc9c9954247beababb1e5a86b3c7a","ec06b0ccebaf4eaa954f0affc8976a5b","d2639c3fc91a47e98d3df86f7d5a3d33","a0bbd8e91e6b48f1a4e95a68ce1e0c60","2e8671b57210445fb53401024d578056","9bc72c3a3d254023ba1c124ee111c068","cff2d3918ad34edb90751646afc0bebe","ebf993cd61c143dbbaa2871aaa1847d3","7cf2f3c7cbbe451681897ed80e011d3c","39c54b7ebd504452a8dc98a8f0e0d036","ccbea320a6e04e248e3ded82a04a9dc4","27c66986ebff4a0c8509cb13ebc7d46d","ea8f89b213a2415882565a05a97e6049","fdab62c990e246888a059a5c3381d53f","ca88778193694d1daaa8aff3daa11794","774a33562150432f8aa0ee4e436f5ca9","4d7deba052de41f8aabd4d057dc912d5","19abac1675b34fb7a15983e0449dc417","d6331398b17e46bdbf4a5bc13a2071dc","126c7f21c8d347f3980ed29483af5878","32ea139c34c24e8f82522b990e0415ab","68deab1ef9d24e1ebf703e5da74fb35e"]},"executionInfo":{"status":"ok","timestamp":1677530029424,"user_tz":300,"elapsed":485,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"a003fda4-765d-40af-cd74-e0c7a1fe0b87"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3dfc9c9954247beababb1e5a86b3c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27c66986ebff4a0c8509cb13ebc7d46d"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["221196"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["We can see that the data is a JSON file with `tokens` and `named_ner_tags`. \n","\n","As described above, the token classification trianing pipeline expects the input sequences to be a lists of words with a tag for each word.\n","\n","When passing the dataset via local files, the tags must be strings representing each class."],"metadata":{"id":"C5O2haE73_my"}},{"cell_type":"code","source":["!head ./wnut_17-train.json --lines=5"],"metadata":{"id":"b2kMBxV134ii","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677530029424,"user_tz":300,"elapsed":6,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"8cd01e05-ff1c-4dad-b3b6-a44076f160e4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"id\":\"0\",\"tokens\":[\"@paulwalk\",\"It\",\"'s\",\"the\",\"view\",\"from\",\"where\",\"I\",\"'m\",\"living\",\"for\",\"two\",\"weeks\",\".\",\"Empire\",\"State\",\"Building\",\"=\",\"ESB\",\".\",\"Pretty\",\"bad\",\"storm\",\"here\",\"last\",\"evening\",\".\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-location\",\"I-location\",\"I-location\",\"O\",\"B-location\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"1\",\"tokens\":[\"From\",\"Green\",\"Newsfeed\",\":\",\"AHFA\",\"extends\",\"deadline\",\"for\",\"Sage\",\"Award\",\"to\",\"Nov\",\".\",\"5\",\"http:\\/\\/tinyurl.com\\/24agj38\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"B-group\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"2\",\"tokens\":[\"Pxleyes\",\"Top\",\"50\",\"Photography\",\"Contest\",\"Pictures\",\"of\",\"August\",\"2010\",\"...\",\"http:\\/\\/bit.ly\\/bgCyZ0\",\"#photography\"],\"named_ner_tags\":[\"B-corporation\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"3\",\"tokens\":[\"today\",\"is\",\"my\",\"last\",\"day\",\"at\",\"the\",\"office\",\".\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"4\",\"tokens\":[\"4Dbling\",\"'s\",\"place\",\"til\",\"monday\",\",\",\"party\",\"party\",\"party\",\".\",\"&lt;\",\"3\"],\"named_ner_tags\":[\"B-person\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n"]}]},{"cell_type":"code","source":["!head ./wnut_17-validation.json --lines=5"],"metadata":{"id":"9JdeJqQC3-SE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677530030345,"user_tz":300,"elapsed":925,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"6cdfa2f9-d0ae-43b9-aeaf-0bf75c202d27"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"id\":\"0\",\"tokens\":[\"Stabilized\",\"approach\",\"or\",\"not\",\"?\",\"That\",\"\\u00b4\",\"s\",\"insane\",\"and\",\"good\",\".\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"1\",\"tokens\":[\"You\",\"should\",\"'\",\"ve\",\"stayed\",\"on\",\"Redondo\",\"Beach\",\"Blvd\",\".\",\"you\",\"were\",\"in\",\"the\",\"borderlines\",\"of\",\"Gardena\",\"\\/\",\"Compton\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-location\",\"I-location\",\"I-location\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-location\",\"O\",\"B-location\"]}\n","{\"id\":\"2\",\"tokens\":[\"All\",\"I\",\"'\",\"ve\",\"been\",\"doing\",\"is\",\"BINGE\",\"watching\",\"Rick\",\"and\",\"Morty\",\"\\ud83d\\ude02\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-creative-work\",\"I-creative-work\",\"I-creative-work\",\"O\"]}\n","{\"id\":\"3\",\"tokens\":[\"wow\",\"emma\",\"and\",\"kaite\",\"is\",\"so\",\"very\",\"cute\",\"and\",\"so\",\"funny\",\"\\ud83d\\ude00\",\"\\ud83d\\ude00\",\"\\ud83d\\ude00\",\"\\ud83d\\ude17\",\"\\ud83d\\ude18\",\"i\",\"wish\",\"im\",\"ryan\",\"\\ud83d\\ude2d\",\"\\ud83d\\ude2d\",\"\\ud83d\\ude2d\"],\"named_ner_tags\":[\"O\",\"B-person\",\"O\",\"B-person\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"O\",\"B-person\",\"O\",\"O\",\"O\"]}\n","{\"id\":\"4\",\"tokens\":[\"THIS\",\"IS\",\"SO\",\"GOOD\"],\"named_ner_tags\":[\"O\",\"O\",\"O\",\"O\"]}\n"]}]},{"cell_type":"markdown","source":["#### **Kick off Training**\n","\n","To use the local files with the CLI, pass `--train_file ./wnut_17-train.csv --validation_file ./wnut_17-validation.csv  --text_column_name tokens --label_column_name named_ner_tags`.\n","\n","Run the following:"],"metadata":{"id":"D70YemqF3_D2"}},{"cell_type":"code","source":["!sparseml.transformers.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --distill_teacher disable \\\n","  --train_file wnut_17-train.json --validation_file wnut_17-validation.json \\\n","  --text_column_name tokens --label_column_name named_ner_tags \\\n","  --output_dir sparse_bert-token_classification_wnut_17_from_json \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1"],"metadata":{"id":"HqDbMuS74YaP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677530643467,"user_tz":300,"elapsed":613124,"user":{"displayName":"Robert Shaw","userId":"07936720368498181527"}},"outputId":"bd1b23f8-4aa4-4077-cb48-0c7885e0088c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0730], device='cuda:0'), zero_point=tensor([170], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.39310073852539, max_val=6.219925403594971)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3610970675945282, max_val=0.32472875714302063)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.688292503356934, max_val=8.927536010742188)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1111], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.382026672363281, max_val=14.166157722473145)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5108], device='cuda:0'), zero_point=tensor([91], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-227.7598876953125, max_val=412.4864501953125)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0692142248153687)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0306], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9015021324157715, max_val=3.7712337970733643)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0236], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.000190496444702, max_val=3.0245020389556885)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0232], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.955418825149536, max_val=2.956886053085327)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0035], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3687712550163269, max_val=0.44248926639556885)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2932], device='cuda:0'), zero_point=tensor([209], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-61.252159118652344, max_val=13.525580406188965)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2616621255874634, max_val=0.9957215785980225)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2069], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=52.58698272705078)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9779436588287354, max_val=2.3392081260681152)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.684203147888184, max_val=3.8017477989196777)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6225888133049011, max_val=0.63664710521698)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.684203147888184, max_val=3.8017477989196777)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5072830319404602, max_val=0.4948429763317108)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0568], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.684203147888184, max_val=3.8017477989196777)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4053649604320526, max_val=0.4026859998703003)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0555], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.324244022369385, max_val=6.835560321807861)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1991], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.593618392944336, max_val=25.38206672668457)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4029], device='cuda:0'), zero_point=tensor([176], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-598.7581176757812, max_val=268.98138427734375)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0611135959625244)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0275], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.502840518951416, max_val=3.383826494216919)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0201], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.6145830154418945, max_val=2.5219016075134277)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.552744150161743, max_val=2.4468419551849365)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3291270136833191, max_val=0.36732470989227295)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1944], device='cuda:0'), zero_point=tensor([190], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-36.92912292480469, max_val=12.644780158996582)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0131], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6694386005401611, max_val=1.473630428314209)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3097], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=78.80287170410156)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.066741943359375, max_val=0.9026337265968323)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0528], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.92583179473877, max_val=3.5394976139068604)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7855653762817383, max_val=0.7524884939193726)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0528], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.92583179473877, max_val=3.5394976139068604)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5414267778396606, max_val=0.5416337251663208)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0528], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.92583179473877, max_val=3.5394976139068604)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.38125964999198914, max_val=0.39517998695373535)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0512], device='cuda:0'), zero_point=tensor([121], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.1857099533081055, max_val=6.867681980133057)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1426], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.252934455871582, max_val=18.1817626953125)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.5927], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-410.8454895019531, max_val=505.30401611328125)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0577160120010376)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0333], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.114287853240967, max_val=4.248649597167969)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0270], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1742608547210693, max_val=3.7054710388183594)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0263], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.0957460403442383, max_val=3.620753765106201)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.673640787601471, max_val=0.555439293384552)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0699], device='cuda:0'), zero_point=tensor([203], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.171676635742188, max_val=3.665248155593872)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4694209694862366, max_val=0.5183411836624146)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=4.207048416137695)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0162], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.0640525817871094, max_val=1.7571440935134888)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): QuantWrapper(\n","    (quant): QuantStub(\n","      (activation_post_process): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0334], device='cuda:0'), zero_point=tensor([185], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.19325590133667, max_val=2.3347363471984863)\n","      )\n","    )\n","    (dequant): DeQuantStub()\n","    (module): Linear(\n","      in_features=768, out_features=13, bias=True\n","      (weight_fake_quant): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0009], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.10971960425376892, max_val=0.09273286908864975)\n","      )\n","      (activation_post_process): Identity()\n","    )\n","  )\n",")\n"," 92% 1284/1391 [08:28<00:48,  2.21it/s][INFO|trainer.py:725] 2023-02-27 20:42:39,695 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, named_ner_tags, tokens. If id, named_ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2916] 2023-02-27 20:42:39,699 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2918] 2023-02-27 20:42:39,699 >>   Num examples = 1009\n","[INFO|trainer.py:2921] 2023-02-27 20:42:39,699 >>   Batch size = 32\n","\n","  0% 0/32 [00:00<?, ?it/s]\u001b[A\n","  6% 2/32 [00:00<00:03,  9.40it/s]\u001b[A\n","  9% 3/32 [00:00<00:04,  6.59it/s]\u001b[A\n"," 12% 4/32 [00:00<00:05,  5.56it/s]\u001b[A\n"," 16% 5/32 [00:00<00:05,  4.77it/s]\u001b[A\n"," 19% 6/32 [00:01<00:05,  4.44it/s]\u001b[A\n"," 22% 7/32 [00:01<00:05,  4.35it/s]\u001b[A\n"," 25% 8/32 [00:01<00:05,  4.20it/s]\u001b[A\n"," 28% 9/32 [00:01<00:05,  4.02it/s]\u001b[A\n"," 31% 10/32 [00:02<00:05,  4.02it/s]\u001b[A\n"," 34% 11/32 [00:02<00:05,  3.83it/s]\u001b[A\n"," 38% 12/32 [00:02<00:05,  3.83it/s]\u001b[A\n"," 41% 13/32 [00:03<00:05,  3.76it/s]\u001b[A\n"," 44% 14/32 [00:03<00:04,  3.71it/s]\u001b[A\n"," 47% 15/32 [00:03<00:04,  3.66it/s]\u001b[A\n"," 50% 16/32 [00:03<00:04,  3.58it/s]\u001b[A\n"," 53% 17/32 [00:04<00:04,  3.52it/s]\u001b[A\n"," 56% 18/32 [00:04<00:04,  3.48it/s]\u001b[A\n"," 59% 19/32 [00:04<00:03,  3.31it/s]\u001b[A\n"," 62% 20/32 [00:05<00:03,  3.19it/s]\u001b[A\n"," 66% 21/32 [00:05<00:03,  3.19it/s]\u001b[A\n"," 69% 22/32 [00:05<00:03,  3.16it/s]\u001b[A\n"," 72% 23/32 [00:06<00:02,  3.17it/s]\u001b[A\n"," 75% 24/32 [00:06<00:02,  3.12it/s]\u001b[A\n"," 78% 25/32 [00:06<00:02,  3.21it/s]\u001b[A\n"," 81% 26/32 [00:07<00:01,  3.02it/s]\u001b[A\n"," 84% 27/32 [00:07<00:01,  3.42it/s]\u001b[A\n"," 88% 28/32 [00:07<00:01,  3.63it/s]\u001b[A\n"," 91% 29/32 [00:07<00:00,  3.98it/s]\u001b[A\n"," 94% 30/32 [00:07<00:00,  4.23it/s]\u001b[A\n"," 97% 31/32 [00:08<00:00,  4.39it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.30031201243400574, 'eval_precision': 0.6072992700729927, 'eval_recall': 0.49760765550239233, 'eval_f1': 0.547008547008547, 'eval_accuracy': 0.9509186852311018, 'eval_runtime': 8.8577, 'eval_samples_per_second': 113.912, 'eval_steps_per_second': 3.613, 'epoch': 12.0}\n"," 92% 1284/1391 [08:37<00:48,  2.21it/s]\n","100% 32/32 [00:08<00:00,  4.56it/s]\u001b[A\n","                                   \u001b[A[INFO|trainer.py:2665] 2023-02-27 20:42:48,558 >> Saving model checkpoint to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284\n","[INFO|configuration_utils.py:447] 2023-02-27 20:42:48,559 >> Configuration saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/config.json\n","[INFO|modeling_utils.py:1624] 2023-02-27 20:42:49,895 >> Model weights saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2123] 2023-02-27 20:42:49,898 >> tokenizer config file saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2130] 2023-02-27 20:42:49,898 >> Special tokens file saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/special_tokens_map.json\n","2023-02-27 20:42:49 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/recipe.yaml\n","INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284/recipe.yaml\n","[INFO|trainer.py:2743] 2023-02-27 20:42:53,520 >> Deleting older checkpoint [sparse_bert-token_classification_wnut_17_from_json/checkpoint-1177] due to args.save_total_limit\n","2023-02-27 20:42:53 sparseml.transformers.sparsification.trainer INFO     BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(\n","        30522, 768, padding_idx=0\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([145], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.047967553138733, max_val=0.7927184700965881)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.62989342212677, max_val=1.2066916227340698)\n","        )\n","      )\n","      (position_embeddings): Embedding(\n","        512, 768\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8870182037353516, max_val=0.8217238187789917)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8878606557846069, max_val=0.8214864730834961)\n","        )\n","      )\n","      (token_type_embeddings): Embedding(\n","        2, 768\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([139], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1226457357406616, max_val=0.9414034485816956)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1188933849334717, max_val=0.9432852268218994)\n","        )\n","      )\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0078], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0001661777496338, max_val=0.879840612411499)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0068], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8689425587654114, max_val=0.8163748979568481)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47256985306739807, max_val=0.5123310089111328)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0590], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.796743392944336, max_val=7.244029998779297)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.149595260620117, max_val=8.741640090942383)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3134], device='cuda:0'), zero_point=tensor([81], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-106.61444854736328, max_val=228.30081176757812)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706408023834229)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.741482734680176, max_val=5.243365287780762)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0399], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.330724239349365, max_val=4.842959403991699)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0389], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.200305461883545, max_val=4.726191520690918)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5881478786468506, max_val=0.8590558767318726)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2132], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-40.059226989746094, max_val=14.29853343963623)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0097], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2329126596450806, max_val=1.1176681518554688)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3559], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=90.57913970947266)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0254], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.2406485080718994, max_val=0.9299792051315308)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5766846537590027, max_val=0.5081972479820251)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5830798149108887, max_val=0.5679207444190979)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0030], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3400424122810364, max_val=0.38109922409057617)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0565], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.966754913330078, max_val=7.4320068359375)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.196357250213623, max_val=7.61046838760376)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8922], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-106.58860778808594, max_val=120.91094970703125)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0705968141555786)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0370], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.183338642120361, max_val=4.7166428565979)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0302], device='cuda:0'), zero_point=tensor([93], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.7964162826538086, max_val=4.892488479614258)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0296], device='cuda:0'), zero_point=tensor([91], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.6964786052703857, max_val=4.840529441833496)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49180564284324646, max_val=0.517935037612915)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2087], device='cuda:0'), zero_point=tensor([229], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-47.8306884765625, max_val=5.384956359863281)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8974893689155579, max_val=0.8359208106994629)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2342], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=59.552886962890625)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.270599126815796, max_val=1.194521188735962)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6264148950576782, max_val=0.6682137250900269)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5783138275146484, max_val=0.6104357242584229)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6071177124977112, max_val=0.4090236723423004)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0847], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.712409019470215, max_val=10.893682479858398)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0834], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.09643268585205, max_val=10.634066581726074)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5424], device='cuda:0'), zero_point=tensor([95], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-241.36492919921875, max_val=406.9468994140625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706632137298584)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0842], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.047092914581299, max_val=10.735162734985352)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0266], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.347923994064331, max_val=3.4384844303131104)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.2531142234802246, max_val=3.2782814502716064)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4091643989086151, max_val=0.40913423895835876)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1860], device='cuda:0'), zero_point=tensor([213], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-39.59893798828125, max_val=7.829713344573975)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0254], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8470697402954102, max_val=3.238227367401123)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1760], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=44.70761489868164)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0292], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7197275161743164, max_val=1.006152868270874)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5235808491706848, max_val=0.5237313508987427)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.70380699634552, max_val=0.7503715753555298)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37008196115493774, max_val=0.4000583291053772)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0585], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.555779933929443, max_val=7.366082668304443)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0791], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.081145286560059, max_val=9.819050788879395)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1101], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-104.85411071777344, max_val=178.21640014648438)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070515751838684)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0340], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.2577338218688965, max_val=4.331313133239746)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0269], device='cuda:0'), zero_point=tensor([135], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6307578086853027, max_val=3.2320778369903564)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([135], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5345218181610107, max_val=3.136894464492798)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3701619505882263, max_val=0.3742188811302185)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5753], device='cuda:0'), zero_point=tensor([236], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-135.55990600585938, max_val=11.14618968963623)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5023609399795532, max_val=1.1005809307098389)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2892], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=73.58505249023438)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0510], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.497493267059326, max_val=0.7756859064102173)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5001065731048584, max_val=0.46389129757881165)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6272882223129272, max_val=0.6635326147079468)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3495381772518158, max_val=0.41613954305648804)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0616], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.227754592895508, max_val=7.468563079833984)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0791], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.081523895263672, max_val=8.604223251342773)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9593], device='cuda:0'), zero_point=tensor([102], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-98.11341857910156, max_val=146.4958953857422)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070567011833191)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0449], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.184720516204834, max_val=5.722867012023926)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.947268486022949, max_val=4.888874053955078)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0374], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.799136161804199, max_val=4.744130611419678)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5237293243408203, max_val=0.4048847258090973)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8509], device='cuda:0'), zero_point=tensor([231], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-196.5778350830078, max_val=20.412586212158203)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7536264657974243, max_val=0.7162963151931763)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1827], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=46.42865753173828)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0459], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.857156753540039, max_val=1.743543028831482)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47966063022613525, max_val=0.46632400155067444)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5950517654418945, max_val=0.5917634963989258)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35314735770225525, max_val=0.39346784353256226)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.47424602508545, max_val=8.278877258300781)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0819], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.836258888244629, max_val=10.443766593933105)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2400], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-141.7823944091797, max_val=174.41354370117188)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0702564716339111)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0448], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.3939433097839355, max_val=5.7085041999816895)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0328], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.266786098480225, max_val=4.10744047164917)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.147034645080566, max_val=3.978872299194336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5388218760490417, max_val=0.3835964798927307)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5509], device='cuda:0'), zero_point=tensor([219], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-120.85987854003906, max_val=19.622455596923828)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7536364793777466, max_val=0.830013632774353)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2775], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=70.60112762451172)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0430], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.486295700073242, max_val=2.9108574390411377)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5570409893989563, max_val=0.494412362575531)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5489100217819214, max_val=0.6205227375030518)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0027], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3476809859275818, max_val=0.3489348888397217)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0726], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.268080711364746, max_val=9.243149757385254)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0941], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.772133827209473, max_val=12.003456115722656)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6935], device='cuda:0'), zero_point=tensor([112], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-189.57237243652344, max_val=242.26821899414062)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0705034732818604)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0398], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.0787811279296875, max_val=4.83437967300415)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0300], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9349193572998047, max_val=3.727266311645508)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0293], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.858894109725952, max_val=3.619227409362793)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32991597056388855, max_val=0.3917962312698364)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4990], device='cuda:0'), zero_point=tensor([216], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-107.92817687988281, max_val=19.319778442382812)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5665174722671509, max_val=0.5297175645828247)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1649], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=41.8827018737793)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0803], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.234334945678711, max_val=1.868071436882019)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46532678604125977, max_val=0.5295001268386841)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6924676895141602, max_val=0.6950094699859619)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3298262357711792, max_val=0.33536869287490845)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0702], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.159579277038574, max_val=8.736872673034668)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1191], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.086130142211914, max_val=15.191490173339844)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5734], device='cuda:0'), zero_point=tensor([59], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-93.33428192138672, max_val=307.89373779296875)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706148147583008)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0344], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.3822455406188965, max_val=3.9834303855895996)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0216], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.823352575302124, max_val=2.697180986404419)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0210], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.7319581508636475, max_val=2.63169264793396)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0027], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32098618149757385, max_val=0.340371310710907)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4690], device='cuda:0'), zero_point=tensor([237], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-111.38723754882812, max_val=8.219098091125488)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6084523797035217, max_val=0.5956369042396545)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1143], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=28.98219871520996)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0247], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.149618625640869, max_val=1.183968186378479)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5705478191375732, max_val=0.6052671670913696)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6530299186706543, max_val=0.686613917350769)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32677072286605835, max_val=0.3034262955188751)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.953970909118652, max_val=8.056598663330078)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1250], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.984609603881836, max_val=15.93139934539795)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8164], device='cuda:0'), zero_point=tensor([82], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-149.3500213623047, max_val=313.81939697265625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070183277130127)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0364], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.645397186279297, max_val=4.246537208557129)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0269], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.495102882385254, max_val=3.3765645027160645)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0261], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.352031946182251, max_val=3.3089637756347656)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4144647717475891, max_val=0.3590836226940155)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3712], device='cuda:0'), zero_point=tensor([232], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-86.22743225097656, max_val=8.426344871520996)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0294268131256104, max_val=0.9667003750801086)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1340], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=33.99042510986328)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0382], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.872303485870361, max_val=2.1298716068267822)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0064], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5414838790893555, max_val=0.8177276849746704)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.747712254524231, max_val=0.6952529549598694)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36119937896728516, max_val=0.32470905780792236)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0694], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.738883018493652, max_val=8.950281143188477)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1111], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.386503219604492, max_val=14.160526275634766)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5202], device='cuda:0'), zero_point=tensor([91], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-228.35604858398438, max_val=414.29632568359375)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0685787200927734)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0304], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.873054265975952, max_val=3.7511823177337646)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.011873483657837, max_val=3.030702590942383)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0231], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.949706554412842, max_val=2.9452056884765625)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0035], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36888542771339417, max_val=0.44252169132232666)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2933], device='cuda:0'), zero_point=tensor([209], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-61.20768356323242, max_val=13.59226131439209)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2616539001464844, max_val=0.9957075119018555)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2067], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=52.54682540893555)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9779481887817383, max_val=2.3393166065216064)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6224795579910278, max_val=0.63664710521698)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.507281482219696, max_val=0.49493589997291565)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4055001139640808, max_val=0.40255290269851685)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0556], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.345387935638428, max_val=6.825032711029053)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1993], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.610374450683594, max_val=25.413562774658203)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4063], device='cuda:0'), zero_point=tensor([176], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-599.65576171875, max_val=268.961181640625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0624542236328125)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0272], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4701437950134277, max_val=3.427612066268921)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0202], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.6111936569213867, max_val=2.5322837829589844)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5470898151397705, max_val=2.4549970626831055)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32924413681030273, max_val=0.3673253059387207)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1933], device='cuda:0'), zero_point=tensor([190], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-36.729190826416016, max_val=12.566998481750488)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0131], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6694059371948242, max_val=1.473612666130066)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3084], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=78.46812438964844)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.066703796386719, max_val=0.9027432203292847)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7855578660964966, max_val=0.7523887753486633)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.541408360004425, max_val=0.5416498184204102)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3812614679336548, max_val=0.3951895534992218)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0514], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.186602592468262, max_val=6.9253692626953125)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1426], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.207953453063965, max_val=18.181306838989258)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.5825], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-408.899658203125, max_val=504.6407165527344)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0567964315414429)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0335], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.137330532073975, max_val=4.269564628601074)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0271], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1987173557281494, max_val=3.706916332244873)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0264], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1253299713134766, max_val=3.610041856765747)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6735051870346069, max_val=0.5553902387619019)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0694], device='cuda:0'), zero_point=tensor([203], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.076272010803223, max_val=3.6325769424438477)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4694751501083374, max_val=0.5183411836624146)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=4.213618755340576)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0162], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.064054012298584, max_val=1.7572755813598633)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): QuantWrapper(\n","    (quant): QuantStub(\n","      (activation_post_process): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0331], device='cuda:0'), zero_point=tensor([185], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.120166301727295, max_val=2.3166956901550293)\n","      )\n","    )\n","    (dequant): DeQuantStub()\n","    (module): Linear(\n","      in_features=768, out_features=13, bias=True\n","      (weight_fake_quant): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0009], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11016236245632172, max_val=0.09309341013431549)\n","      )\n","      (activation_post_process): Identity()\n","    )\n","  )\n",")\n","INFO:sparseml.transformers.sparsification.trainer:BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(\n","        30522, 768, padding_idx=0\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([145], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.047967553138733, max_val=0.7927184700965881)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0128], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.62989342212677, max_val=1.2066916227340698)\n","        )\n","      )\n","      (position_embeddings): Embedding(\n","        512, 768\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8870182037353516, max_val=0.8217238187789917)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8878606557846069, max_val=0.8214864730834961)\n","        )\n","      )\n","      (token_type_embeddings): Embedding(\n","        2, 768\n","        (activation_post_process): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([139], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1226457357406616, max_val=0.9414034485816956)\n","        )\n","        (weight_fake_quant): FakeQuantize(\n","          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0088], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.1188933849334717, max_val=0.9432852268218994)\n","        )\n","      )\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0078], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0001661777496338, max_val=0.879840612411499)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0068], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8689425587654114, max_val=0.8163748979568481)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0470], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.848331451416016, max_val=3.1389319896698)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47256985306739807, max_val=0.5123310089111328)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0590], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.796743392944336, max_val=7.244029998779297)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.149595260620117, max_val=8.741640090942383)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.3134], device='cuda:0'), zero_point=tensor([81], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-106.61444854736328, max_val=228.30081176757812)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706408023834229)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0450], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.741482734680176, max_val=5.243365287780762)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0399], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.330724239349365, max_val=4.842959403991699)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0389], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.200305461883545, max_val=4.726191520690918)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5881478786468506, max_val=0.8590558767318726)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2132], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-40.059226989746094, max_val=14.29853343963623)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0097], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2329126596450806, max_val=1.1176681518554688)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3559], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=90.57913970947266)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0254], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.2406485080718994, max_val=0.9299792051315308)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5766846537590027, max_val=0.5081972479820251)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5830798149108887, max_val=0.5679207444190979)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0572], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.764806747436523, max_val=4.816766262054443)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0030], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3400424122810364, max_val=0.38109922409057617)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0565], device='cuda:0'), zero_point=tensor([123], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.966754913330078, max_val=7.4320068359375)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0597], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.196357250213623, max_val=7.61046838760376)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8922], device='cuda:0'), zero_point=tensor([119], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-106.58860778808594, max_val=120.91094970703125)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0705968141555786)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0370], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.183338642120361, max_val=4.7166428565979)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0302], device='cuda:0'), zero_point=tensor([93], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.7964162826538086, max_val=4.892488479614258)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0296], device='cuda:0'), zero_point=tensor([91], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.6964786052703857, max_val=4.840529441833496)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.49180564284324646, max_val=0.517935037612915)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2087], device='cuda:0'), zero_point=tensor([229], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-47.8306884765625, max_val=5.384956359863281)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8974893689155579, max_val=0.8359208106994629)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2342], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=59.552886962890625)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0257], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.270599126815796, max_val=1.194521188735962)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6264148950576782, max_val=0.6682137250900269)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5783138275146484, max_val=0.6104357242584229)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0772], device='cuda:0'), zero_point=tensor([196], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.164734840393066, max_val=4.530308723449707)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6071177124977112, max_val=0.4090236723423004)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0847], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.712409019470215, max_val=10.893682479858398)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0834], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.09643268585205, max_val=10.634066581726074)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5424], device='cuda:0'), zero_point=tensor([95], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-241.36492919921875, max_val=406.9468994140625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706632137298584)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0842], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.047092914581299, max_val=10.735162734985352)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0266], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.347923994064331, max_val=3.4384844303131104)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0256], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.2531142234802246, max_val=3.2782814502716064)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4091643989086151, max_val=0.40913423895835876)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1860], device='cuda:0'), zero_point=tensor([213], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-39.59893798828125, max_val=7.829713344573975)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0254], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8470697402954102, max_val=3.238227367401123)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1760], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=44.70761489868164)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0292], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.7197275161743164, max_val=1.006152868270874)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5235808491706848, max_val=0.5237313508987427)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.70380699634552, max_val=0.7503715753555298)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0766], device='cuda:0'), zero_point=tensor([182], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.92369556427002, max_val=5.603531837463379)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.37008196115493774, max_val=0.4000583291053772)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0585], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.555779933929443, max_val=7.366082668304443)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0791], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.081145286560059, max_val=9.819050788879395)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.1101], device='cuda:0'), zero_point=tensor([94], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-104.85411071777344, max_val=178.21640014648438)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070515751838684)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0340], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.2577338218688965, max_val=4.331313133239746)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0269], device='cuda:0'), zero_point=tensor([135], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.6307578086853027, max_val=3.2320778369903564)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0262], device='cuda:0'), zero_point=tensor([135], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.5345218181610107, max_val=3.136894464492798)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3701619505882263, max_val=0.3742188811302185)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5753], device='cuda:0'), zero_point=tensor([236], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-135.55990600585938, max_val=11.14618968963623)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0118], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5023609399795532, max_val=1.1005809307098389)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2892], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=73.58505249023438)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0510], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.497493267059326, max_val=0.7756859064102173)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0039], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5001065731048584, max_val=0.46389129757881165)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6272882223129272, max_val=0.6635326147079468)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0638], device='cuda:0'), zero_point=tensor([164], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.448723793029785, max_val=5.818667888641357)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3495381772518158, max_val=0.41613954305648804)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0616], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.227754592895508, max_val=7.468563079833984)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0791], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.081523895263672, max_val=8.604223251342773)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.9593], device='cuda:0'), zero_point=tensor([102], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-98.11341857910156, max_val=146.4958953857422)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070567011833191)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0449], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.184720516204834, max_val=5.722867012023926)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0386], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.947268486022949, max_val=4.888874053955078)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0374], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.799136161804199, max_val=4.744130611419678)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5237293243408203, max_val=0.4048847258090973)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.8509], device='cuda:0'), zero_point=tensor([231], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-196.5778350830078, max_val=20.412586212158203)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7536264657974243, max_val=0.7162963151931763)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1827], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=46.42865753173828)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0459], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.857156753540039, max_val=1.743543028831482)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0038], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.47966063022613525, max_val=0.46632400155067444)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5950517654418945, max_val=0.5917634963989258)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([156], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.265321731567383, max_val=6.487215995788574)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.35314735770225525, max_val=0.39346784353256226)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0657], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.47424602508545, max_val=8.278877258300781)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0819], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.836258888244629, max_val=10.443766593933105)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.2400], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-141.7823944091797, max_val=174.41354370117188)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0702564716339111)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0448], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.3939433097839355, max_val=5.7085041999816895)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0328], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.266786098480225, max_val=4.10744047164917)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0319], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.147034645080566, max_val=3.978872299194336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5388218760490417, max_val=0.3835964798927307)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.5509], device='cuda:0'), zero_point=tensor([219], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-120.85987854003906, max_val=19.622455596923828)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7536364793777466, max_val=0.830013632774353)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2775], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=70.60112762451172)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0430], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.486295700073242, max_val=2.9108574390411377)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5570409893989563, max_val=0.494412362575531)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5489100217819214, max_val=0.6205227375030518)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0691], device='cuda:0'), zero_point=tensor([161], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.119749069213867, max_val=6.509830951690674)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0027], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3476809859275818, max_val=0.3489348888397217)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0726], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.268080711364746, max_val=9.243149757385254)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0941], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.772133827209473, max_val=12.003456115722656)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.6935], device='cuda:0'), zero_point=tensor([112], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-189.57237243652344, max_val=242.26821899414062)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0705034732818604)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0398], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.0787811279296875, max_val=4.83437967300415)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0300], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9349193572998047, max_val=3.727266311645508)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0293], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.858894109725952, max_val=3.619227409362793)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32991597056388855, max_val=0.3917962312698364)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4990], device='cuda:0'), zero_point=tensor([216], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-107.92817687988281, max_val=19.319778442382812)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5665174722671509, max_val=0.5297175645828247)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1649], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=41.8827018737793)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0803], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.234334945678711, max_val=1.868071436882019)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46532678604125977, max_val=0.5295001268386841)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6924676895141602, max_val=0.6950094699859619)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0686], device='cuda:0'), zero_point=tensor([159], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.932562828063965, max_val=6.549822807312012)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3298262357711792, max_val=0.33536869287490845)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0702], device='cuda:0'), zero_point=tensor([131], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.159579277038574, max_val=8.736872673034668)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1191], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.086130142211914, max_val=15.191490173339844)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.5734], device='cuda:0'), zero_point=tensor([59], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-93.33428192138672, max_val=307.89373779296875)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0706148147583008)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0344], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.3822455406188965, max_val=3.9834303855895996)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0216], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.823352575302124, max_val=2.697180986404419)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0210], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.7319581508636475, max_val=2.63169264793396)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0027], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32098618149757385, max_val=0.340371310710907)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.4690], device='cuda:0'), zero_point=tensor([237], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-111.38723754882812, max_val=8.219098091125488)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6084523797035217, max_val=0.5956369042396545)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1143], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=28.98219871520996)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0247], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.149618625640869, max_val=1.183968186378479)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5705478191375732, max_val=0.6052671670913696)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6530299186706543, max_val=0.686613917350769)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([171], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.67480754852295, max_val=6.705545425415039)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32677072286605835, max_val=0.3034262955188751)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([134], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.953970909118652, max_val=8.056598663330078)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1250], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.984609603881836, max_val=15.93139934539795)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.8164], device='cuda:0'), zero_point=tensor([82], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-149.3500213623047, max_val=313.81939697265625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.070183277130127)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0364], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.645397186279297, max_val=4.246537208557129)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0269], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.495102882385254, max_val=3.3765645027160645)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0261], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.352031946182251, max_val=3.3089637756347656)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4144647717475891, max_val=0.3590836226940155)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3712], device='cuda:0'), zero_point=tensor([232], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-86.22743225097656, max_val=8.426344871520996)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0294268131256104, max_val=0.9667003750801086)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1340], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=33.99042510986328)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0382], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.872303485870361, max_val=2.1298716068267822)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0064], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5414838790893555, max_val=0.8177276849746704)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0059], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.747712254524231, max_val=0.6952529549598694)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0740], device='cuda:0'), zero_point=tensor([172], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.7013578414917, max_val=6.175769329071045)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36119937896728516, max_val=0.32470905780792236)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0694], device='cuda:0'), zero_point=tensor([126], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.738883018493652, max_val=8.950281143188477)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1111], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.386503219604492, max_val=14.160526275634766)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([2.5202], device='cuda:0'), zero_point=tensor([91], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-228.35604858398438, max_val=414.29632568359375)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0685787200927734)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0304], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.873054265975952, max_val=3.7511823177337646)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0237], device='cuda:0'), zero_point=tensor([127], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.011873483657837, max_val=3.030702590942383)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0231], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.949706554412842, max_val=2.9452056884765625)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0035], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36888542771339417, max_val=0.44252169132232666)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2933], device='cuda:0'), zero_point=tensor([209], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-61.20768356323242, max_val=13.59226131439209)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0099], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.2616539001464844, max_val=0.9957075119018555)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2067], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=52.54682540893555)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0312], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.9779481887817383, max_val=2.3393166065216064)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6224795579910278, max_val=0.63664710521698)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.507281482219696, max_val=0.49493589997291565)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0579], device='cuda:0'), zero_point=tensor([189], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.964104652404785, max_val=3.8085317611694336)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0032], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4055001139640808, max_val=0.40255290269851685)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0556], device='cuda:0'), zero_point=tensor([132], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.345387935638428, max_val=6.825032711029053)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1993], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-19.610374450683594, max_val=25.413562774658203)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.4063], device='cuda:0'), zero_point=tensor([176], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-599.65576171875, max_val=268.961181640625)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0624542236328125)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0272], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.4701437950134277, max_val=3.427612066268921)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0202], device='cuda:0'), zero_point=tensor([129], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.6111936569213867, max_val=2.5322837829589844)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0196], device='cuda:0'), zero_point=tensor([130], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.5470898151397705, max_val=2.4549970626831055)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32924413681030273, max_val=0.3673253059387207)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1933], device='cuda:0'), zero_point=tensor([190], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-36.729190826416016, max_val=12.566998481750488)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0131], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6694059371948242, max_val=1.473612666130066)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.3084], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=78.46812438964844)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0554], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.066703796386719, max_val=0.9027432203292847)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7855578660964966, max_val=0.7523887753486633)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (key): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0042], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.541408360004425, max_val=0.5416498184204102)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (value): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0530], device='cuda:0'), zero_point=tensor([188], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.970175743103027, max_val=3.5350093841552734)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0031], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3812614679336548, max_val=0.3951895534992218)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (attention_scores_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0514], device='cuda:0'), zero_point=tensor([120], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.186602592468262, max_val=6.9253692626953125)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.1426], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.207953453063965, max_val=18.181306838989258)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([3.5825], device='cuda:0'), zero_point=tensor([114], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-408.899658203125, max_val=504.6407165527344)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (context_layer_matmul): QATWrapper(\n","                (forward_fn): QATMatMul()\n","                (input_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0567964315414429)\n","                    )\n","                  )\n","                  (1): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0335], device='cuda:0'), zero_point=tensor([128], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-4.137330532073975, max_val=4.269564628601074)\n","                    )\n","                  )\n","                )\n","                (output_quant_stubs): ModuleList(\n","                  (0): QuantStub(\n","                    (activation_post_process): FakeQuantize(\n","                      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0271], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                      (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1987173557281494, max_val=3.706916332244873)\n","                    )\n","                  )\n","                )\n","                (output_dequant_stubs): ModuleList(\n","                  (0): DeQuantStub()\n","                )\n","              )\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): QuantWrapper(\n","                (quant): QuantStub(\n","                  (activation_post_process): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0264], device='cuda:0'), zero_point=tensor([118], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-3.1253299713134766, max_val=3.610041856765747)\n","                  )\n","                )\n","                (dequant): DeQuantStub()\n","                (module): Linear(\n","                  in_features=768, out_features=768, bias=True\n","                  (weight_fake_quant): FakeQuantize(\n","                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6735051870346069, max_val=0.5553902387619019)\n","                  )\n","                  (activation_post_process): Identity()\n","                )\n","              )\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0694], device='cuda:0'), zero_point=tensor([203], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.076272010803223, max_val=3.6325769424438477)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=768, out_features=3072, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4694751501083374, max_val=0.5183411836624146)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): QuantWrapper(\n","              (quant): QuantStub(\n","                (activation_post_process): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0172], device='cuda:0'), zero_point=tensor([10], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=4.213618755340576)\n","                )\n","              )\n","              (dequant): DeQuantStub()\n","              (module): Linear(\n","                in_features=3072, out_features=768, bias=True\n","                (weight_fake_quant): FakeQuantize(\n","                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0162], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.064054012298584, max_val=1.7572755813598633)\n","                )\n","                (activation_post_process): Identity()\n","              )\n","            )\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): QuantWrapper(\n","    (quant): QuantStub(\n","      (activation_post_process): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0331], device='cuda:0'), zero_point=tensor([185], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.120166301727295, max_val=2.3166956901550293)\n","      )\n","    )\n","    (dequant): DeQuantStub()\n","    (module): Linear(\n","      in_features=768, out_features=13, bias=True\n","      (weight_fake_quant): FakeQuantize(\n","        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0009], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n","        (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.11016236245632172, max_val=0.09309341013431549)\n","      )\n","      (activation_post_process): Identity()\n","    )\n","  )\n",")\n","100% 1391/1391 [09:29<00:00,  2.93it/s][INFO|trainer.py:725] 2023-02-27 20:43:40,998 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, named_ner_tags, tokens. If id, named_ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2916] 2023-02-27 20:43:41,001 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2918] 2023-02-27 20:43:41,001 >>   Num examples = 1009\n","[INFO|trainer.py:2921] 2023-02-27 20:43:41,001 >>   Batch size = 32\n","\n","  0% 0/32 [00:00<?, ?it/s]\u001b[A\n","  6% 2/32 [00:00<00:01, 16.04it/s]\u001b[A\n"," 12% 4/32 [00:00<00:02, 10.69it/s]\u001b[A\n"," 19% 6/32 [00:00<00:02,  9.35it/s]\u001b[A\n"," 25% 8/32 [00:00<00:02,  9.05it/s]\u001b[A\n"," 28% 9/32 [00:00<00:02,  8.70it/s]\u001b[A\n"," 31% 10/32 [00:01<00:02,  8.66it/s]\u001b[A\n"," 34% 11/32 [00:01<00:02,  7.87it/s]\u001b[A\n"," 38% 12/32 [00:01<00:02,  8.10it/s]\u001b[A\n"," 41% 13/32 [00:01<00:02,  8.06it/s]\u001b[A\n"," 44% 14/32 [00:01<00:02,  8.18it/s]\u001b[A\n"," 47% 15/32 [00:01<00:02,  8.30it/s]\u001b[A\n"," 50% 16/32 [00:01<00:01,  8.09it/s]\u001b[A\n"," 53% 17/32 [00:01<00:01,  7.99it/s]\u001b[A\n"," 56% 18/32 [00:02<00:01,  7.94it/s]\u001b[A\n"," 59% 19/32 [00:02<00:01,  7.82it/s]\u001b[A\n"," 62% 20/32 [00:02<00:01,  7.73it/s]\u001b[A\n"," 66% 21/32 [00:02<00:01,  7.96it/s]\u001b[A\n"," 69% 22/32 [00:02<00:01,  7.83it/s]\u001b[A\n"," 72% 23/32 [00:02<00:01,  8.05it/s]\u001b[A\n"," 75% 24/32 [00:02<00:01,  7.18it/s]\u001b[A\n"," 78% 25/32 [00:03<00:00,  7.25it/s]\u001b[A\n"," 81% 26/32 [00:03<00:00,  6.51it/s]\u001b[A\n"," 84% 27/32 [00:03<00:00,  6.95it/s]\u001b[A\n"," 88% 28/32 [00:03<00:00,  6.92it/s]\u001b[A\n"," 94% 30/32 [00:03<00:00,  8.08it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.30526039004325867, 'eval_precision': 0.6259426847662142, 'eval_recall': 0.4964114832535885, 'eval_f1': 0.5537024683122083, 'eval_accuracy': 0.950664377900693, 'eval_runtime': 4.3707, 'eval_samples_per_second': 230.855, 'eval_steps_per_second': 7.321, 'epoch': 13.0}\n","100% 1391/1391 [09:33<00:00,  2.93it/s]\n","100% 32/32 [00:04<00:00,  8.25it/s]\u001b[A\n","                                   \u001b[A[INFO|trainer.py:2665] 2023-02-27 20:43:45,374 >> Saving model checkpoint to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391\n","[INFO|configuration_utils.py:447] 2023-02-27 20:43:45,375 >> Configuration saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/config.json\n","[INFO|modeling_utils.py:1624] 2023-02-27 20:43:46,827 >> Model weights saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2123] 2023-02-27 20:43:46,829 >> tokenizer config file saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2130] 2023-02-27 20:43:46,829 >> Special tokens file saved in sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/special_tokens_map.json\n","2023-02-27 20:43:46 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/recipe.yaml\n","INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/checkpoint-1391/recipe.yaml\n","[INFO|trainer.py:2743] 2023-02-27 20:43:50,336 >> Deleting older checkpoint [sparse_bert-token_classification_wnut_17_from_json/checkpoint-1284] due to args.save_total_limit\n","[INFO|trainer.py:1856] 2023-02-27 20:43:50,608 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 579.1596, 'train_samples_per_second': 76.183, 'train_steps_per_second': 2.402, 'train_loss': 0.08909911752861237, 'epoch': 13.0}\n","100% 1391/1391 [09:39<00:00,  2.40it/s]\n","2023-02-27 20:43:50 sparseml.transformers.sparsification.trainer INFO     Finalized SparseML recipe argument applied to the model\n","INFO:sparseml.transformers.sparsification.trainer:Finalized SparseML recipe argument applied to the model\n","2023-02-27 20:43:50 sparseml.transformers.sparsification.trainer INFO     Sparsification info for /root/.cache/sparsezoo/d9914a7a-fdc4-459c-9268-d6e7aa1833b8/training: 108901645 total params. Of those there are 84944640 prunable params which have 89.98942134547866 avg sparsity.\n","INFO:sparseml.transformers.sparsification.trainer:Sparsification info for /root/.cache/sparsezoo/d9914a7a-fdc4-459c-9268-d6e7aa1833b8/training: 108901645 total params. Of those there are 84944640 prunable params which have 89.98942134547866 avg sparsity.\n","2023-02-27 20:43:50 sparseml.transformers.sparsification.trainer INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 108901645, \"sparse\": 76441190, \"sparsity_percent\": 70.19286990568416, \"prunable\": 84944640, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.98942134547866, \"quantizable\": 85027597, \"quantized\": 85027597, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9312151074409485, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": true}, \"bert.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": true}, \"bert.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": true}, \"bert.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9295671582221985, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.871110737323761, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": true}, \"bert.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": true}, \"bert.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": true}, \"bert.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.868516743183136, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": true}, \"bert.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8958757519721985, \"quantized\": true}, \"bert.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": true}, \"bert.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.892473042011261, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8780840039253235, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": true}, \"bert.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": true}, \"bert.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": true}, \"bert.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066784858704, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442883491516, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": true}, \"bert.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": true}, \"bert.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": true}, \"bert.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.873399555683136, \"quantized\": true}, \"bert.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": true}, \"bert.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": true}, \"bert.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8541887402534485, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 9984, \"sparsity\": 0.0, \"quantized\": true}}}\n","INFO:sparseml.transformers.sparsification.trainer:sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 108901645, \"sparse\": 76441190, \"sparsity_percent\": 70.19286990568416, \"prunable\": 84944640, \"prunable_sparse\": 76441190, \"prunable_sparsity_percent\": 89.98942134547866, \"quantizable\": 85027597, \"quantized\": 85027597, \"quantized_percent\": 100.0}, \"params_info\": {\"bert.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8644917607307434, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8680216670036316, \"quantized\": true}, \"bert.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9312151074409485, \"quantized\": true}, \"bert.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9232262372970581, \"quantized\": true}, \"bert.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9153103232383728, \"quantized\": true}, \"bert.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9356380105018616, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8620554804801941, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8625064492225647, \"quantized\": true}, \"bert.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9279242753982544, \"quantized\": true}, \"bert.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9245097041130066, \"quantized\": true}, \"bert.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8945091962814331, \"quantized\": true}, \"bert.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9265751242637634, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8451063632965088, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8532799482345581, \"quantized\": true}, \"bert.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9295671582221985, \"quantized\": true}, \"bert.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9288228154182434, \"quantized\": true}, \"bert.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8895581364631653, \"quantized\": true}, \"bert.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9237624406814575, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.871110737323761, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8704121708869934, \"quantized\": true}, \"bert.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9085676670074463, \"quantized\": true}, \"bert.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9130028486251831, \"quantized\": true}, \"bert.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8868213295936584, \"quantized\": true}, \"bert.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9209082126617432, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8635711669921875, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8665059208869934, \"quantized\": true}, \"bert.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8824039101600647, \"quantized\": true}, \"bert.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8957400918006897, \"quantized\": true}, \"bert.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8822059631347656, \"quantized\": true}, \"bert.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9172935485839844, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.868516743183136, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8675944209098816, \"quantized\": true}, \"bert.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843333125114441, \"quantized\": true}, \"bert.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8958757519721985, \"quantized\": true}, \"bert.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8838331699371338, \"quantized\": true}, \"bert.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91917884349823, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8699256181716919, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8717482089996338, \"quantized\": true}, \"bert.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.892473042011261, \"quantized\": true}, \"bert.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.9078572392463684, \"quantized\": true}, \"bert.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8827946782112122, \"quantized\": true}, \"bert.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9222526550292969, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792538046836853, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8780840039253235, \"quantized\": true}, \"bert.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8773871660232544, \"quantized\": true}, \"bert.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8886498212814331, \"quantized\": true}, \"bert.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8991622924804688, \"quantized\": true}, \"bert.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9271066784858704, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8583950400352478, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.856842041015625, \"quantized\": true}, \"bert.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8692152500152588, \"quantized\": true}, \"bert.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8843451738357544, \"quantized\": true}, \"bert.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9019758701324463, \"quantized\": true}, \"bert.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9253442883491516, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8518574833869934, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8523983359336853, \"quantized\": true}, \"bert.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8783705234527588, \"quantized\": true}, \"bert.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8867306113243103, \"quantized\": true}, \"bert.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9011484980583191, \"quantized\": true}, \"bert.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.91860032081604, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8570064902305603, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8588087558746338, \"quantized\": true}, \"bert.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.873399555683136, \"quantized\": true}, \"bert.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8768836259841919, \"quantized\": true}, \"bert.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9072990417480469, \"quantized\": true}, \"bert.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9249801635742188, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8541887402534485, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8596123456954956, \"quantized\": true}, \"bert.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8792198896408081, \"quantized\": true}, \"bert.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.8855014443397522, \"quantized\": true}, \"bert.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.8986706137657166, \"quantized\": true}, \"bert.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.9309417009353638, \"quantized\": true}, \"classifier.module.weight\": {\"numel\": 9984, \"sparsity\": 0.0, \"quantized\": true}}}\n","***** train metrics *****\n","  epoch                    =       13.0\n","  train_loss               =     0.0891\n","  train_runtime            = 0:09:39.15\n","  train_samples            =       3394\n","  train_samples_per_second =     76.183\n","  train_steps_per_second   =      2.402\n","[INFO|trainer.py:2665] 2023-02-27 20:43:51,005 >> Saving model checkpoint to sparse_bert-token_classification_wnut_17_from_json\n","[INFO|configuration_utils.py:447] 2023-02-27 20:43:51,006 >> Configuration saved in sparse_bert-token_classification_wnut_17_from_json/config.json\n","[INFO|modeling_utils.py:1624] 2023-02-27 20:43:52,789 >> Model weights saved in sparse_bert-token_classification_wnut_17_from_json/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2123] 2023-02-27 20:43:52,790 >> tokenizer config file saved in sparse_bert-token_classification_wnut_17_from_json/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2130] 2023-02-27 20:43:52,791 >> Special tokens file saved in sparse_bert-token_classification_wnut_17_from_json/special_tokens_map.json\n","2023-02-27 20:43:52 sparseml.transformers.sparsification.trainer INFO     Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/recipe.yaml\n","INFO:sparseml.transformers.sparsification.trainer:Saved SparseML recipe with model state to sparse_bert-token_classification_wnut_17_from_json/recipe.yaml\n","2023-02-27 20:43:56 sparseml.transformers.token_classification INFO     *** Evaluate ***\n","INFO:sparseml.transformers.token_classification:*** Evaluate ***\n","[INFO|trainer.py:725] 2023-02-27 20:43:56,253 >> The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, named_ner_tags, tokens. If id, named_ner_tags, tokens are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2916] 2023-02-27 20:43:56,258 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2918] 2023-02-27 20:43:56,258 >>   Num examples = 1009\n","[INFO|trainer.py:2921] 2023-02-27 20:43:56,258 >>   Batch size = 32\n","100% 32/32 [00:04<00:00,  7.49it/s]\n","***** eval metrics *****\n","  epoch                   =       13.0\n","  eval_accuracy           =     0.9507\n","  eval_f1                 =     0.5541\n","  eval_loss               =     0.3052\n","  eval_precision          =     0.6269\n","  eval_recall             =     0.4964\n","  eval_runtime            = 0:00:04.50\n","  eval_samples            =       1009\n","  eval_samples_per_second =    223.837\n","  eval_steps_per_second   =      7.099\n"]}]}]}