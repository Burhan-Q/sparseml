{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJkdBaD+0/bFboejdsoook"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Token Classification: Sparse Transfer Learning with the CLI**\n","\n","In this example, you will fine-tune a 90% pruned BERT model onto some token classification datasets using SparseML's CLI.\n","\n","### **Sparse Transfer Learning Overview**\n","\n","Sparse Transfer Learning is very similiar to typical fine-tuning you are used to when training models. However, with Sparse Transfer Learning, we start the training process from a pre-sparsified checkpoint and maintain the sparsity structure while the fine tuning occurs. At the end, you will have a sparse model trained on your dataset, ready to be deployed with DeepSparse for GPU-class performance on CPUs!\n","\n","### **Pre-Sparsified BERT**\n","SparseZoo, Neural Magic's open source repository of pre-sparsified models, contains a 90% pruned version of BERT, which has been sparsified on the upstream Wikipedia and BookCorpus datasets with the\n","masked language modeling objective. [Check out the model card](https://sparsezoo.neuralmagic.com/models/nlp%2Fmasked_language_modeling%2Fobert-base%2Fpytorch%2Fhuggingface%2Fwikipedia_bookcorpus%2Fpruned90-none). We will use this model as the starting point for the transfer learning process.\n","\n","\n","***Let's dive in!***"],"metadata":{"id":"kSNEB-3orJ9C"}},{"cell_type":"markdown","source":["## **Installation**\n","\n","Install SparseML via `pip`.\n","\n"],"metadata":{"id":"Y0WybTbssU0g"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Nr5jM2zoqzuG"},"outputs":[],"source":["%pip uninstall torch torchvision -y\n","%pip install sparseml[torch]"]},{"cell_type":"markdown","source":["If you are running on Google Colab, restart the runtime after this step."],"metadata":{"id":"1SlYUvD61ppy"}},{"cell_type":"code","source":["!sparseml.transformers.text_classification --help"],"metadata":{"collapsed":true,"id":"BT_fq1tk2O4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Sparse Transfer Learning with Conll2003**\n","\n","SparseML's CLI enables you to kick-off training workflows with various utilities like dataset loading, checkpoint saving, \n","metric reporting, and logging handled for you. All we have to do is pass a `model_name_or_path` (the starting checkpoint), a `task` (the GLUE task to train on), and a `recipe` (a YAML file specifying the sparsity related parameters) and we are up and running. The `recipes` is critical for instructing the training script how to modify the training process with sparsity related algorithms. For Sparse Transfer Learning, we will use a `recipe` that instructs SparseML to maintain sparsity during the training process and to apply quantization over the final few epochs. "],"metadata":{"id":"vG_qKQcXsfgW"}},{"cell_type":"markdown","source":["### **Run Transfer Learning**\n","\n","For Conll2003, there is a pre-made transfer learning recipe available in [SparseZoo](https://sparsezoo.neuralmagic.com/models/nlp%2Ftoken_classification%2Fobert-base%2Fpytorch%2Fhuggingface%2Fconll2003%2Fpruned90_quant-none). As such, we kick off transfer learning with the following:"],"metadata":{"id":"jM1QJxiMu5T8"}},{"cell_type":"code","source":["!sparseml.transformers.train.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --distill_teacher zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/base-none \\\n","  --dataset_name conll2003 \\\n","  --output_dir sparse_bert-token_classification_conll2003 \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1 \\\n","  --max_train_samples 2000"],"metadata":{"id":"hz8o5CNlsNo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's discuss the key arguments:\n","- `--dataset_name conll2003` instructs SparseML to download and fine-tune onto the Conll2003 dataset. The script automatically downloads the dataset from the Hugging Face hub.\n","\n","- `--model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none` specifies the starting checkpoint for the fine tuning. Here, we passed a SparseZoo stub identifying the 90% pruned version of BERT trained with masked language modeling on the Wikipedia and BookCorpus datasets. SparseML downloads the checkpoint from the Zoo when the script starts.\n","\n","- `--recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none` specifies the recipe to be applied by SparseML. Here, we passed a SparseZoo stub identifying the transfer learning recipe for the Conll2003 dataset. SparseML downloads the recipe from the Zoo when the script starts. See below for the details of what this recipe looks like.\n","\n","- `--distill_teacher zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/base-none` is an optional argument that specifies a model to use for as a teacher to apply distillation during the training process. We passed a SparseZoo stub identifying a dense BERT model trained on Conll2003. SparseML downloads the teacher from the Zoo when the script starts.\n","\n","The script downloads the starting checkpoint, the teacher model, and transfer learning recipe from SparseZoo as well as the Conll2003 dataset and trains the model for 13 epochs, converging to ~98.5% accuracy on the validation set. The final model is quantized with 90% of weights pruned!"],"metadata":{"id":"-HFYMN3yq6fJ"}},{"cell_type":"markdown","source":["#### **Transfer Learning Recipe**\n","\n","Here's what the transfer learning recipe for the Conll2003 dataset looks like.\n","\n","The \"Modifiers\" are the important items that encode how SparseML should modify the training process for Sparse Transfer Learning:\n","- `ConstantPruningModifier` tells SparseML to pin weights at 0 over all epochs, maintaining the sparsity structure of the network\n","- `QuantizationModifier` tells SparseML to quanitze the weights with quantization aware training over the last 5 epochs\n","- `DistillationModifier` tells SparseML how to apply distillation to the model, including the layer and some hyperparameters\n","\n","SparseML parses the modifiers and updates the training process to implement the algorithms and hyperparameters specified in the recipes."],"metadata":{"id":"ZJnGKuXnwhWH"}},{"cell_type":"code","source":["from sparsezoo import Model\n","transfer_stub = \"zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none\"\n","download_dir = \"./transfer_recipe\"\n","zoo_model = Model(transfer_stub, download_path=download_dir)\n","recipe_path = zoo_model.recipes.default.path\n","print(recipe_path)"],"metadata":{"id":"D5Z_a3HOwrpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cat ./transfer_recipe/recipe/recipe_original.md"],"metadata":{"id":"Oeu1PvyYxR0V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Export to ONNX**\n","\n","Once you have trained your model, export to ONNX in order to deploy with DeepSparse. The artifacts of the training process \n","are saved to your local filesystem. \n","\n","Run the following to convert your PyTorch checkpoint to ONNX:"],"metadata":{"id":"bFbppO99xGeT"}},{"cell_type":"code","source":["!sparseml.transformers.export_onnx \\\n","  --model_path ./sparse_bert-token_classification_conll2003 \\\n","  --task text_classification"],"metadata":{"id":"OHKEUvPOx-su"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The script above creates a `deployment` folder in your local directory, which has all of the files needed for deployment with DeepSparse including the `model.onnx`, `config.json`, and `tokenizer.json` files."],"metadata":{"id":"B3JKl3xIyDIe"}},{"cell_type":"markdown","source":["## **Sparse Transfer Learning with a Custom Dataset**\n","\n","Beyond the Conll2003 dataset, we can also use a dataset from the Hugging Face Hub or pass via local files. Let's try an example of each for the sentiment analysis using [WNUT 17](wnut_17), which is also a NER task.\n","\n","For simplicity, we will perform the fine-tuning without distillation. Although the transfer learning recipe contains distillation\n","modifiers, by setting `--distill_teacher disable` we instruct SparseML to skip distillation."],"metadata":{"id":"bCE4HKWvyWRu"}},{"cell_type":"markdown","source":["### **Using a Hugging Face Dataset**\n","\n","Let's walk through how to pass a Hugging Face dataset identifier to the CLI."],"metadata":{"id":"n5ab0yh6yspE"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from pprint import pprint\n","\n","wnut_17 = load_dataset(\"wnut_17\")\n","print(wnut_17)\n","print(wnut_17[\"train\"][0])"],"metadata":{"id":"hfKp5RaMybmt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Kick off Training**\n","\n","To use this dataset with the CLI, we can replace the `--dataset_name conll2003` argument with `--dataset_name wnut_17 --input_column_names tokens --label_column_name label`. SparseML will then download the dataset from the Hugging Face hub and run training as before."],"metadata":{"id":"pdpvTT654LAy"}},{"cell_type":"code","source":["!sparseml.transformers.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}' \\\n","  --distill_teacher disable \\\n","  --dataset_name wnut_17 --text_column_name tokens --label_column_name ner_tags \\\n","  --output_dir sparse_bert-token_classification_wnut_17 \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1"],"metadata":{"id":"GMxgkhMRz-x3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You will notice that we used the same recipe as we did in the Conll2003 case (identified by the SparseZoo stub `zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none`). Since the WNUT and Conll2003 tasks are similiar, we chose to start with the same hyperparameters as we used in Conll2003 training.\n","\n","\n","To update a recipe, you can download the YAML file from SparseZoo, make updates to the YAML directly, and pass the local path to SparseML. In this case, we used `--recipe_args '{\"num_epochs\":12,\"qat_start_epoch\":7.0, \"observer_epoch\": 11.0}'` to modify a recipe on the fly, updating to only run for 11 epochs."],"metadata":{"id":"7NfB9uhi0CBX"}},{"cell_type":"markdown","source":["### **Using Local JSON Files**\n","\n","Let's walk through how to pass a JSON dataset to the CLI."],"metadata":{"id":"79KVF5TB11xD"}},{"cell_type":"markdown","source":["#### **Save Dataset as a JSON File**\n","\n","For this example, we use Hugging Face `datasets` to create a JSON file for WNUT 17 that can be passed to SparseML's CLI."],"metadata":{"id":"eZF7myQc23uc"}},{"cell_type":"code","source":["from datasets import load_dataset\n","from pprint import pprint\n","\n","dataset = load_dataset(\"wnut_17\")\n","print(dataset)\n","print(dataset[\"train\"][0])"],"metadata":{"id":"zWsAD8b_2tQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n","print(label_list)"],"metadata":{"id":"7baLI9-ZeIcI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the NER task, the input in our JSON files should be labeled"],"metadata":{"id":"yi3374Mxft5s"}},{"cell_type":"code","source":["named_labels = []\n","for i in range(len(dataset[\"train\"])):\n","  named_labels_i = [label_list[label_idx] for label_idx in dataset[\"train\"][i][\"ner_tags\"]]\n","  named_labels.append(named_labels_i)\n","\n","eval_named_labels = []\n","for i in range(len(dataset[\"validation\"])):\n","  named_labels_i = [label_list[label_idx] for label_idx in dataset[\"validation\"][i][\"ner_tags\"]]\n","  eval_named_labels.append(named_labels_i)\n","\n","dataset[\"train\"] = dataset[\"train\"].add_column(\"named_ner_tags\", named_labels)\n","dataset[\"validation\"] = dataset[\"validation\"].add_column(\"named_ner_tags\", eval_named_labels)\n","dataset[\"train\"] = dataset[\"train\"].remove_columns(\"ner_tags\")\n","dataset[\"validation\"] = dataset[\"validation\"].remove_columns(\"ner_tags\")"],"metadata":{"id":"zBZCHbeDfjPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[\"train\"].to_json(\"./wnut_17-train.json\")\n","dataset[\"validation\"].to_json(\"./wnut_17-validation.json\")"],"metadata":{"id":"CTwUYc913uDB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the data is a JSON file with `tokens` and `named_ner_tags`. \n","\n","As described above, the token classification trianing pipeline expects the input sequences to be a lists of words with a tag for each word.\n","\n","When passing the dataset via local files, the tags must be strings representing each class."],"metadata":{"id":"C5O2haE73_my"}},{"cell_type":"code","source":["!head ./wnut_17-train.json --lines=5"],"metadata":{"id":"b2kMBxV134ii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head ./wnut_17-validation.json --lines=5"],"metadata":{"id":"9JdeJqQC3-SE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Kick off Training**\n","\n","To use the local files with the CLI, pass `--train_file ./wnut_17-train.json --validation_file ./wnut_17-validation.json  --text_column_name tokens --label_column_name named_ner_tags`.\n","\n","Run the following:"],"metadata":{"id":"D70YemqF3_D2"}},{"cell_type":"code","source":["!sparseml.transformers.token_classification \\\n","  --model_name_or_path zoo:nlp/masked_language_modeling/obert-base/pytorch/huggingface/wikipedia_bookcorpus/pruned90-none \\\n","  --recipe zoo:nlp/token_classification/obert-base/pytorch/huggingface/conll2003/pruned90_quant-none \\\n","  --distill_teacher disable \\\n","  --train_file wnut_17-train.json --validation_file wnut_17-validation.json \\\n","  --text_column_name tokens --label_column_name named_ner_tags \\\n","  --output_dir sparse_bert-token_classification_wnut_17_from_json \\\n","  --per_device_train_batch_size 32 --per_device_eval_batch_size 32 --preprocessing_num_workers 6 \\\n","  --do_train --do_eval --evaluation_strategy epoch --fp16 --seed 29204  \\\n","  --save_strategy epoch --save_total_limit 1"],"metadata":{"id":"HqDbMuS74YaP"},"execution_count":null,"outputs":[]}]}