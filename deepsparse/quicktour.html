

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Quick Tour &mdash; DeepSparse 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" />
    <link rel="prev" title="DeepSparse 0.1" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> DeepSparse
          

          
            
            <img src="_static/icon-engine.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">General</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Tour</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quickstart-with-sparsezoo-onnx-models">Quickstart with SparseZoo ONNX Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#quickstart-with-custom-onnx-models">Quickstart with custom ONNX models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hardware.html">Hardware Support</a></li>
</ul>
<p class="caption"><span class="caption-text">Performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="debugging-optimizing/index.html">Debugging and Optimizing</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/deepsparse.html">deepsparse package</a></li>
</ul>
<p class="caption"><span class="caption-text">Help and Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="`Bugs, Feature Requests &lt;https://github.com/neuralmagic/deepsparse/discussions&gt;`_">`Bugs, Feature Requests &lt;https://github.com/neuralmagic/deepsparse/discussions&gt;`_</a></li>
<li class="toctree-l1"><a class="reference external" href="`Support, General Q&amp;A &lt;https://github.com/neuralmagic/deepsparse/issues&gt;`_">`Support, General Q&amp;A &lt;https://github.com/neuralmagic/deepsparse/issues&gt;`_</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">DeepSparse</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Quick Tour</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/quicktour.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <!--
Copyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--><div class="section" id="quick-tour">
<h1>Quick Tour<a class="headerlink" href="#quick-tour" title="Permalink to this headline">¶</a></h1>
<p>To expedite inference and benchmarking on real models, we include the <code class="docutils literal notranslate"><span class="pre">sparsezoo</span></code> package.
<a class="reference external" href="https://github.com/neuralmagic/sparsezoo">SparseZoo</a> hosts inference optimized models,
trained on repeatable optimization recipes using state-of-the-art techniques from
<a class="reference external" href="https://github.com/neuralmagic/sparseml">SparseML</a>.</p>
<div class="section" id="quickstart-with-sparsezoo-onnx-models">
<h2>Quickstart with SparseZoo ONNX Models<a class="headerlink" href="#quickstart-with-sparsezoo-onnx-models" title="Permalink to this headline">¶</a></h2>
<p><strong>MobileNetV1 Dense</strong></p>
<p>Here is how to quickly perform inference with DeepSparse Engine on a pre-trained dense MobileNetV1 from SparseZoo.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">deepsparse</span> <span class="kn">import</span> <span class="n">compile_model</span>
<span class="kn">from</span> <span class="nn">sparsezoo.models</span> <span class="kn">import</span> <span class="n">classification</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># Download model and compile as optimized executable for your machine</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">classification</span><span class="o">.</span><span class="n">mobilenet_v1</span><span class="p">()</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># Fetch sample input and predict output using engine</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">data_inputs</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">inference_time</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">timed_run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>MobileNetV1 Optimized</strong></p>
<p>When exploring available optimized models, you can use the <code class="docutils literal notranslate"><span class="pre">Zoo.search_optimized_models</span></code>
utility to find models that share a base.</p>
<p>Let us try this on the dense MobileNetV1 to see what is available.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sparsezoo</span> <span class="kn">import</span> <span class="n">Zoo</span>
<span class="kn">from</span> <span class="nn">sparsezoo.models</span> <span class="kn">import</span> <span class="n">classification</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Zoo</span><span class="o">.</span><span class="n">search_optimized_models</span><span class="p">(</span><span class="n">classification</span><span class="o">.</span><span class="n">mobilenet_v1</span><span class="p">()))</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">Model</span><span class="p">(</span><span class="n">stub</span><span class="o">=</span><span class="n">cv</span><span class="o">/</span><span class="n">classification</span><span class="o">/</span><span class="n">mobilenet_v1</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">sparseml</span><span class="o">/</span><span class="n">imagenet</span><span class="o">/</span><span class="n">base</span><span class="o">-</span><span class="n">none</span><span class="p">),</span>
 <span class="n">Model</span><span class="p">(</span><span class="n">stub</span><span class="o">=</span><span class="n">cv</span><span class="o">/</span><span class="n">classification</span><span class="o">/</span><span class="n">mobilenet_v1</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">sparseml</span><span class="o">/</span><span class="n">imagenet</span><span class="o">/</span><span class="n">pruned</span><span class="o">-</span><span class="n">conservative</span><span class="p">),</span>
 <span class="n">Model</span><span class="p">(</span><span class="n">stub</span><span class="o">=</span><span class="n">cv</span><span class="o">/</span><span class="n">classification</span><span class="o">/</span><span class="n">mobilenet_v1</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">sparseml</span><span class="o">/</span><span class="n">imagenet</span><span class="o">/</span><span class="n">pruned</span><span class="o">-</span><span class="n">moderate</span><span class="p">),</span>
 <span class="n">Model</span><span class="p">(</span><span class="n">stub</span><span class="o">=</span><span class="n">cv</span><span class="o">/</span><span class="n">classification</span><span class="o">/</span><span class="n">mobilenet_v1</span><span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">sparseml</span><span class="o">/</span><span class="n">imagenet</span><span class="o">/</span><span class="n">pruned_quant</span><span class="o">-</span><span class="n">moderate</span><span class="p">)]</span>
</pre></div>
</div>
<p>Great. We can see there are two pruned versions targeting FP32,
<code class="docutils literal notranslate"><span class="pre">conservative</span></code> at 100% and <code class="docutils literal notranslate"><span class="pre">moderate</span></code> at &gt;= 99% of baseline accuracy.
There is also a <code class="docutils literal notranslate"><span class="pre">pruned_quant</span></code> variant targeting INT8.</p>
<p>Let’s say you want to evaluate best performance on FP32 and are okay with a small drop in accuracy,
so we can choose <code class="docutils literal notranslate"><span class="pre">pruned-moderate</span></code> over <code class="docutils literal notranslate"><span class="pre">pruned-conservative</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">deepsparse</span> <span class="kn">import</span> <span class="n">compile_model</span>
<span class="kn">from</span> <span class="nn">sparsezoo.models</span> <span class="kn">import</span> <span class="n">classification</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">classification</span><span class="o">.</span><span class="n">mobilenet_v1</span><span class="p">(</span><span class="n">optim_name</span><span class="o">=</span><span class="s2">&quot;pruned&quot;</span><span class="p">,</span> <span class="n">optim_category</span><span class="o">=</span><span class="s2">&quot;moderate&quot;</span><span class="p">)</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">compile_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">data_inputs</span><span class="o">.</span><span class="n">sample_batch</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">outputs</span><span class="p">,</span> <span class="n">inference_time</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">timed_run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="quickstart-with-custom-onnx-models">
<h2>Quickstart with custom ONNX models<a class="headerlink" href="#quickstart-with-custom-onnx-models" title="Permalink to this headline">¶</a></h2>
<p>We accept ONNX files for custom models, too. Simply plug in your model to compare performance with other solutions.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt; wget https://github.com/onnx/models/raw/master/vision/classification/mobilenet/model/mobilenetv2-7.onnx
Saving to: ‘mobilenetv2-7.onnx’
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">deepsparse</span> <span class="kn">import</span> <span class="n">compile_model</span>
<span class="kn">from</span> <span class="nn">deepsparse.utils</span> <span class="kn">import</span> <span class="n">generate_random_inputs</span>
<span class="n">onnx_filepath</span> <span class="o">=</span> <span class="s2">&quot;mobilenetv2-7.onnx&quot;</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="c1"># Generate random sample input</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">generate_random_inputs</span><span class="p">(</span><span class="n">onnx_filepath</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="c1"># Compile and run</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">compile_model</span><span class="p">(</span><span class="n">onnx_filepath</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="DeepSparse 0.1" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021 - present / Neuralmagic, Inc. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the &#34;License&#34;).

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-128364174-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>