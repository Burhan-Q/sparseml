{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sparseml/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Note, working with 1.7 Deepsparse and SparseML here\n",
    "\n",
    "from sparseml.transformers import oneshot, SparseAutoModel, SparseAutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from typing import Union\n",
    "from evaluate import evaluator\n",
    "from sparseml import export\n",
    "import sparseml.core.session as session_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "dataset_name = \"tweet_eval\"\n",
    "dataset_subname = \"sentiment\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_calibration_samples = 512\n",
    "dataset_train = load_dataset(dataset_name, dataset_subname, split=\"train\").shuffle(seed=69).select(range(num_calibration_samples))\n",
    "dataset_test = load_dataset(dataset_name, dataset_subname, split=\"test\").shuffle(seed=420)#.select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"\"\"\n",
    "test_stage:\n",
    "  obcq_modifiers:\n",
    "    QuantizationModifier:\n",
    "      ignore:\n",
    "      - classifier\n",
    "      - LayerNorm\n",
    "      - GELUActivation\n",
    "      scheme_overrides:\n",
    "        Embedding:\n",
    "          input_activations: null\n",
    "          weights:\n",
    "            num_bits: 8\n",
    "            symmetric: false\n",
    "        Linear:\n",
    "          input_activations:\n",
    "            num_bits: 8\n",
    "            symmetric: false\n",
    "          weights:\n",
    "            num_bits: 8\n",
    "            symmetric: true\n",
    "    SparseGPTModifier:\n",
    "      sparsity: 0.0\n",
    "      quantize: true\n",
    "      targets: [\"re:roberta.encoder.layer.\\\\\\d+$\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-05-14 15:22:42 sparseml.transformers.finetune.text_generation WARNING  Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: True, 16-bits training: False\n",
      "Logging all SparseML modifier-level logs to sparse_logs/14-05-2024_15.22.43.log\n",
      "2024-05-14 15:22:43 sparseml.core.logger.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/14-05-2024_15.22.43.log\n",
      "/root/sparseml/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "2024-05-14 15:22:43 sparseml.transformers.finetune.runner INFO     *** One Shot ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['label', 'labels', 'input_ids', 'attention_mask']\n",
      "{'labels': tensor([2], device='cuda:0'), 'input_ids': tensor([[    0,   113,   387, 11702,   324,  4966, 12019,  4428, 13910,   205,\n",
      "           112,   620,   631,    11,     5,   475,  4244,     4,  3180,  7428,\n",
      "            53,    11,    10,   203,   357,  6711,     4,  1009, 26003,  1420,\n",
      "            11,     5,   935,     6,   101,    38,   437,    10,  1528,   310,\n",
      "           102,  3226,   113,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 15:22:43 sparseml.core.recipe.recipe WARNING  Could not process input as a file path or zoo stub, attempting to process it as a string.\n",
      "2024-05-14 15:22:43 sparseml.core.recipe.recipe WARNING  Input string: \n",
      "test_stage:\n",
      "  obcq_modifiers:\n",
      "    QuantizationModifier:\n",
      "      ignore:\n",
      "      - classifier\n",
      "      - LayerNorm\n",
      "      - GELUActivation\n",
      "      scheme_overrides:\n",
      "        Embedding:\n",
      "          input_activations: null\n",
      "          weights:\n",
      "            num_bits: 8\n",
      "            symmetric: false\n",
      "        Linear:\n",
      "          input_activations:\n",
      "            num_bits: 8\n",
      "            symmetric: false\n",
      "          weights:\n",
      "            num_bits: 8\n",
      "            symmetric: true\n",
      "    SparseGPTModifier:\n",
      "      sparsity: 0.0\n",
      "      quantize: true\n",
      "      targets: [\"re:roberta.encoder.layer.\\\\d+$\"]\n",
      "\n",
      "2024-05-14 15:22:44 sparseml.modifiers.quantization.pytorch INFO     Running QuantizationModifier calibration with 512 samples...\n",
      "100%|██████████| 512/512 [00:37<00:00, 13.61it/s]\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.0 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.1 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.2 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.3 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.4 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.5 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.6 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.7 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.8 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.9 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.10 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.11 for compression\n",
      "2024-05-14 15:23:21 sparseml.modifiers.pruning.wanda.pytorch INFO     Running SparseGPTModifier calibration with 512 samples...\n",
      "100%|██████████| 512/512 [00:20<00:00, 24.94it/s]\n",
      "2024-05-14 15:23:42 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 1/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:42 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.query.module...\n",
      "2024-05-14 15:23:42 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.24\n",
      "2024-05-14 15:23:42 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.37\n",
      "2024-05-14 15:23:42 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.key.module...\n",
      "2024-05-14 15:23:42 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:42 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.59\n",
      "2024-05-14 15:23:42 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.value.module...\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.23\n",
      "2024-05-14 15:23:43 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.output.dense.module...\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.60\n",
      "2024-05-14 15:23:43 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.intermediate.dense.module...\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:43 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 72.54\n",
      "2024-05-14 15:23:43 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.output.dense.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.69\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 17.91\n",
      "2024-05-14 15:23:44 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 2/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.query.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.59\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.key.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.83\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.value.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.51\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.output.dense.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.90\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.intermediate.dense.module...\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:44 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 86.34\n",
      "2024-05-14 15:23:44 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.output.dense.module...\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 49.67\n",
      "2024-05-14 15:23:45 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 3/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:45 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.query.module...\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.02\n",
      "2024-05-14 15:23:45 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.key.module...\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:45 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 6.07\n",
      "2024-05-14 15:23:45 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.value.module...\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.73\n",
      "2024-05-14 15:23:46 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.output.dense.module...\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.91\n",
      "2024-05-14 15:23:46 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.intermediate.dense.module...\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:46 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 45.70\n",
      "2024-05-14 15:23:46 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.output.dense.module...\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 25.29\n",
      "2024-05-14 15:23:47 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 4/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:47 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.query.module...\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.80\n",
      "2024-05-14 15:23:47 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.key.module...\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.94\n",
      "2024-05-14 15:23:47 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.value.module...\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.30\n",
      "2024-05-14 15:23:47 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.output.dense.module...\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:47 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.58\n",
      "2024-05-14 15:23:47 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.intermediate.dense.module...\n",
      "2024-05-14 15:23:48 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:48 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 44.29\n",
      "2024-05-14 15:23:48 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.output.dense.module...\n",
      "2024-05-14 15:23:48 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.73\n",
      "2024-05-14 15:23:48 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 30.54\n",
      "2024-05-14 15:23:48 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 5/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:48 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.query.module...\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.61\n",
      "2024-05-14 15:23:49 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.key.module...\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.30\n",
      "2024-05-14 15:23:49 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.value.module...\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.48\n",
      "2024-05-14 15:23:49 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.output.dense.module...\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.43\n",
      "2024-05-14 15:23:49 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.intermediate.dense.module...\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:49 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 50.73\n",
      "2024-05-14 15:23:49 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.output.dense.module...\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 42.54\n",
      "2024-05-14 15:23:50 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 6/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:50 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.query.module...\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.62\n",
      "2024-05-14 15:23:50 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.key.module...\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.36\n",
      "2024-05-14 15:23:50 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.value.module...\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:50 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.87\n",
      "2024-05-14 15:23:50 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.output.dense.module...\n",
      "2024-05-14 15:23:51 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:51 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.49\n",
      "2024-05-14 15:23:51 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.intermediate.dense.module...\n",
      "2024-05-14 15:23:51 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:51 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 63.42\n",
      "2024-05-14 15:23:51 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.output.dense.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.71\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 46.77\n",
      "2024-05-14 15:23:52 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 7/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.query.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.54\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.key.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.25\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.value.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.52\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.output.dense.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.41\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.intermediate.dense.module...\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:52 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 46.90\n",
      "2024-05-14 15:23:52 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.output.dense.module...\n",
      "2024-05-14 15:23:53 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:53 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 40.36\n",
      "2024-05-14 15:23:53 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 8/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:53 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.query.module...\n",
      "2024-05-14 15:23:53 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:53 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.45\n",
      "2024-05-14 15:23:53 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.key.module...\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.67\n",
      "2024-05-14 15:23:54 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.value.module...\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.36\n",
      "2024-05-14 15:23:54 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.output.dense.module...\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.47\n",
      "2024-05-14 15:23:54 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.intermediate.dense.module...\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:54 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 45.90\n",
      "2024-05-14 15:23:54 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.output.dense.module...\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 80.67\n",
      "2024-05-14 15:23:55 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 9/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:55 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.query.module...\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.13\n",
      "2024-05-14 15:23:55 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.key.module...\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.04\n",
      "2024-05-14 15:23:55 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.value.module...\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.55\n",
      "2024-05-14 15:23:55 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.output.dense.module...\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:55 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.18\n",
      "2024-05-14 15:23:55 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.intermediate.dense.module...\n",
      "2024-05-14 15:23:56 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:56 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 51.86\n",
      "2024-05-14 15:23:56 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.output.dense.module...\n",
      "2024-05-14 15:23:56 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:23:56 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 52.67\n",
      "2024-05-14 15:23:56 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 10/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:56 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.query.module...\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.99\n",
      "2024-05-14 15:23:57 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.key.module...\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.03\n",
      "2024-05-14 15:23:57 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.value.module...\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.54\n",
      "2024-05-14 15:23:57 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.output.dense.module...\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.29\n",
      "2024-05-14 15:23:57 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.intermediate.dense.module...\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:57 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 38.10\n",
      "2024-05-14 15:23:57 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.output.dense.module...\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.74\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 57.26\n",
      "2024-05-14 15:23:58 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 11/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:23:58 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.query.module...\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.12\n",
      "2024-05-14 15:23:58 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.key.module...\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:58 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.64\n",
      "2024-05-14 15:23:58 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.value.module...\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.04\n",
      "2024-05-14 15:23:59 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.output.dense.module...\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.60\n",
      "2024-05-14 15:23:59 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.intermediate.dense.module...\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-14 15:23:59 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 32.62\n",
      "2024-05-14 15:23:59 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.output.dense.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 39.67\n",
      "2024-05-14 15:24:00 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 12/12 to sparsity 0.0 =====\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.query.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.22\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.key.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.48\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.value.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.40\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.output.dense.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.77\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.intermediate.dense.module...\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-14 15:24:00 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.64\n",
      "2024-05-14 15:24:00 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.output.dense.module...\n",
      "2024-05-14 15:24:01 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.71\n",
      "2024-05-14 15:24:01 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 5.50\n",
      "manager stage: Modifiers initialized\n",
      "manager stage: Modifiers finalized\n",
      "2024-05-14 15:24:03 sparseml.pytorch.model_load.helpers INFO     Saving output to /root/sparseml/oneshot_output\n"
     ]
    }
   ],
   "source": [
    "### Apply One-Shot\n",
    "\n",
    "def format_data(data):\n",
    "    return {\"text\": data[\"text\"], \"labels\": data[\"label\"]}\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=dataset_train,\n",
    "    recipe=recipe,\n",
    "    preprocessing_func = format_data,\n",
    "    output_dir=\"./oneshot_output\",\n",
    "    pad_to_max_length=False,\n",
    "    num_calibration_samples = num_calibration_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 15:44:22 sparseml.transformers.utils.sparse_model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied ./oneshot_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:3765] 2024-05-14 15:44:23,233 >> Some weights of the model checkpoint at ./oneshot_output were not used when initializing RobertaForSequenceClassification: ['roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.weight', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.output.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.module.bias', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.module.bias', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.module.weight', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.bias', 'roberta.encoder.layer.0.intermediate.dense.module.bias', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.module.weight', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.8.intermediate.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.key.module.weight', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.scale', 'roberta.embeddings.position_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.output.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.query.module.bias', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.output.dense.module.bias', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.output.dense.module.weight', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.intermediate.dense.module.bias', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.bias', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.intermediate.dense.module.weight', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.module.bias', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.module.bias', 'roberta.encoder.layer.2.attention.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.module.weight', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.weight', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.query.module.weight', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.bias', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.module.bias', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.output.dense.module.weight', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.attention.output.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.query.module.bias', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.key.module.weight', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.bias', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.6.intermediate.dense.module.bias', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.query.module.weight', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.module.weight', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.bias', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.module.weight', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.bias', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.bias', 'roberta.encoder.layer.10.intermediate.dense.module.bias', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.output.dense.module.bias', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.key.module.weight', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.bias', 'roberta.encoder.layer.10.attention.self.key.module.bias', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.intermediate.dense.module.weight', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.output.dense.module.bias', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.output.dense.module.bias', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.bias', 'roberta.encoder.layer.5.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.output.dense.module.weight', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.module.bias', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight', 'roberta.encoder.layer.9.attention.self.value.module.bias', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.query.module.bias', 'roberta.encoder.layer.5.attention.self.value.module.bias', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.module.weight', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.7.attention.self.value.module.weight', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.output.dense.module.weight', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.module.weight', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.bias', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.value.module.bias', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.module.bias', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.output.dense.module.bias', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.6.output.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.module.bias', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.key.module.weight', 'roberta.encoder.layer.8.attention.self.query.module.bias', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.value.module.weight', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.module.bias', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.key.module.weight', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight', 'roberta.encoder.layer.9.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.module.bias', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.scale', 'roberta.embeddings.word_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.output.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.output.dense.module.weight', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.query.module.weight', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.output.dense.module.weight', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.module.weight', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.output.dense.module.bias', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.module.weight', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.output.dense.module.weight', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.query.module.bias', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.value.module.weight', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.query.module.weight', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.module.bias', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.bias', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.output.dense.module.bias', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.self.value.module.weight', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.output.dense.module.bias', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.module.bias', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.self.key.module.bias', 'roberta.encoder.layer.7.output.dense.module.bias', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.output.dense.module.weight', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.bias', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.key.module.weight', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.output.dense.module.weight', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.intermediate.dense.module.bias', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight', 'roberta.encoder.layer.1.attention.self.query.module.weight', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.output.dense.module.bias', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.bias', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.output.dense.module.bias', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.module.weight', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.bias', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.attention.output.dense.module.weight', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.weight', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.output.dense.module.weight', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.module.weight', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.query.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.key.module.weight', 'roberta.encoder.layer.2.attention.self.key.module.weight', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.module.weight', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.module.bias', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.scale']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3777] 2024-05-14 15:44:23,238 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./oneshot_output and are newly initialized: ['roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-14 15:44:23 sparseml.transformers.utils.helpers INFO     Found recipe in the model_path: ./oneshot_output/recipe.yaml\n",
      "2024-05-14 15:44:23 sparseml.core.recipe.recipe INFO     Loading recipe from file ./oneshot_output/recipe.yaml\n",
      "manager stage: Model structure initialized\n",
      "2024-05-14 15:44:23 sparseml.pytorch.model_load.helpers INFO     Applied an unstaged recipe to the model at ./oneshot_output\n",
      "2024-05-14 15:44:23 sparseml.pytorch.model_load.helpers INFO     Reloaded 1230 model params for SparseML Recipe from ./oneshot_output\n",
      "2024-05-14 15:44:23 sparseml.pytorch.model_load.helpers INFO     Loaded student from ./oneshot_output with 124647939 total params. Of those there are 85526784 prunable params which have 11.718467047702857 avg sparsity.\n",
      "2024-05-14 15:44:24 sparseml.pytorch.model_load.helpers INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 124647939, \"sparse\": 10023196, \"sparsity_percent\": 8.041204756702797, \"prunable\": 85526784, \"prunable_sparse\": 10022428, \"prunable_sparsity_percent\": 11.718467047702857, \"quantizable\": 85610499, \"quantized\": 85017600, \"quantized_percent\": 99.30744592436028}, \"params_info\": {\"roberta.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02089436911046505, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0292629674077034, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02561102993786335, \"quantized\": true}, \"roberta.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0823177769780159, \"quantized\": true}, \"roberta.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.06538306176662445, \"quantized\": true}, \"roberta.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1479068398475647, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0253618024289608, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0338473841547966, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0316043421626091, \"quantized\": true}, \"roberta.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0757090225815773, \"quantized\": true}, \"roberta.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.06315655261278152, \"quantized\": true}, \"roberta.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.23669010400772095, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02553643099963665, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0330963134765625, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0389777272939682, \"quantized\": true}, \"roberta.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0653042271733284, \"quantized\": true}, \"roberta.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.041697606444358826, \"quantized\": true}, \"roberta.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.16139094531536102, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0308397077023983, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0334286168217659, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0319112129509449, \"quantized\": true}, \"roberta.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0632951557636261, \"quantized\": true}, \"roberta.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0430857352912426, \"quantized\": true}, \"roberta.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.19721010327339172, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0278811976313591, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0321129709482193, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0274946428835392, \"quantized\": true}, \"roberta.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0589836984872818, \"quantized\": true}, \"roberta.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.048330944031476974, \"quantized\": true}, \"roberta.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.252916544675827, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0282575823366642, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0331217460334301, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0276777483522892, \"quantized\": true}, \"roberta.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0560760498046875, \"quantized\": true}, \"roberta.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.057293787598609924, \"quantized\": true}, \"roberta.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.2977595925331116, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0335879847407341, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0337066650390625, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0344865582883358, \"quantized\": true}, \"roberta.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0522172711789608, \"quantized\": true}, \"roberta.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0504065603017807, \"quantized\": true}, \"roberta.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.307538777589798, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.026824951171875, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02929009310901165, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02887471579015255, \"quantized\": true}, \"roberta.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0582953542470932, \"quantized\": true}, \"roberta.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0503031425178051, \"quantized\": true}, \"roberta.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3988359272480011, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02451578713953495, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0331234410405159, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02893575094640255, \"quantized\": true}, \"roberta.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0550316721200943, \"quantized\": true}, \"roberta.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0568813756108284, \"quantized\": true}, \"roberta.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3552602231502533, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0250125452876091, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02913750521838665, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0304599329829216, \"quantized\": true}, \"roberta.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0519053153693676, \"quantized\": true}, \"roberta.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.057056427001953125, \"quantized\": true}, \"roberta.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31312432885169983, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02436319924890995, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02783881314098835, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02444966696202755, \"quantized\": true}, \"roberta.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0576510950922966, \"quantized\": true}, \"roberta.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.05695851519703865, \"quantized\": true}, \"roberta.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3723564147949219, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02547539584338665, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0274658203125, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02739630825817585, \"quantized\": true}, \"roberta.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0513543039560318, \"quantized\": true}, \"roberta.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.02618408203125, \"quantized\": true}, \"roberta.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.14533022046089172, \"quantized\": true}, \"classifier.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.out_proj.weight\": {\"numel\": 2304, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "2024-05-14 15:44:24 sparseml.pytorch.model_load.helpers INFO     Reloaded model state after SparseML recipe structure modifications from ./oneshot_output\n",
      "2024-05-14 15:44:24 sparseml.pytorch.model_load.helpers INFO     Delayed load of model ./oneshot_output detected. Will print out model information once SparseML recipes have loaded\n",
      "[WARNING|modeling_utils.py:3765] 2024-05-14 15:49:22,239 >> Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation quantized model:\n",
      "{'accuracy': 0.7218332790621947, 'total_time_in_seconds': 296.39042346703354, 'samples_per_second': 41.445333679501644, 'latency_in_seconds': 0.02412816863131175}\n",
      "Evaluation baseline model:\n",
      "{'accuracy': 0.7218332790621947, 'total_time_in_seconds': 73.40699948498514, 'samples_per_second': 167.34099045299627, 'latency_in_seconds': 0.005975822165824254}\n"
     ]
    }
   ],
   "source": [
    "### Evaluate\n",
    "\n",
    "active_session = session_manager.active_session()\n",
    "active_session.reset()\n",
    "\n",
    "def evaluate_model(model: Union[str, AutoModel]):\n",
    "    task_evaluator = evaluator(\"text-classification\")\n",
    "    eval_results = task_evaluator.compute(\n",
    "        model_or_pipeline=model,\n",
    "        tokenizer = tokenizer,\n",
    "        data=dataset_test,\n",
    "        metric=\"accuracy\",\n",
    "        label_mapping=config.label2id,\n",
    "        )\n",
    "    return eval_results\n",
    "\n",
    "m = SparseAutoModel.text_classification_from_pretrained(\"./oneshot_output\")\n",
    "eval_quant = evaluate_model(m)\n",
    "eval_baseline = evaluate_model(model_name)\n",
    "\n",
    "print(f\"Evaluation quantized model:\\n{eval_quant}\")\n",
    "print(f\"Evaluation baseline model:\\n{eval_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_stage:\n",
      "  obcq_modifiers:\n",
      "    QuantizationModifier:\n",
      "      ignore: [classifier, LayerNorm, GELUActivation]\n",
      "      scheme_overrides:\n",
      "        Embedding:\n",
      "          input_activations: null\n",
      "          weights: {num_bits: 8, symmetric: false}\n",
      "        Linear:\n",
      "          input_activations: {num_bits: 8, symmetric: false}\n",
      "          weights: {num_bits: 8, symmetric: true}\n",
      "    SparseGPTModifier:\n",
      "      sparsity: 0.0\n",
      "      quantize: true\n",
      "      targets: ['re:roberta.encoder.layer.\\d+$']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print recipe.yaml from ./oneshot_output\n",
    "with open(\"./oneshot_output/recipe.yaml\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(\n",
       "        50265, 768, padding_idx=1\n",
       "        (activation_post_process): Identity()\n",
       "        (weight_fake_quant): FakeQuantizeWrapper(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0109], device='cuda:0'), zero_point=tensor([-3], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3561680316925049, max_val=1.420897126197815)\n",
       "        )\n",
       "      )\n",
       "      (position_embeddings): Embedding(\n",
       "        514, 768, padding_idx=1\n",
       "        (activation_post_process): Identity()\n",
       "        (weight_fake_quant): FakeQuantizeWrapper(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4828029870986938, max_val=1.4631582498550415)\n",
       "        )\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(\n",
       "        1, 768\n",
       "        (activation_post_process): Identity()\n",
       "        (weight_fake_quant): FakeQuantizeWrapper(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0015], device='cuda:0'), zero_point=tensor([24], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.23259595036506653, max_val=0.15881764888763428)\n",
       "        )\n",
       "      )\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([-27], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6601641178131104, max_val=2.5111331939697266)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0051], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.618140459060669, max_val=0.6453260779380798)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([-27], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6601641178131104, max_val=2.5111331939697266)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0069], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8630175590515137, max_val=0.8860040903091431)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0164], device='cuda:0'), zero_point=tensor([-27], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6601641178131104, max_val=2.5111331939697266)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0021], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2670512795448303, max_val=0.23542672395706177)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([30], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.8914214372634888, max_val=1.160179853439331)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0061], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7221469879150391, max_val=0.7813868522644043)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1814], device='cuda:0'), zero_point=tensor([-54], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.451823234558105, max_val=32.797122955322266)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0084], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6586595773696899, max_val=1.066491961479187)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0479], device='cuda:0'), zero_point=tensor([-124], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=12.03664493560791)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0145], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6680229902267456, max_val=1.8484076261520386)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0732], device='cuda:0'), zero_point=tensor([-47], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.919826507568359, max_val=12.743324279785156)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0044], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5587633848190308, max_val=0.5650312900543213)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0732], device='cuda:0'), zero_point=tensor([-47], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.919826507568359, max_val=12.743324279785156)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0057], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7205859422683716, max_val=0.6624219417572021)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0732], device='cuda:0'), zero_point=tensor([-47], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.919826507568359, max_val=12.743324279785156)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36692938208580017, max_val=0.3550468385219574)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0165], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2268929481506348, max_val=1.9699058532714844)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0055], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7004179954528809, max_val=0.6737506985664368)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1746], device='cuda:0'), zero_point=tensor([-58], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.21845817565918, max_val=32.29315948486328)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0086], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0967119932174683, max_val=0.7356662154197693)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0578], device='cuda:0'), zero_point=tensor([-125], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=14.579336166381836)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0245], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.278036117553711, max_val=3.123696804046631)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1054], device='cuda:0'), zero_point=tensor([-70], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.142152786254883, max_val=20.746156692504883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5265164375305176, max_val=0.5183249115943909)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1054], device='cuda:0'), zero_point=tensor([-70], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.142152786254883, max_val=20.746156692504883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0051], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5784733891487122, max_val=0.6455265283584595)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1054], device='cuda:0'), zero_point=tensor([-70], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.142152786254883, max_val=20.746156692504883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5068112015724182, max_val=0.4779423177242279)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0147], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.873989224433899, max_val=1.8628402948379517)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6432022452354431, max_val=0.6292092204093933)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1744], device='cuda:0'), zero_point=tensor([-79], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.602585792541504, max_val=35.87180709838867)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0058], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6718555688858032, max_val=0.733588457107544)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0284], device='cuda:0'), zero_point=tensor([-122], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=7.08471155166626)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0168], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.1389541625976562, max_val=1.5440884828567505)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0997], device='cuda:0'), zero_point=tensor([-62], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.545657634735107, max_val=18.871191024780273)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6678327918052673, max_val=0.6236758828163147)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0997], device='cuda:0'), zero_point=tensor([-62], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.545657634735107, max_val=18.871191024780273)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0053], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6772737503051758, max_val=0.6502019762992859)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0997], device='cuda:0'), zero_point=tensor([-62], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.545657634735107, max_val=18.871191024780273)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0033], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.4217713475227356, max_val=0.41670501232147217)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0124], device='cuda:0'), zero_point=tensor([-5], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5178626775741577, max_val=1.6336613893508911)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6276994943618774, max_val=0.5486763715744019)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1776], device='cuda:0'), zero_point=tensor([-75], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.42763614654541, max_val=35.86450958251953)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0058], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7392950654029846, max_val=0.707061767578125)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0514], device='cuda:0'), zero_point=tensor([-125], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997121274471283, max_val=12.945658683776855)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0200], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7141196727752686, max_val=2.5457255840301514)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0961], device='cuda:0'), zero_point=tensor([-48], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.649097919464111, max_val=16.85982322692871)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0049], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5057156682014465, max_val=0.6215968728065491)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0961], device='cuda:0'), zero_point=tensor([-48], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.649097919464111, max_val=16.85982322692871)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6667072176933289, max_val=0.6386003494262695)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0961], device='cuda:0'), zero_point=tensor([-48], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.649097919464111, max_val=16.85982322692871)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3541368246078491, max_val=0.32894930243492126)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0109], device='cuda:0'), zero_point=tensor([-5], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.3435664176940918, max_val=1.435440182685852)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5710070729255676, max_val=0.5596646070480347)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1858], device='cuda:0'), zero_point=tensor([-66], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.577583312988281, max_val=35.7951545715332)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7864381670951843, max_val=0.7794861197471619)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0627], device='cuda:0'), zero_point=tensor([-125], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=15.815690040588379)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0243], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0359035730361938, max_val=3.104058027267456)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1110], device='cuda:0'), zero_point=tensor([-43], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.40121841430664, max_val=18.905637741088867)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.587039053440094, max_val=0.547551691532135)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1110], device='cuda:0'), zero_point=tensor([-43], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.40121841430664, max_val=18.905637741088867)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0050], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6335949301719666, max_val=0.600550651550293)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1110], device='cuda:0'), zero_point=tensor([-43], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-9.40121841430664, max_val=18.905637741088867)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3366301953792572, max_val=0.3743240237236023)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0123], device='cuda:0'), zero_point=tensor([-9], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.47092866897583, max_val=1.6776589155197144)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5860006213188171, max_val=0.5522385835647583)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1932], device='cuda:0'), zero_point=tensor([-61], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.94591236114502, max_val=36.31972885131836)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0070], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8973497748374939, max_val=0.8229506015777588)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0845], device='cuda:0'), zero_point=tensor([-126], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997121274471283, max_val=21.365550994873047)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0277], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.208718180656433, max_val=3.5254974365234375)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1119], device='cuda:0'), zero_point=tensor([-33], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.611227989196777, max_val=17.922914505004883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0054], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5887453556060791, max_val=0.6839380860328674)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1119], device='cuda:0'), zero_point=tensor([-33], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.611227989196777, max_val=17.922914505004883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0052], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6657301187515259, max_val=0.6101093292236328)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1119], device='cuda:0'), zero_point=tensor([-33], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.611227989196777, max_val=17.922914505004883)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0037], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.46683257818222046, max_val=0.3402788043022156)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0108], device='cuda:0'), zero_point=tensor([6], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4512449502944946, max_val=1.308656930923462)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5765660405158997, max_val=0.5406519770622253)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1930], device='cuda:0'), zero_point=tensor([-58], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.571475982666016, max_val=35.6464958190918)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0064], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8208041191101074, max_val=0.7944635152816772)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0668], device='cuda:0'), zero_point=tensor([-125], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=16.86212921142578)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0296], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.705305576324463, max_val=3.7748541831970215)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1246], device='cuda:0'), zero_point=tensor([-38], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.255583763122559, max_val=20.5250186920166)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0047], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5177690982818604, max_val=0.5965904593467712)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1246], device='cuda:0'), zero_point=tensor([-38], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.255583763122559, max_val=20.5250186920166)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6034659147262573, max_val=0.6170825362205505)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1246], device='cuda:0'), zero_point=tensor([-38], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.255583763122559, max_val=20.5250186920166)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0026], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.32888907194137573, max_val=0.31254705786705017)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0108], device='cuda:0'), zero_point=tensor([7], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.4565205574035645, max_val=1.2945020198822021)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5474953651428223, max_val=0.5327461957931519)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1949], device='cuda:0'), zero_point=tensor([-55], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.257596015930176, max_val=35.43898010253906)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0065], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7764174342155457, max_val=0.8250603079795837)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1204], device='cuda:0'), zero_point=tensor([-127], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997121274471283, max_val=30.530424118041992)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0412], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0569249391555786, max_val=5.249917507171631)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1260], device='cuda:0'), zero_point=tensor([-36], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.564591407775879, max_val=20.577926635742188)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0041], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5189374685287476, max_val=0.503798246383667)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1260], device='cuda:0'), zero_point=tensor([-36], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.564591407775879, max_val=20.577926635742188)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0051], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5702518224716187, max_val=0.6545721888542175)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1260], device='cuda:0'), zero_point=tensor([-36], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-11.564591407775879, max_val=20.577926635742188)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.31768670678138733, max_val=0.3659029006958008)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0081], device='cuda:0'), zero_point=tensor([3], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0584534406661987, max_val=1.0014235973358154)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5589761734008789, max_val=0.5731400847434998)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1946], device='cuda:0'), zero_point=tensor([-50], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.12575912475586, max_val=34.50131607055664)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0072], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9226207733154297, max_val=0.7175493240356445)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0822], device='cuda:0'), zero_point=tensor([-126], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=20.778329849243164)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0370], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.889462947845459, max_val=4.713297367095947)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1334], device='cuda:0'), zero_point=tensor([-31], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.910224914550781, max_val=21.094629287719727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5012902617454529, max_val=0.5106655359268188)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1334], device='cuda:0'), zero_point=tensor([-31], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.910224914550781, max_val=21.094629287719727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5787078142166138, max_val=0.5375296473503113)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1334], device='cuda:0'), zero_point=tensor([-31], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-12.910224914550781, max_val=21.094629287719727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3638445734977722, max_val=0.3080691695213318)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0116], device='cuda:0'), zero_point=tensor([22], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.745133876800537, max_val=1.2222905158996582)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5294023752212524, max_val=0.5475834608078003)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1875], device='cuda:0'), zero_point=tensor([-44], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-15.812396049499512, max_val=31.98993492126465)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0067], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.8555608987808228, max_val=0.8550942540168762)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1601], device='cuda:0'), zero_point=tensor([-127], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=40.657135009765625)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0327], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.210292100906372, max_val=4.165809631347656)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1366], device='cuda:0'), zero_point=tensor([-26], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.937139511108398, max_val=20.89697265625)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0040], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5091990828514099, max_val=0.3982117772102356)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1366], device='cuda:0'), zero_point=tensor([-26], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.937139511108398, max_val=20.89697265625)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5154428482055664, max_val=0.5461276769638062)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1366], device='cuda:0'), zero_point=tensor([-26], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.937139511108398, max_val=20.89697265625)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0023], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.2911045253276825, max_val=0.2651950418949127)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0132], device='cuda:0'), zero_point=tensor([4], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.7433788776397705, max_val=1.6237136125564575)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0048], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.6104379892349243, max_val=0.5696321129798889)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1863], device='cuda:0'), zero_point=tensor([-48], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.977255821228027, max_val=32.531558990478516)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0062], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.7878066301345825, max_val=0.7786189317703247)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0720], device='cuda:0'), zero_point=tensor([-126], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=18.189434051513672)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0384], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.9459289312362671, max_val=4.897655963897705)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1287], device='cuda:0'), zero_point=tensor([-20], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.878045082092285, max_val=18.952783584594727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0043], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5154982805252075, max_val=0.5427044630050659)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (key): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1287], device='cuda:0'), zero_point=tensor([-20], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.878045082092285, max_val=18.952783584594727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0045], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.5692673921585083, max_val=0.5097682476043701)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (value): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1287], device='cuda:0'), zero_point=tensor([-20], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-13.878045082092285, max_val=18.952783584594727)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0028], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.3511736989021301, max_val=0.3580920994281769)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): QuantWrapper(\n",
       "                (quant): QuantStub(\n",
       "                  (activation_post_process): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0173], device='cuda:0'), zero_point=tensor([1], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-2.2278621196746826, max_val=2.1795814037323)\n",
       "                  )\n",
       "                )\n",
       "                (dequant): DeQuantStub()\n",
       "                (module): Linear(\n",
       "                  in_features=768, out_features=768, bias=True\n",
       "                  (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0046], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                    (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.53647780418396, max_val=0.5810508728027344)\n",
       "                  )\n",
       "                  (activation_post_process): Identity()\n",
       "                )\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1084], device='cuda:0'), zero_point=tensor([5], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.445683479309082, max_val=13.200942039489746)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=768, out_features=3072, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0029], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.36745786666870117, max_val=0.35584548115730286)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): QuantWrapper(\n",
       "              (quant): QuantStub(\n",
       "                (activation_post_process): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0175], device='cuda:0'), zero_point=tensor([-118], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-0.16997122764587402, max_val=4.29701042175293)\n",
       "                )\n",
       "              )\n",
       "              (dequant): DeQuantStub()\n",
       "              (module): Linear(\n",
       "                in_features=3072, out_features=768, bias=True\n",
       "                (weight_fake_quant): FakeQuantizeWrapper(\n",
       "                  fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([0.0120], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.5331852436065674, max_val=1.5024077892303467)\n",
       "                )\n",
       "                (activation_post_process): Identity()\n",
       "              )\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "roberta\n",
      "roberta.embeddings\n",
      "roberta.embeddings.word_embeddings\n",
      "roberta.embeddings.word_embeddings.activation_post_process\n",
      "roberta.embeddings.word_embeddings.weight_fake_quant\n",
      "roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process\n",
      "roberta.embeddings.position_embeddings\n",
      "roberta.embeddings.position_embeddings.activation_post_process\n",
      "roberta.embeddings.position_embeddings.weight_fake_quant\n",
      "roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process\n",
      "roberta.embeddings.token_type_embeddings\n",
      "roberta.embeddings.token_type_embeddings.activation_post_process\n",
      "roberta.embeddings.token_type_embeddings.weight_fake_quant\n",
      "roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process\n",
      "roberta.embeddings.LayerNorm\n",
      "roberta.embeddings.dropout\n",
      "roberta.encoder\n",
      "roberta.encoder.layer\n",
      "roberta.encoder.layer.0\n",
      "roberta.encoder.layer.0.attention\n",
      "roberta.encoder.layer.0.attention.self\n",
      "roberta.encoder.layer.0.attention.self.query\n",
      "roberta.encoder.layer.0.attention.self.query.quant\n",
      "roberta.encoder.layer.0.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.query.dequant\n",
      "roberta.encoder.layer.0.attention.self.query.module\n",
      "roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.key\n",
      "roberta.encoder.layer.0.attention.self.key.quant\n",
      "roberta.encoder.layer.0.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.key.dequant\n",
      "roberta.encoder.layer.0.attention.self.key.module\n",
      "roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.value\n",
      "roberta.encoder.layer.0.attention.self.value.quant\n",
      "roberta.encoder.layer.0.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.value.dequant\n",
      "roberta.encoder.layer.0.attention.self.value.module\n",
      "roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.0.attention.self.dropout\n",
      "roberta.encoder.layer.0.attention.output\n",
      "roberta.encoder.layer.0.attention.output.dense\n",
      "roberta.encoder.layer.0.attention.output.dense.quant\n",
      "roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.attention.output.dense.dequant\n",
      "roberta.encoder.layer.0.attention.output.dense.module\n",
      "roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm\n",
      "roberta.encoder.layer.0.attention.output.dropout\n",
      "roberta.encoder.layer.0.intermediate\n",
      "roberta.encoder.layer.0.intermediate.dense\n",
      "roberta.encoder.layer.0.intermediate.dense.quant\n",
      "roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.intermediate.dense.dequant\n",
      "roberta.encoder.layer.0.intermediate.dense.module\n",
      "roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.0.output\n",
      "roberta.encoder.layer.0.output.dense\n",
      "roberta.encoder.layer.0.output.dense.quant\n",
      "roberta.encoder.layer.0.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.0.output.dense.dequant\n",
      "roberta.encoder.layer.0.output.dense.module\n",
      "roberta.encoder.layer.0.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.0.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.0.output.LayerNorm\n",
      "roberta.encoder.layer.0.output.dropout\n",
      "roberta.encoder.layer.1\n",
      "roberta.encoder.layer.1.attention\n",
      "roberta.encoder.layer.1.attention.self\n",
      "roberta.encoder.layer.1.attention.self.query\n",
      "roberta.encoder.layer.1.attention.self.query.quant\n",
      "roberta.encoder.layer.1.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.query.dequant\n",
      "roberta.encoder.layer.1.attention.self.query.module\n",
      "roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.key\n",
      "roberta.encoder.layer.1.attention.self.key.quant\n",
      "roberta.encoder.layer.1.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.key.dequant\n",
      "roberta.encoder.layer.1.attention.self.key.module\n",
      "roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.value\n",
      "roberta.encoder.layer.1.attention.self.value.quant\n",
      "roberta.encoder.layer.1.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.value.dequant\n",
      "roberta.encoder.layer.1.attention.self.value.module\n",
      "roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.1.attention.self.dropout\n",
      "roberta.encoder.layer.1.attention.output\n",
      "roberta.encoder.layer.1.attention.output.dense\n",
      "roberta.encoder.layer.1.attention.output.dense.quant\n",
      "roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.attention.output.dense.dequant\n",
      "roberta.encoder.layer.1.attention.output.dense.module\n",
      "roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm\n",
      "roberta.encoder.layer.1.attention.output.dropout\n",
      "roberta.encoder.layer.1.intermediate\n",
      "roberta.encoder.layer.1.intermediate.dense\n",
      "roberta.encoder.layer.1.intermediate.dense.quant\n",
      "roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.intermediate.dense.dequant\n",
      "roberta.encoder.layer.1.intermediate.dense.module\n",
      "roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.1.output\n",
      "roberta.encoder.layer.1.output.dense\n",
      "roberta.encoder.layer.1.output.dense.quant\n",
      "roberta.encoder.layer.1.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.1.output.dense.dequant\n",
      "roberta.encoder.layer.1.output.dense.module\n",
      "roberta.encoder.layer.1.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.1.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.1.output.LayerNorm\n",
      "roberta.encoder.layer.1.output.dropout\n",
      "roberta.encoder.layer.2\n",
      "roberta.encoder.layer.2.attention\n",
      "roberta.encoder.layer.2.attention.self\n",
      "roberta.encoder.layer.2.attention.self.query\n",
      "roberta.encoder.layer.2.attention.self.query.quant\n",
      "roberta.encoder.layer.2.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.query.dequant\n",
      "roberta.encoder.layer.2.attention.self.query.module\n",
      "roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.key\n",
      "roberta.encoder.layer.2.attention.self.key.quant\n",
      "roberta.encoder.layer.2.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.key.dequant\n",
      "roberta.encoder.layer.2.attention.self.key.module\n",
      "roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.value\n",
      "roberta.encoder.layer.2.attention.self.value.quant\n",
      "roberta.encoder.layer.2.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.value.dequant\n",
      "roberta.encoder.layer.2.attention.self.value.module\n",
      "roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.2.attention.self.dropout\n",
      "roberta.encoder.layer.2.attention.output\n",
      "roberta.encoder.layer.2.attention.output.dense\n",
      "roberta.encoder.layer.2.attention.output.dense.quant\n",
      "roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.attention.output.dense.dequant\n",
      "roberta.encoder.layer.2.attention.output.dense.module\n",
      "roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm\n",
      "roberta.encoder.layer.2.attention.output.dropout\n",
      "roberta.encoder.layer.2.intermediate\n",
      "roberta.encoder.layer.2.intermediate.dense\n",
      "roberta.encoder.layer.2.intermediate.dense.quant\n",
      "roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.intermediate.dense.dequant\n",
      "roberta.encoder.layer.2.intermediate.dense.module\n",
      "roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.2.output\n",
      "roberta.encoder.layer.2.output.dense\n",
      "roberta.encoder.layer.2.output.dense.quant\n",
      "roberta.encoder.layer.2.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.2.output.dense.dequant\n",
      "roberta.encoder.layer.2.output.dense.module\n",
      "roberta.encoder.layer.2.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.2.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.2.output.LayerNorm\n",
      "roberta.encoder.layer.2.output.dropout\n",
      "roberta.encoder.layer.3\n",
      "roberta.encoder.layer.3.attention\n",
      "roberta.encoder.layer.3.attention.self\n",
      "roberta.encoder.layer.3.attention.self.query\n",
      "roberta.encoder.layer.3.attention.self.query.quant\n",
      "roberta.encoder.layer.3.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.query.dequant\n",
      "roberta.encoder.layer.3.attention.self.query.module\n",
      "roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.key\n",
      "roberta.encoder.layer.3.attention.self.key.quant\n",
      "roberta.encoder.layer.3.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.key.dequant\n",
      "roberta.encoder.layer.3.attention.self.key.module\n",
      "roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.value\n",
      "roberta.encoder.layer.3.attention.self.value.quant\n",
      "roberta.encoder.layer.3.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.value.dequant\n",
      "roberta.encoder.layer.3.attention.self.value.module\n",
      "roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.3.attention.self.dropout\n",
      "roberta.encoder.layer.3.attention.output\n",
      "roberta.encoder.layer.3.attention.output.dense\n",
      "roberta.encoder.layer.3.attention.output.dense.quant\n",
      "roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.attention.output.dense.dequant\n",
      "roberta.encoder.layer.3.attention.output.dense.module\n",
      "roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm\n",
      "roberta.encoder.layer.3.attention.output.dropout\n",
      "roberta.encoder.layer.3.intermediate\n",
      "roberta.encoder.layer.3.intermediate.dense\n",
      "roberta.encoder.layer.3.intermediate.dense.quant\n",
      "roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.intermediate.dense.dequant\n",
      "roberta.encoder.layer.3.intermediate.dense.module\n",
      "roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.3.output\n",
      "roberta.encoder.layer.3.output.dense\n",
      "roberta.encoder.layer.3.output.dense.quant\n",
      "roberta.encoder.layer.3.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.3.output.dense.dequant\n",
      "roberta.encoder.layer.3.output.dense.module\n",
      "roberta.encoder.layer.3.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.3.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.3.output.LayerNorm\n",
      "roberta.encoder.layer.3.output.dropout\n",
      "roberta.encoder.layer.4\n",
      "roberta.encoder.layer.4.attention\n",
      "roberta.encoder.layer.4.attention.self\n",
      "roberta.encoder.layer.4.attention.self.query\n",
      "roberta.encoder.layer.4.attention.self.query.quant\n",
      "roberta.encoder.layer.4.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.query.dequant\n",
      "roberta.encoder.layer.4.attention.self.query.module\n",
      "roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.key\n",
      "roberta.encoder.layer.4.attention.self.key.quant\n",
      "roberta.encoder.layer.4.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.key.dequant\n",
      "roberta.encoder.layer.4.attention.self.key.module\n",
      "roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.value\n",
      "roberta.encoder.layer.4.attention.self.value.quant\n",
      "roberta.encoder.layer.4.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.value.dequant\n",
      "roberta.encoder.layer.4.attention.self.value.module\n",
      "roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.4.attention.self.dropout\n",
      "roberta.encoder.layer.4.attention.output\n",
      "roberta.encoder.layer.4.attention.output.dense\n",
      "roberta.encoder.layer.4.attention.output.dense.quant\n",
      "roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.attention.output.dense.dequant\n",
      "roberta.encoder.layer.4.attention.output.dense.module\n",
      "roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm\n",
      "roberta.encoder.layer.4.attention.output.dropout\n",
      "roberta.encoder.layer.4.intermediate\n",
      "roberta.encoder.layer.4.intermediate.dense\n",
      "roberta.encoder.layer.4.intermediate.dense.quant\n",
      "roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.intermediate.dense.dequant\n",
      "roberta.encoder.layer.4.intermediate.dense.module\n",
      "roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.4.output\n",
      "roberta.encoder.layer.4.output.dense\n",
      "roberta.encoder.layer.4.output.dense.quant\n",
      "roberta.encoder.layer.4.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.4.output.dense.dequant\n",
      "roberta.encoder.layer.4.output.dense.module\n",
      "roberta.encoder.layer.4.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.4.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.4.output.LayerNorm\n",
      "roberta.encoder.layer.4.output.dropout\n",
      "roberta.encoder.layer.5\n",
      "roberta.encoder.layer.5.attention\n",
      "roberta.encoder.layer.5.attention.self\n",
      "roberta.encoder.layer.5.attention.self.query\n",
      "roberta.encoder.layer.5.attention.self.query.quant\n",
      "roberta.encoder.layer.5.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.query.dequant\n",
      "roberta.encoder.layer.5.attention.self.query.module\n",
      "roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.key\n",
      "roberta.encoder.layer.5.attention.self.key.quant\n",
      "roberta.encoder.layer.5.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.key.dequant\n",
      "roberta.encoder.layer.5.attention.self.key.module\n",
      "roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.value\n",
      "roberta.encoder.layer.5.attention.self.value.quant\n",
      "roberta.encoder.layer.5.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.value.dequant\n",
      "roberta.encoder.layer.5.attention.self.value.module\n",
      "roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.5.attention.self.dropout\n",
      "roberta.encoder.layer.5.attention.output\n",
      "roberta.encoder.layer.5.attention.output.dense\n",
      "roberta.encoder.layer.5.attention.output.dense.quant\n",
      "roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.attention.output.dense.dequant\n",
      "roberta.encoder.layer.5.attention.output.dense.module\n",
      "roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm\n",
      "roberta.encoder.layer.5.attention.output.dropout\n",
      "roberta.encoder.layer.5.intermediate\n",
      "roberta.encoder.layer.5.intermediate.dense\n",
      "roberta.encoder.layer.5.intermediate.dense.quant\n",
      "roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.intermediate.dense.dequant\n",
      "roberta.encoder.layer.5.intermediate.dense.module\n",
      "roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.5.output\n",
      "roberta.encoder.layer.5.output.dense\n",
      "roberta.encoder.layer.5.output.dense.quant\n",
      "roberta.encoder.layer.5.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.5.output.dense.dequant\n",
      "roberta.encoder.layer.5.output.dense.module\n",
      "roberta.encoder.layer.5.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.5.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.5.output.LayerNorm\n",
      "roberta.encoder.layer.5.output.dropout\n",
      "roberta.encoder.layer.6\n",
      "roberta.encoder.layer.6.attention\n",
      "roberta.encoder.layer.6.attention.self\n",
      "roberta.encoder.layer.6.attention.self.query\n",
      "roberta.encoder.layer.6.attention.self.query.quant\n",
      "roberta.encoder.layer.6.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.query.dequant\n",
      "roberta.encoder.layer.6.attention.self.query.module\n",
      "roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.key\n",
      "roberta.encoder.layer.6.attention.self.key.quant\n",
      "roberta.encoder.layer.6.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.key.dequant\n",
      "roberta.encoder.layer.6.attention.self.key.module\n",
      "roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.value\n",
      "roberta.encoder.layer.6.attention.self.value.quant\n",
      "roberta.encoder.layer.6.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.value.dequant\n",
      "roberta.encoder.layer.6.attention.self.value.module\n",
      "roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.6.attention.self.dropout\n",
      "roberta.encoder.layer.6.attention.output\n",
      "roberta.encoder.layer.6.attention.output.dense\n",
      "roberta.encoder.layer.6.attention.output.dense.quant\n",
      "roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.attention.output.dense.dequant\n",
      "roberta.encoder.layer.6.attention.output.dense.module\n",
      "roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm\n",
      "roberta.encoder.layer.6.attention.output.dropout\n",
      "roberta.encoder.layer.6.intermediate\n",
      "roberta.encoder.layer.6.intermediate.dense\n",
      "roberta.encoder.layer.6.intermediate.dense.quant\n",
      "roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.intermediate.dense.dequant\n",
      "roberta.encoder.layer.6.intermediate.dense.module\n",
      "roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.6.output\n",
      "roberta.encoder.layer.6.output.dense\n",
      "roberta.encoder.layer.6.output.dense.quant\n",
      "roberta.encoder.layer.6.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.6.output.dense.dequant\n",
      "roberta.encoder.layer.6.output.dense.module\n",
      "roberta.encoder.layer.6.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.6.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.6.output.LayerNorm\n",
      "roberta.encoder.layer.6.output.dropout\n",
      "roberta.encoder.layer.7\n",
      "roberta.encoder.layer.7.attention\n",
      "roberta.encoder.layer.7.attention.self\n",
      "roberta.encoder.layer.7.attention.self.query\n",
      "roberta.encoder.layer.7.attention.self.query.quant\n",
      "roberta.encoder.layer.7.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.query.dequant\n",
      "roberta.encoder.layer.7.attention.self.query.module\n",
      "roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.key\n",
      "roberta.encoder.layer.7.attention.self.key.quant\n",
      "roberta.encoder.layer.7.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.key.dequant\n",
      "roberta.encoder.layer.7.attention.self.key.module\n",
      "roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.value\n",
      "roberta.encoder.layer.7.attention.self.value.quant\n",
      "roberta.encoder.layer.7.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.value.dequant\n",
      "roberta.encoder.layer.7.attention.self.value.module\n",
      "roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.7.attention.self.dropout\n",
      "roberta.encoder.layer.7.attention.output\n",
      "roberta.encoder.layer.7.attention.output.dense\n",
      "roberta.encoder.layer.7.attention.output.dense.quant\n",
      "roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.attention.output.dense.dequant\n",
      "roberta.encoder.layer.7.attention.output.dense.module\n",
      "roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm\n",
      "roberta.encoder.layer.7.attention.output.dropout\n",
      "roberta.encoder.layer.7.intermediate\n",
      "roberta.encoder.layer.7.intermediate.dense\n",
      "roberta.encoder.layer.7.intermediate.dense.quant\n",
      "roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.intermediate.dense.dequant\n",
      "roberta.encoder.layer.7.intermediate.dense.module\n",
      "roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.7.output\n",
      "roberta.encoder.layer.7.output.dense\n",
      "roberta.encoder.layer.7.output.dense.quant\n",
      "roberta.encoder.layer.7.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.7.output.dense.dequant\n",
      "roberta.encoder.layer.7.output.dense.module\n",
      "roberta.encoder.layer.7.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.7.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.7.output.LayerNorm\n",
      "roberta.encoder.layer.7.output.dropout\n",
      "roberta.encoder.layer.8\n",
      "roberta.encoder.layer.8.attention\n",
      "roberta.encoder.layer.8.attention.self\n",
      "roberta.encoder.layer.8.attention.self.query\n",
      "roberta.encoder.layer.8.attention.self.query.quant\n",
      "roberta.encoder.layer.8.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.query.dequant\n",
      "roberta.encoder.layer.8.attention.self.query.module\n",
      "roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.key\n",
      "roberta.encoder.layer.8.attention.self.key.quant\n",
      "roberta.encoder.layer.8.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.key.dequant\n",
      "roberta.encoder.layer.8.attention.self.key.module\n",
      "roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.value\n",
      "roberta.encoder.layer.8.attention.self.value.quant\n",
      "roberta.encoder.layer.8.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.value.dequant\n",
      "roberta.encoder.layer.8.attention.self.value.module\n",
      "roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.8.attention.self.dropout\n",
      "roberta.encoder.layer.8.attention.output\n",
      "roberta.encoder.layer.8.attention.output.dense\n",
      "roberta.encoder.layer.8.attention.output.dense.quant\n",
      "roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.attention.output.dense.dequant\n",
      "roberta.encoder.layer.8.attention.output.dense.module\n",
      "roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm\n",
      "roberta.encoder.layer.8.attention.output.dropout\n",
      "roberta.encoder.layer.8.intermediate\n",
      "roberta.encoder.layer.8.intermediate.dense\n",
      "roberta.encoder.layer.8.intermediate.dense.quant\n",
      "roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.intermediate.dense.dequant\n",
      "roberta.encoder.layer.8.intermediate.dense.module\n",
      "roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.8.output\n",
      "roberta.encoder.layer.8.output.dense\n",
      "roberta.encoder.layer.8.output.dense.quant\n",
      "roberta.encoder.layer.8.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.8.output.dense.dequant\n",
      "roberta.encoder.layer.8.output.dense.module\n",
      "roberta.encoder.layer.8.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.8.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.8.output.LayerNorm\n",
      "roberta.encoder.layer.8.output.dropout\n",
      "roberta.encoder.layer.9\n",
      "roberta.encoder.layer.9.attention\n",
      "roberta.encoder.layer.9.attention.self\n",
      "roberta.encoder.layer.9.attention.self.query\n",
      "roberta.encoder.layer.9.attention.self.query.quant\n",
      "roberta.encoder.layer.9.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.query.dequant\n",
      "roberta.encoder.layer.9.attention.self.query.module\n",
      "roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.key\n",
      "roberta.encoder.layer.9.attention.self.key.quant\n",
      "roberta.encoder.layer.9.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.key.dequant\n",
      "roberta.encoder.layer.9.attention.self.key.module\n",
      "roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.value\n",
      "roberta.encoder.layer.9.attention.self.value.quant\n",
      "roberta.encoder.layer.9.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.value.dequant\n",
      "roberta.encoder.layer.9.attention.self.value.module\n",
      "roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.9.attention.self.dropout\n",
      "roberta.encoder.layer.9.attention.output\n",
      "roberta.encoder.layer.9.attention.output.dense\n",
      "roberta.encoder.layer.9.attention.output.dense.quant\n",
      "roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.attention.output.dense.dequant\n",
      "roberta.encoder.layer.9.attention.output.dense.module\n",
      "roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm\n",
      "roberta.encoder.layer.9.attention.output.dropout\n",
      "roberta.encoder.layer.9.intermediate\n",
      "roberta.encoder.layer.9.intermediate.dense\n",
      "roberta.encoder.layer.9.intermediate.dense.quant\n",
      "roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.intermediate.dense.dequant\n",
      "roberta.encoder.layer.9.intermediate.dense.module\n",
      "roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.9.output\n",
      "roberta.encoder.layer.9.output.dense\n",
      "roberta.encoder.layer.9.output.dense.quant\n",
      "roberta.encoder.layer.9.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.9.output.dense.dequant\n",
      "roberta.encoder.layer.9.output.dense.module\n",
      "roberta.encoder.layer.9.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.9.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.9.output.LayerNorm\n",
      "roberta.encoder.layer.9.output.dropout\n",
      "roberta.encoder.layer.10\n",
      "roberta.encoder.layer.10.attention\n",
      "roberta.encoder.layer.10.attention.self\n",
      "roberta.encoder.layer.10.attention.self.query\n",
      "roberta.encoder.layer.10.attention.self.query.quant\n",
      "roberta.encoder.layer.10.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.query.dequant\n",
      "roberta.encoder.layer.10.attention.self.query.module\n",
      "roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.key\n",
      "roberta.encoder.layer.10.attention.self.key.quant\n",
      "roberta.encoder.layer.10.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.key.dequant\n",
      "roberta.encoder.layer.10.attention.self.key.module\n",
      "roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.value\n",
      "roberta.encoder.layer.10.attention.self.value.quant\n",
      "roberta.encoder.layer.10.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.value.dequant\n",
      "roberta.encoder.layer.10.attention.self.value.module\n",
      "roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.10.attention.self.dropout\n",
      "roberta.encoder.layer.10.attention.output\n",
      "roberta.encoder.layer.10.attention.output.dense\n",
      "roberta.encoder.layer.10.attention.output.dense.quant\n",
      "roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.attention.output.dense.dequant\n",
      "roberta.encoder.layer.10.attention.output.dense.module\n",
      "roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm\n",
      "roberta.encoder.layer.10.attention.output.dropout\n",
      "roberta.encoder.layer.10.intermediate\n",
      "roberta.encoder.layer.10.intermediate.dense\n",
      "roberta.encoder.layer.10.intermediate.dense.quant\n",
      "roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.intermediate.dense.dequant\n",
      "roberta.encoder.layer.10.intermediate.dense.module\n",
      "roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.10.output\n",
      "roberta.encoder.layer.10.output.dense\n",
      "roberta.encoder.layer.10.output.dense.quant\n",
      "roberta.encoder.layer.10.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.10.output.dense.dequant\n",
      "roberta.encoder.layer.10.output.dense.module\n",
      "roberta.encoder.layer.10.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.10.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.10.output.LayerNorm\n",
      "roberta.encoder.layer.10.output.dropout\n",
      "roberta.encoder.layer.11\n",
      "roberta.encoder.layer.11.attention\n",
      "roberta.encoder.layer.11.attention.self\n",
      "roberta.encoder.layer.11.attention.self.query\n",
      "roberta.encoder.layer.11.attention.self.query.quant\n",
      "roberta.encoder.layer.11.attention.self.query.quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.query.dequant\n",
      "roberta.encoder.layer.11.attention.self.query.module\n",
      "roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.query.module.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.key\n",
      "roberta.encoder.layer.11.attention.self.key.quant\n",
      "roberta.encoder.layer.11.attention.self.key.quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.key.dequant\n",
      "roberta.encoder.layer.11.attention.self.key.module\n",
      "roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.key.module.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.value\n",
      "roberta.encoder.layer.11.attention.self.value.quant\n",
      "roberta.encoder.layer.11.attention.self.value.quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.value.dequant\n",
      "roberta.encoder.layer.11.attention.self.value.module\n",
      "roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.value.module.activation_post_process\n",
      "roberta.encoder.layer.11.attention.self.dropout\n",
      "roberta.encoder.layer.11.attention.output\n",
      "roberta.encoder.layer.11.attention.output.dense\n",
      "roberta.encoder.layer.11.attention.output.dense.quant\n",
      "roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.attention.output.dense.dequant\n",
      "roberta.encoder.layer.11.attention.output.dense.module\n",
      "roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.attention.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm\n",
      "roberta.encoder.layer.11.attention.output.dropout\n",
      "roberta.encoder.layer.11.intermediate\n",
      "roberta.encoder.layer.11.intermediate.dense\n",
      "roberta.encoder.layer.11.intermediate.dense.quant\n",
      "roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.intermediate.dense.dequant\n",
      "roberta.encoder.layer.11.intermediate.dense.module\n",
      "roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.intermediate.dense.module.activation_post_process\n",
      "roberta.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "roberta.encoder.layer.11.output\n",
      "roberta.encoder.layer.11.output.dense\n",
      "roberta.encoder.layer.11.output.dense.quant\n",
      "roberta.encoder.layer.11.output.dense.quant.activation_post_process\n",
      "roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process\n",
      "roberta.encoder.layer.11.output.dense.dequant\n",
      "roberta.encoder.layer.11.output.dense.module\n",
      "roberta.encoder.layer.11.output.dense.module.weight_fake_quant\n",
      "roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process\n",
      "roberta.encoder.layer.11.output.dense.module.activation_post_process\n",
      "roberta.encoder.layer.11.output.LayerNorm\n",
      "roberta.encoder.layer.11.output.dropout\n",
      "classifier\n",
      "classifier.dense\n",
      "classifier.dropout\n",
      "classifier.out_proj\n"
     ]
    }
   ],
   "source": [
    "for n_, _ in m.named_modules():\n",
    "    print(n_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 15:30:14 sparseml.export.export INFO     Starting export for transformers model...\n",
      "2024-05-14 15:30:14 sparseml.export.export WARNING  Deployment directory at: ./oneshot_deployment/deployment already exists.Overwriting the existing deployment directory... \n",
      "2024-05-14 15:30:14 sparseml.transformers.integration_helper_functions INFO     Fetching default helper functions for transformers integration\n",
      "2024-05-14 15:30:14 sparseml.export.export INFO     Creating model for the export...\n",
      "2024-05-14 15:30:14 sparseml.transformers.integration_helper_functions WARNING  trust_remote_code is set to False. It is possible, that the model will not be loaded correctly.\n",
      "2024-05-14 15:30:14 sparseml.transformers.utils.sparse_model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied /root/sparseml/oneshot_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:3765] 2024-05-14 15:30:15,241 >> Some weights of the model checkpoint at /root/sparseml/oneshot_output were not used when initializing RobertaForSequenceClassification: ['roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.weight', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.output.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.module.bias', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.module.bias', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.module.weight', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.bias', 'roberta.encoder.layer.0.intermediate.dense.module.bias', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.module.weight', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.8.intermediate.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.key.module.weight', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.scale', 'roberta.embeddings.position_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.output.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.query.module.bias', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.output.dense.module.bias', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.output.dense.module.weight', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.intermediate.dense.module.bias', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.bias', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.intermediate.dense.module.weight', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.output.dense.module.weight', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.module.bias', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.module.bias', 'roberta.encoder.layer.2.attention.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.module.weight', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.weight', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.query.module.weight', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.bias', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.module.bias', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.output.dense.module.weight', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.attention.output.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.query.module.bias', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.key.module.weight', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.bias', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.6.intermediate.dense.module.bias', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.query.module.weight', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.module.weight', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.bias', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.weight', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.module.weight', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.bias', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.bias', 'roberta.encoder.layer.10.intermediate.dense.module.bias', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.output.dense.module.bias', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.key.module.weight', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.bias', 'roberta.encoder.layer.10.attention.self.key.module.bias', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.intermediate.dense.module.weight', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.output.dense.module.bias', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.output.dense.module.bias', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.bias', 'roberta.encoder.layer.5.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.output.dense.module.weight', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.module.bias', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.output.dense.module.weight', 'roberta.encoder.layer.9.attention.self.value.module.bias', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.query.module.bias', 'roberta.encoder.layer.5.attention.self.value.module.bias', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.module.weight', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.7.attention.self.value.module.weight', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.output.dense.module.weight', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.module.weight', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.bias', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.value.module.bias', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.output.dense.module.bias', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.module.bias', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.output.dense.module.bias', 'roberta.encoder.layer.3.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.6.output.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.self.query.module.bias', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.key.module.weight', 'roberta.encoder.layer.8.attention.self.query.module.bias', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.value.module.weight', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.module.bias', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.key.module.weight', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight', 'roberta.encoder.layer.9.output.dense.module.bias', 'roberta.encoder.layer.11.attention.self.key.module.bias', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.intermediate.dense.module.weight', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.output.dense.module.weight', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.embeddings.word_embeddings.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.module.bias', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.scale', 'roberta.embeddings.word_embeddings.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.output.dense.module.bias', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.attention.output.dense.module.weight', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.module.bias', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.query.module.weight', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.query.module.weight', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.output.dense.module.weight', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.module.weight', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.embeddings.position_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.7.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.output.dense.module.bias', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.6.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.output.dense.module.weight', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.module.weight', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.1.attention.output.dense.module.weight', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.query.module.bias', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.self.value.module.weight', 'roberta.encoder.layer.3.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.query.module.weight', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.module.bias', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.bias', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.output.dense.module.bias', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.1.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.self.value.module.weight', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.0.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.8.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.11.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.8.output.dense.module.bias', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.scale', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.attention.self.key.module.bias', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.0.attention.self.key.module.bias', 'roberta.encoder.layer.7.output.dense.module.bias', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.2.attention.output.dense.module.weight', 'roberta.encoder.layer.5.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.2.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.scale', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.9.intermediate.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.bias', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.10.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.1.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.7.attention.self.key.module.weight', 'roberta.encoder.layer.9.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.output.dense.module.weight', 'roberta.encoder.layer.9.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.11.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.value.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.6.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.7.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.3.intermediate.dense.module.bias', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.7.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.8.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.4.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.5.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.11.intermediate.dense.module.bias', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.8.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.9.attention.self.value.module.weight', 'roberta.encoder.layer.1.attention.self.query.module.weight', 'roberta.encoder.layer.11.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.query.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.0.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.output.dense.module.bias', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.query.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.11.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.4.attention.self.query.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.bias', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.5.attention.output.dense.module.bias', 'roberta.encoder.layer.8.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.embeddings.word_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.output.dense.module.weight', 'roberta.encoder.layer.1.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.9.attention.self.query.module.weight', 'roberta.encoder.layer.6.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.9.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.0.attention.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.7.output.dense.module.weight_fake_quant.observer_enabled', 'roberta.embeddings.position_embeddings.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.intermediate.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.9.attention.self.query.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.7.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.attention.self.value.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.quant.activation_post_process.scale', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.value.module.bias', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.5.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.key.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.10.attention.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.5.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.11.attention.self.key.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.value.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.11.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.3.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.10.attention.self.query.module.weight_fake_quant.scale', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.attention.output.dense.module.weight', 'roberta.encoder.layer.7.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.6.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.attention.self.key.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.attention.self.query.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.5.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.9.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.key.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.4.attention.self.key.module.weight', 'roberta.encoder.layer.0.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.1.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.2.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.0.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.3.attention.self.value.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.scale', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.10.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.7.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.output.dense.quant.activation_post_process.zero_point', 'roberta.embeddings.token_type_embeddings.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.4.attention.self.query.module.weight', 'roberta.encoder.layer.2.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.4.attention.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.3.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.key.quant.activation_post_process.zero_point', 'roberta.encoder.layer.5.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.6.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.9.output.dense.module.weight', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.scale', 'roberta.encoder.layer.3.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.observer_enabled', 'roberta.encoder.layer.8.attention.output.dense.module.weight', 'roberta.encoder.layer.2.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.11.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.0.attention.self.query.module.weight', 'roberta.encoder.layer.3.output.dense.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.9.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.10.attention.output.dense.module.weight_fake_quant.zero_point', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.8.output.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.5.attention.output.dense.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.10.intermediate.dense.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.8.attention.self.query.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.1.output.dense.module.weight_fake_quant.scale', 'roberta.encoder.layer.4.intermediate.dense.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.3.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.1.attention.self.key.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.zero_point', 'roberta.encoder.layer.6.attention.self.value.quant.activation_post_process.fake_quant_enabled', 'roberta.encoder.layer.1.attention.self.key.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.4.intermediate.dense.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.4.attention.output.dense.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.3.attention.self.key.module.weight', 'roberta.encoder.layer.2.attention.self.key.module.weight', 'roberta.encoder.layer.4.attention.self.query.quant.activation_post_process.zero_point', 'roberta.encoder.layer.8.attention.output.dense.quant.activation_post_process.activation_post_process.eps', 'roberta.encoder.layer.5.attention.self.key.quant.activation_post_process.observer_enabled', 'roberta.encoder.layer.5.output.dense.quant.activation_post_process.activation_post_process.min_val', 'roberta.encoder.layer.0.attention.self.value.module.weight_fake_quant.activation_post_process.min_val', 'roberta.encoder.layer.10.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.6.attention.self.value.module.weight_fake_quant.scale', 'roberta.encoder.layer.9.attention.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.4.intermediate.dense.module.weight', 'roberta.encoder.layer.8.attention.self.key.module.weight_fake_quant.activation_post_process.eps', 'roberta.encoder.layer.2.attention.self.key.module.bias', 'roberta.encoder.layer.8.output.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.1.attention.self.query.module.weight_fake_quant.fake_quant_enabled', 'roberta.encoder.layer.7.intermediate.dense.quant.activation_post_process.activation_post_process.max_val', 'roberta.encoder.layer.6.intermediate.dense.module.weight', 'roberta.encoder.layer.11.attention.self.value.module.weight_fake_quant.activation_post_process.max_val', 'roberta.encoder.layer.2.output.dense.quant.activation_post_process.scale']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3777] 2024-05-14 15:30:15,242 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /root/sparseml/oneshot_output and are newly initialized: ['roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-14 15:30:15 sparseml.transformers.utils.helpers INFO     Found recipe in the model_path: /root/sparseml/oneshot_output/recipe.yaml\n",
      "2024-05-14 15:30:15 sparseml.core.recipe.recipe INFO     Loading recipe from file /root/sparseml/oneshot_output/recipe.yaml\n",
      "manager stage: Model structure initialized\n",
      "2024-05-14 15:30:15 sparseml.pytorch.model_load.helpers INFO     Applied an unstaged recipe to the model at /root/sparseml/oneshot_output\n",
      "2024-05-14 15:30:15 sparseml.pytorch.model_load.helpers INFO     Reloaded 1230 model params for SparseML Recipe from /root/sparseml/oneshot_output\n",
      "2024-05-14 15:30:16 sparseml.pytorch.model_load.helpers INFO     Loaded student from /root/sparseml/oneshot_output with 124647939 total params. Of those there are 85526784 prunable params which have 11.718467047702857 avg sparsity.\n",
      "2024-05-14 15:30:16 sparseml.pytorch.model_load.helpers INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 124647939, \"sparse\": 10023196, \"sparsity_percent\": 8.041204756702797, \"prunable\": 85526784, \"prunable_sparse\": 10022428, \"prunable_sparsity_percent\": 11.718467047702857, \"quantizable\": 85610499, \"quantized\": 85017600, \"quantized_percent\": 99.30744592436028}, \"params_info\": {\"roberta.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02089436911046505, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0292629674077034, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02561102993786335, \"quantized\": true}, \"roberta.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0823177769780159, \"quantized\": true}, \"roberta.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.06538306176662445, \"quantized\": true}, \"roberta.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1479068398475647, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0253618024289608, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0338473841547966, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0316043421626091, \"quantized\": true}, \"roberta.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0757090225815773, \"quantized\": true}, \"roberta.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.06315655261278152, \"quantized\": true}, \"roberta.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.23669010400772095, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02553643099963665, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0330963134765625, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0389777272939682, \"quantized\": true}, \"roberta.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0653042271733284, \"quantized\": true}, \"roberta.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.041697606444358826, \"quantized\": true}, \"roberta.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.16139094531536102, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0308397077023983, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0334286168217659, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0319112129509449, \"quantized\": true}, \"roberta.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0632951557636261, \"quantized\": true}, \"roberta.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0430857352912426, \"quantized\": true}, \"roberta.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.19721010327339172, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0278811976313591, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0321129709482193, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0274946428835392, \"quantized\": true}, \"roberta.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0589836984872818, \"quantized\": true}, \"roberta.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.048330944031476974, \"quantized\": true}, \"roberta.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.252916544675827, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0282575823366642, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0331217460334301, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0276777483522892, \"quantized\": true}, \"roberta.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0560760498046875, \"quantized\": true}, \"roberta.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.057293787598609924, \"quantized\": true}, \"roberta.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.2977595925331116, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0335879847407341, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0337066650390625, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0344865582883358, \"quantized\": true}, \"roberta.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0522172711789608, \"quantized\": true}, \"roberta.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0504065603017807, \"quantized\": true}, \"roberta.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.307538777589798, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.026824951171875, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02929009310901165, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02887471579015255, \"quantized\": true}, \"roberta.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0582953542470932, \"quantized\": true}, \"roberta.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0503031425178051, \"quantized\": true}, \"roberta.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3988359272480011, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02451578713953495, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0331234410405159, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02893575094640255, \"quantized\": true}, \"roberta.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0550316721200943, \"quantized\": true}, \"roberta.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0568813756108284, \"quantized\": true}, \"roberta.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3552602231502533, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0250125452876091, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02913750521838665, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0304599329829216, \"quantized\": true}, \"roberta.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0519053153693676, \"quantized\": true}, \"roberta.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.057056427001953125, \"quantized\": true}, \"roberta.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31312432885169983, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02436319924890995, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02783881314098835, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02444966696202755, \"quantized\": true}, \"roberta.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0576510950922966, \"quantized\": true}, \"roberta.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.05695851519703865, \"quantized\": true}, \"roberta.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3723564147949219, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02547539584338665, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0274658203125, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02739630825817585, \"quantized\": true}, \"roberta.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0513543039560318, \"quantized\": true}, \"roberta.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.02618408203125, \"quantized\": true}, \"roberta.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.14533022046089172, \"quantized\": true}, \"classifier.dense.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": false}, \"classifier.out_proj.weight\": {\"numel\": 2304, \"sparsity\": 0.0, \"quantized\": false}}}\n",
      "2024-05-14 15:30:16 sparseml.pytorch.model_load.helpers INFO     Reloaded model state after SparseML recipe structure modifications from /root/sparseml/oneshot_output\n",
      "2024-05-14 15:30:16 sparseml.pytorch.model_load.helpers INFO     Delayed load of model /root/sparseml/oneshot_output detected. Will print out model information once SparseML recipes have loaded\n",
      "2024-05-14 15:30:16 sparseml.export.export INFO     Creating data loader for the export...\n",
      "2024-05-14 15:30:16 sparseml.export.export INFO     Created additional items that will be used for the export: ['tokenizer', 'sequence_length', 'config', 'input_names']\n",
      "2024-05-14 15:30:16 sparseml.export.export INFO     Exporting model.onnx to ./oneshot_deployment...\n",
      "/root/sparseml/src/sparseml/modifiers/quantization/utils/fake_quant_wrapper.py:43: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.fake_quant_enabled[0] == 0:\n",
      "2024-05-14 15:30:23 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 131 matches\n",
      "2024-05-14 15:30:24 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 163 matches\n",
      "2024-05-14 15:30:24 sparseml.exporters.transforms.onnx_transform INFO     [UnwrapBatchNorms] Transformed 0 matches\n",
      "2024-05-14 15:30:25 sparseml.exporters.transforms.onnx_transform INFO     [DeleteTrivialOnnxAdds] Transformed 0 matches\n",
      "2024-05-14 15:30:26 sparseml.exporters.transforms.onnx_transform INFO     [ConstantsToInitializers] Transformed 393 matches\n",
      "2024-05-14 15:30:26 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 0 matches\n",
      "2024-05-14 15:30:26 sparseml.exporters.transforms.onnx_transform INFO     [InitializersToUint8] Transformed 40 matches\n",
      "2024-05-14 15:30:27 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 0 matches\n",
      "2024-05-14 15:30:27 sparseml.exporters.transforms.onnx_transform INFO     [FoldConvDivBn] Transformed 0 matches\n",
      "2024-05-14 15:30:27 sparseml.exporters.transforms.onnx_transform INFO     [DeleteRepeatedQdq] Transformed 0 matches\n",
      "2024-05-14 15:30:28 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeQATEmbedding] Transformed 3 matches\n",
      "2024-05-14 15:30:28 sparseml.exporters.transforms.onnx_transform INFO     [PropagateEmbeddingQuantization] Transformed 0 matches\n",
      "2024-05-14 15:30:28 sparseml.exporters.transforms.onnx_transform INFO     [PropagateDequantThroughSplit] Transformed 0 matches\n",
      "2024-05-14 15:30:30 sparseml.exporters.transforms.onnx_transform INFO     [MatMulAddToMatMulIntegerAddCastMul] Transformed 72 matches\n",
      "2024-05-14 15:30:30 sparseml.exporters.transforms.onnx_transform INFO     [MatMulToMatMulIntegerCastMul] Transformed 0 matches\n",
      "2024-05-14 15:30:31 sparseml.exporters.transforms.onnx_transform INFO     [FoldReLUQuants] Transformed 0 matches\n",
      "2024-05-14 15:30:31 sparseml.exporters.transforms.onnx_transform INFO     [ConvToConvIntegerAddCastMul] Transformed 0 matches\n",
      "2024-05-14 15:30:31 sparseml.exporters.transforms.onnx_transform INFO     [GemmToQLinearMatMul] Transformed 0 matches\n",
      "2024-05-14 15:30:31 sparseml.exporters.transforms.onnx_transform INFO     [GemmToMatMulIntegerAddCastMul] Transformed 0 matches\n",
      "2024-05-14 15:30:33 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeResiduals] Transformed 0 matches\n",
      "2024-05-14 15:30:33 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQConvWeights] Transformed 0 matches\n",
      "2024-05-14 15:30:34 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQuantizeOps] Transformed 0 matches\n",
      "2024-05-14 15:30:34 sparseml.export.export INFO     Successfully exported model.onnx to ./oneshot_deployment/model.onnx...\n",
      "2024-05-14 15:30:34 sparseml.export.export INFO     Creating deployment folder deployment at directory: ./oneshot_deployment...\n",
      "2024-05-14 15:30:34 sparseml.export.helpers WARNING  Optional file tokenizer.model not found in source path /root/sparseml/oneshot_output\n",
      "2024-05-14 15:30:34 sparseml.export.export INFO     Applying optimizations: all to the exported model...\n",
      "2024-05-14 15:30:34 sparseml.export.export INFO     Validating model structure...\n",
      "2024-05-14 15:30:34 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-inputs is missing.\n",
      "2024-05-14 15:30:34 sparseml.export.validators WARNING  File ./oneshot_deployment/deployment/tokenizer.model is missing.\n",
      "2024-05-14 15:30:34 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-outputs is missing.\n",
      "2024-05-14 15:30:34 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-labels is missing.\n",
      "2024-05-14 15:30:34 sparseml.export.export INFO     Successfully exported model from:\n",
      "./oneshot_deployment\n",
      "to\n",
      "./oneshot_deployment/deployment\n",
      "for integration: transformers\n"
     ]
    }
   ],
   "source": [
    "### Export\n",
    "export(source_path = \"./oneshot_output\", target_path = \"./oneshot_deployment\", task=\"text-classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
