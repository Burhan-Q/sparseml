{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/sparseml/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Note, working with 1.7 Deepsparse and SparseML here\n",
    "\n",
    "from sparseml.transformers import oneshot, SparseAutoModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "from typing import Union\n",
    "from evaluate import evaluator\n",
    "from sparseml import export\n",
    "import sparseml.core.session as session_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "dataset_name = \"tweet_eval\"\n",
    "dataset_subname = \"sentiment\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset_train = load_dataset(dataset_name, dataset_subname, split=\"train\")\n",
    "dataset_test = load_dataset(dataset_name, dataset_subname, split=\"test\").shuffle(seed=420).select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"\"\"\n",
    "test_stage:\n",
    "  obcq_modifiers:\n",
    "    SmoothQuantModifier:\n",
    "      smoothing_strength: 0.5\n",
    "      mappings: [\n",
    "            [[\"re:.*query\", \"re:.*key\", \"re:.*value\"], \"re:.*output.LayerNorm\"],\n",
    "            [[\"re:.*intermediate.dense\"], \"re:.*output.LayerNorm\"],\n",
    "        ]\n",
    "    QuantizationModifier:\n",
    "      scheme_overrides:\n",
    "        Embedding:\n",
    "          input_activations: null\n",
    "          weights:\n",
    "            num_bits: 8\n",
    "            symmetric: false\n",
    "        Linear:\n",
    "          input_activations:\n",
    "            num_bits: 8\n",
    "            symmetric: false\n",
    "          weights:\n",
    "            num_bits: 8\n",
    "            symmetric: true\n",
    "    SparseGPTModifier:\n",
    "      sparsity: 0.0\n",
    "      quantize: true\n",
    "      targets: [\"re:roberta.encoder.layer.\\\\\\d+$\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:05:46 sparseml.transformers.finetune.text_generation WARNING  Process rank: 0, device: cuda:0, n_gpu: 2, distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:05:46 sparseml.transformers.finetune.text_generation WARNING  Moving cardiffnlp/twitter-roberta-base-sentiment-latest to device cuda:0 for One-Shot\n",
      "[WARNING|modeling_roberta.py:882] 2024-05-10 09:05:46,760 >> If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "2024-05-10 09:05:47 sparseml.transformers.utils.helpers INFO     model_path is a huggingface model id. Attempting to download recipe from https://huggingface.co/\n",
      "2024-05-10 09:05:47 sparseml.transformers.utils.helpers INFO     Found recipe: recipe.yaml for model id: cardiffnlp/twitter-roberta-base-sentiment-latest. Downloading...\n",
      "2024-05-10 09:05:47 sparseml.transformers.utils.helpers INFO     Unable to to find recipe recipe.yaml for model id: cardiffnlp/twitter-roberta-base-sentiment-latest: 404 Client Error. (Request ID: Root=1-663de36b-43d6d733132f1fde79c2a9d1;8802b9c9-52e6-4482-aad5-8a120f59f6ec)\n",
      "\n",
      "Entry Not Found for url: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest/resolve/main/recipe.yaml.. Skipping recipe resolution.\n",
      "2024-05-10 09:05:47 sparseml.transformers.utils.helpers INFO     Failed to infer the recipe from the model_path\n",
      "Running tokenizer on dataset: 100%|██████████| 45615/45615 [00:02<00:00, 16540.07 examples/s]\n",
      "Adding labels: 100%|██████████| 45615/45615 [00:02<00:00, 18079.87 examples/s]\n",
      "/root/sparseml/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "2024-05-10 09:05:52 sparseml.transformers.finetune.runner INFO     *** One Shot ***\n",
      "2024-05-10 09:05:52 sparseml.core.recipe.recipe WARNING  Could not process input as a file path or zoo stub, attempting to process it as a string.\n",
      "2024-05-10 09:05:52 sparseml.core.recipe.recipe WARNING  Input string: \n",
      "test_stage:\n",
      "  obcq_modifiers:\n",
      "    SmoothQuantModifier:\n",
      "      smoothing_strength: 0.5\n",
      "      mappings: [\n",
      "            [[\"re:.*query\", \"re:.*key\", \"re:.*value\"], \"re:.*output.LayerNorm\"],\n",
      "            [[\"re:.*intermediate.dense\"], \"re:.*output.LayerNorm\"],\n",
      "        ]\n",
      "    QuantizationModifier:\n",
      "      scheme_overrides:\n",
      "        Embedding:\n",
      "          input_activations: null\n",
      "          weights:\n",
      "            num_bits: 8\n",
      "            symmetric: false\n",
      "        Linear:\n",
      "          input_activations:\n",
      "            num_bits: 8\n",
      "            symmetric: false\n",
      "          weights:\n",
      "            num_bits: 8\n",
      "            symmetric: true\n",
      "    SparseGPTModifier:\n",
      "      sparsity: 0.0\n",
      "      quantize: true\n",
      "      targets: [\"re:roberta.encoder.layer.\\\\d+$\"]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask', 'labels']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:05:53 sparseml.modifiers.smoothquant.pytorch INFO     Running SmoothQuantModifier calibration with 512 samples...\n",
      "100%|██████████| 512/512 [00:04<00:00, 116.73it/s]\n",
      "2024-05-10 09:05:57 sparseml.modifiers.smoothquant.pytorch INFO     Smoothing activation scales...\n",
      "2024-05-10 09:05:57 sparseml.modifiers.quantization.pytorch INFO     Running QuantizationModifier calibration with 512 samples...\n",
      "100%|██████████| 512/512 [00:45<00:00, 11.25it/s]\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.0 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.1 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.2 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.3 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.4 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.5 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.6 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.7 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.8 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.9 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.10 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Preparing roberta.encoder.layer.11 for compression\n",
      "2024-05-10 09:06:43 sparseml.modifiers.pruning.wanda.pytorch INFO     Running SparseGPTModifier calibration with 512 samples...\n",
      "100%|██████████| 512/512 [00:23<00:00, 22.12it/s]\n",
      "2024-05-10 09:07:06 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 1/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:06 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.query.module...\n",
      "2024-05-10 09:07:06 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:06 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 50.32\n",
      "2024-05-10 09:07:06 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.key.module...\n",
      "2024-05-10 09:07:06 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:06 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 29.30\n",
      "2024-05-10 09:07:06 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.self.value.module...\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.64\n",
      "2024-05-10 09:07:07 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.attention.output.dense.module...\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 21.88\n",
      "2024-05-10 09:07:07 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.intermediate.dense.module...\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:07 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 13.94\n",
      "2024-05-10 09:07:07 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.0.roberta.encoder.layer.0.output.dense.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.71\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 13.56\n",
      "2024-05-10 09:07:08 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 2/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.query.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 25.77\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.key.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 36.57\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.self.value.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.48\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.attention.output.dense.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.12\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.intermediate.dense.module...\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:08 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.00\n",
      "2024-05-10 09:07:08 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.1.roberta.encoder.layer.1.output.dense.module...\n",
      "2024-05-10 09:07:09 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-10 09:07:09 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 9.02\n",
      "2024-05-10 09:07:09 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 3/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:09 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.query.module...\n",
      "2024-05-10 09:07:09 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:09 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.80\n",
      "2024-05-10 09:07:09 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.key.module...\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.21\n",
      "2024-05-10 09:07:10 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.self.value.module...\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.54\n",
      "2024-05-10 09:07:10 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.attention.output.dense.module...\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.33\n",
      "2024-05-10 09:07:10 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.intermediate.dense.module...\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:10 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.85\n",
      "2024-05-10 09:07:10 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.2.roberta.encoder.layer.2.output.dense.module...\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.68\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.59\n",
      "2024-05-10 09:07:11 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 4/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:11 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.query.module...\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.52\n",
      "2024-05-10 09:07:11 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.key.module...\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.27\n",
      "2024-05-10 09:07:11 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.self.value.module...\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.43\n",
      "2024-05-10 09:07:11 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.attention.output.dense.module...\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:11 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.94\n",
      "2024-05-10 09:07:11 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.intermediate.dense.module...\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.24\n",
      "2024-05-10 09:07:12 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.3.roberta.encoder.layer.3.output.dense.module...\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.68\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 4.40\n",
      "2024-05-10 09:07:12 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 5/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:12 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.query.module...\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:12 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.15\n",
      "2024-05-10 09:07:12 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.key.module...\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.60\n",
      "2024-05-10 09:07:13 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.self.value.module...\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.65\n",
      "2024-05-10 09:07:13 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.attention.output.dense.module...\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.00\n",
      "2024-05-10 09:07:13 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.intermediate.dense.module...\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:13 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.49\n",
      "2024-05-10 09:07:13 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.4.roberta.encoder.layer.4.output.dense.module...\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.68\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 8.61\n",
      "2024-05-10 09:07:14 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 6/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:14 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.query.module...\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.80\n",
      "2024-05-10 09:07:14 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.key.module...\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.87\n",
      "2024-05-10 09:07:14 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.self.value.module...\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.56\n",
      "2024-05-10 09:07:14 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.attention.output.dense.module...\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:14 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.65\n",
      "2024-05-10 09:07:14 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.intermediate.dense.module...\n",
      "2024-05-10 09:07:15 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:15 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.57\n",
      "2024-05-10 09:07:15 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.5.roberta.encoder.layer.5.output.dense.module...\n",
      "2024-05-10 09:07:15 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.68\n",
      "2024-05-10 09:07:15 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 10.65\n",
      "2024-05-10 09:07:15 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 7/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:15 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.query.module...\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.44\n",
      "2024-05-10 09:07:16 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.key.module...\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.64\n",
      "2024-05-10 09:07:16 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.self.value.module...\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.55\n",
      "2024-05-10 09:07:16 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.attention.output.dense.module...\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.93\n",
      "2024-05-10 09:07:16 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.intermediate.dense.module...\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:16 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.05\n",
      "2024-05-10 09:07:16 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.6.roberta.encoder.layer.6.output.dense.module...\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.69\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 13.53\n",
      "2024-05-10 09:07:17 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 8/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:17 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.query.module...\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.41\n",
      "2024-05-10 09:07:17 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.key.module...\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.62\n",
      "2024-05-10 09:07:17 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.self.value.module...\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.19\n",
      "2024-05-10 09:07:17 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.29\n",
      "2024-05-10 09:07:17 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.attention.output.dense.module...\n",
      "2024-05-10 09:07:18 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:18 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.37\n",
      "2024-05-10 09:07:18 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.intermediate.dense.module...\n",
      "2024-05-10 09:07:18 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:18 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.59\n",
      "2024-05-10 09:07:18 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.7.roberta.encoder.layer.7.output.dense.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.69\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 63.13\n",
      "2024-05-10 09:07:19 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 9/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.query.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.19\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.key.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.05\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.self.value.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.29\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.attention.output.dense.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.77\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.intermediate.dense.module...\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:19 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.40\n",
      "2024-05-10 09:07:19 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.8.roberta.encoder.layer.8.output.dense.module...\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.71\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 28.36\n",
      "2024-05-10 09:07:20 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 10/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:20 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.query.module...\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.28\n",
      "2024-05-10 09:07:20 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.key.module...\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:20 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 2.58\n",
      "2024-05-10 09:07:20 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.self.value.module...\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.45\n",
      "2024-05-10 09:07:21 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.attention.output.dense.module...\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.72\n",
      "2024-05-10 09:07:21 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.intermediate.dense.module...\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:21 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 3.40\n",
      "2024-05-10 09:07:21 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.9.roberta.encoder.layer.9.output.dense.module...\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.69\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 10.37\n",
      "2024-05-10 09:07:22 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 11/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:22 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.query.module...\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.49\n",
      "2024-05-10 09:07:22 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.key.module...\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.65\n",
      "2024-05-10 09:07:22 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.self.value.module...\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.17\n",
      "2024-05-10 09:07:22 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.attention.output.dense.module...\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:22 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.45\n",
      "2024-05-10 09:07:22 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.intermediate.dense.module...\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.47\n",
      "2024-05-10 09:07:23 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.10.roberta.encoder.layer.10.output.dense.module...\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.71\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 6.86\n",
      "2024-05-10 09:07:23 sparseml.modifiers.pruning.wanda.pytorch INFO     \n",
      "===== Compressing layer 12/12 to sparsity 0.0 =====\n",
      "2024-05-10 09:07:23 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.query.module...\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.18\n",
      "2024-05-10 09:07:23 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.01\n",
      "2024-05-10 09:07:23 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.key.module...\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.12\n",
      "2024-05-10 09:07:24 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.self.value.module...\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.13\n",
      "2024-05-10 09:07:24 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.attention.output.dense.module...\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.13\n",
      "2024-05-10 09:07:24 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.intermediate.dense.module...\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.17\n",
      "2024-05-10 09:07:24 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 1.00\n",
      "2024-05-10 09:07:24 sparseml.modifiers.utils.layer_compressor INFO     Compressing roberta.encoder.layer.11.roberta.encoder.layer.11.output.dense.module...\n",
      "2024-05-10 09:07:25 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     time 0.70\n",
      "2024-05-10 09:07:25 sparseml.modifiers.obcq.utils.sgpt_wrapper INFO     error 0.76\n",
      "manager stage: Modifiers initialized\n",
      "manager stage: Modifiers finalized\n",
      "2024-05-10 09:07:26 sparseml.pytorch.model_load.helpers INFO     Saving output to /root/sparseml/oneshot_output\n"
     ]
    }
   ],
   "source": [
    "### Apply One-Shot\n",
    "\n",
    "def format_data(data):\n",
    "    return {\"text\": data[\"text\"]}\n",
    "\n",
    "oneshot(\n",
    "    model=model_name,\n",
    "    dataset=dataset_train,\n",
    "    recipe=recipe,\n",
    "    preprocessing_func = format_data,\n",
    "    output_dir=\"./oneshot_output\",\n",
    "    pad_to_max_length=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:08:16 sparseml.transformers.utils.sparse_model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied ./oneshot_output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:3777] 2024-05-10 09:08:16,936 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./oneshot_output and are newly initialized: ['roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'classifier.out_proj.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'classifier.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'classifier.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-10 09:08:16 sparseml.transformers.utils.helpers INFO     Found recipe in the model_path: ./oneshot_output/recipe.yaml\n",
      "2024-05-10 09:08:16 sparseml.core.recipe.recipe INFO     Loading recipe from file ./oneshot_output/recipe.yaml\n",
      "manager stage: Model structure initialized\n",
      "2024-05-10 09:08:16 sparseml.pytorch.model_load.helpers INFO     Applied an unstaged recipe to the model at ./oneshot_output\n",
      "[WARNING|modeling_utils.py:3777] 2024-05-10 09:08:17,548 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./oneshot_output and are newly initialized: ['classifier.dense.module.weight_fake_quant.observer_enabled', 'classifier.dense.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.module.weight_fake_quant.scale', 'classifier.out_proj.module.weight', 'classifier.dense.quant.activation_post_process.zero_point', 'classifier.out_proj.quant.activation_post_process.zero_point', 'classifier.dense.quant.activation_post_process.observer_enabled', 'classifier.dense.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.quant.activation_post_process.scale', 'classifier.dense.module.weight_fake_quant.activation_post_process.max_val', 'classifier.dense.module.weight_fake_quant.activation_post_process.eps', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.max_val', 'classifier.out_proj.quant.activation_post_process.scale', 'classifier.dense.module.bias', 'classifier.out_proj.quant.activation_post_process.activation_post_process.eps', 'classifier.dense.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.module.weight_fake_quant.scale', 'classifier.dense.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.module.weight', 'classifier.out_proj.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.out_proj.quant.activation_post_process.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.module.bias', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.eps']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers WARNING  Missing keys found when reloading model state for SparseML recipe:['classifier.dense.module.weight_fake_quant.observer_enabled', 'classifier.dense.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.module.weight_fake_quant.scale', 'classifier.out_proj.module.weight', 'classifier.dense.quant.activation_post_process.zero_point', 'classifier.out_proj.quant.activation_post_process.zero_point', 'classifier.dense.quant.activation_post_process.observer_enabled', 'classifier.dense.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.quant.activation_post_process.scale', 'classifier.dense.module.weight_fake_quant.activation_post_process.max_val', 'classifier.dense.module.weight_fake_quant.activation_post_process.eps', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.max_val', 'classifier.out_proj.quant.activation_post_process.scale', 'classifier.dense.module.bias', 'classifier.out_proj.quant.activation_post_process.activation_post_process.eps', 'classifier.dense.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.module.weight_fake_quant.scale', 'classifier.dense.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.module.weight', 'classifier.out_proj.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.out_proj.quant.activation_post_process.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.module.bias', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.eps']\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers WARNING  Unexpected keys found when reloading model state for SparseML recipe:['lm_head.decoder.quant.activation_post_process.zero_point', 'lm_head.decoder.module.weight_fake_quant.fake_quant_enabled', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.eps', 'lm_head.decoder.module.bias', 'lm_head.dense.module.weight_fake_quant.observer_enabled', 'lm_head.layer_norm.quant.activation_post_process.scale', 'lm_head.decoder.quant.activation_post_process.observer_enabled', 'lm_head.dense.quant.activation_post_process.zero_point', 'lm_head.dense.module.weight_fake_quant.activation_post_process.eps', 'lm_head.dense.module.bias', 'lm_head.decoder.module.weight_fake_quant.scale', 'lm_head.decoder.quant.activation_post_process.fake_quant_enabled', 'lm_head.layer_norm.quant.activation_post_process.observer_enabled', 'lm_head.decoder.module.weight_fake_quant.observer_enabled', 'lm_head.decoder.quant.activation_post_process.scale', 'lm_head.dense.module.weight_fake_quant.fake_quant_enabled', 'lm_head.dense.quant.activation_post_process.fake_quant_enabled', 'lm_head.layer_norm.quant.activation_post_process.zero_point', 'lm_head.decoder.quant.activation_post_process.activation_post_process.max_val', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.max_val', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.min_val', 'lm_head.decoder.quant.activation_post_process.activation_post_process.eps', 'lm_head.dense.quant.activation_post_process.activation_post_process.eps', 'lm_head.dense.module.weight_fake_quant.activation_post_process.min_val', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.min_val', 'lm_head.dense.quant.activation_post_process.activation_post_process.max_val', 'lm_head.layer_norm.module.weight', 'lm_head.dense.module.weight_fake_quant.zero_point', 'lm_head.dense.quant.activation_post_process.activation_post_process.min_val', 'lm_head.decoder.module.weight_fake_quant.zero_point', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.eps', 'lm_head.layer_norm.quant.activation_post_process.fake_quant_enabled', 'lm_head.dense.quant.activation_post_process.scale', 'lm_head.dense.quant.activation_post_process.observer_enabled', 'lm_head.dense.module.weight_fake_quant.scale', 'lm_head.decoder.module.weight', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.max_val', 'lm_head.decoder.quant.activation_post_process.activation_post_process.min_val', 'lm_head.dense.module.weight', 'lm_head.bias', 'lm_head.dense.module.weight_fake_quant.activation_post_process.max_val', 'lm_head.layer_norm.module.bias']\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers INFO     Reloaded 1485 model params for SparseML Recipe from ./oneshot_output\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers INFO     Loaded student from ./oneshot_output with 124647939 total params. Of those there are 85526784 prunable params which have 12.627738931467364 avg sparsity.\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 124647939, \"sparse\": 10801638, \"sparsity_percent\": 8.665717288755172, \"prunable\": 85526784, \"prunable_sparse\": 10800099, \"prunable_sparsity_percent\": 12.627738931467364, \"quantizable\": 85610499, \"quantized\": 85610499, \"quantized_percent\": 100.0}, \"params_info\": {\"roberta.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0401645228266716, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.03113640658557415, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0321282297372818, \"quantized\": true}, \"roberta.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0824008509516716, \"quantized\": true}, \"roberta.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.04801347479224205, \"quantized\": true}, \"roberta.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.14651447534561157, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0421329066157341, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0525377057492733, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0338049978017807, \"quantized\": true}, \"roberta.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0759921595454216, \"quantized\": true}, \"roberta.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.055060915648937225, \"quantized\": true}, \"roberta.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.23680199682712555, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0260637067258358, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0455305315554142, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0333896204829216, \"quantized\": true}, \"roberta.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0655924454331398, \"quantized\": true}, \"roberta.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.044445887207984924, \"quantized\": true}, \"roberta.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.16232426464557648, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02632988803088665, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0442318394780159, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0382605642080307, \"quantized\": true}, \"roberta.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0631120502948761, \"quantized\": true}, \"roberta.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0517289899289608, \"quantized\": true}, \"roberta.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1978318989276886, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0352528877556324, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0445454902946949, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0441979318857193, \"quantized\": true}, \"roberta.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0588141530752182, \"quantized\": true}, \"roberta.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.060012392699718475, \"quantized\": true}, \"roberta.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.2542177736759186, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02959696389734745, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0488976389169693, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0362684465944767, \"quantized\": true}, \"roberta.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0561184361577034, \"quantized\": true}, \"roberta.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.07045449316501617, \"quantized\": true}, \"roberta.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.29985299706459045, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0362124964594841, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510389544069767, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0326470285654068, \"quantized\": true}, \"roberta.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.052398681640625, \"quantized\": true}, \"roberta.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.07209396362304688, \"quantized\": true}, \"roberta.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31009674072265625, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0373602956533432, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0535990409553051, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0319146066904068, \"quantized\": true}, \"roberta.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0583224818110466, \"quantized\": true}, \"roberta.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.08474943041801453, \"quantized\": true}, \"roberta.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.4055476784706116, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.03656005859375, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510576032102108, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02870686911046505, \"quantized\": true}, \"roberta.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0551791712641716, \"quantized\": true}, \"roberta.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.08564291894435883, \"quantized\": true}, \"roberta.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3597395122051239, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0463528111577034, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510457344353199, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0348409004509449, \"quantized\": true}, \"roberta.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0517985038459301, \"quantized\": true}, \"roberta.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1108008474111557, \"quantized\": true}, \"roberta.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31593406200408936, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0291103795170784, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0565422922372818, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0291290283203125, \"quantized\": true}, \"roberta.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0573645681142807, \"quantized\": true}, \"roberta.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0795491561293602, \"quantized\": true}, \"roberta.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3796229958534241, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0537075474858284, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.057281494140625, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0306854248046875, \"quantized\": true}, \"roberta.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0516120046377182, \"quantized\": true}, \"roberta.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.05897776409983635, \"quantized\": true}, \"roberta.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1474219411611557, \"quantized\": true}, \"classifier.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.out_proj.module.weight\": {\"numel\": 2304, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers INFO     Reloaded model state after SparseML recipe structure modifications from ./oneshot_output\n",
      "2024-05-10 09:08:17 sparseml.pytorch.model_load.helpers INFO     Delayed load of model ./oneshot_output detected. Will print out model information once SparseML recipes have loaded\n",
      "[WARNING|modeling_utils.py:3765] 2024-05-10 09:08:32,982 >> Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation quantized model:\n",
      "{'accuracy': 0.332, 'total_time_in_seconds': 14.503595827962272, 'samples_per_second': 34.47420942577721, 'latency_in_seconds': 0.029007191655924545}\n",
      "Evaluation baseline model:\n",
      "{'accuracy': 0.682, 'total_time_in_seconds': 2.8320093309739605, 'samples_per_second': 176.55309060300456, 'latency_in_seconds': 0.005664018661947921}\n"
     ]
    }
   ],
   "source": [
    "### Evaluate\n",
    "\n",
    "active_session = session_manager.active_session()\n",
    "active_session.reset()\n",
    "\n",
    "def evaluate_model(model: Union[str, AutoModel]):\n",
    "    task_evaluator = evaluator(\"text-classification\")\n",
    "    eval_results = task_evaluator.compute(\n",
    "        model_or_pipeline=model,\n",
    "        tokenizer = tokenizer,\n",
    "        data=dataset_test,\n",
    "        metric=\"accuracy\",\n",
    "        label_mapping=config.label2id,\n",
    "        )\n",
    "    return eval_results\n",
    "\n",
    "eval_quant = evaluate_model(SparseAutoModel.text_classification_from_pretrained(\"./oneshot_output\"))\n",
    "eval_baseline = evaluate_model(model_name)\n",
    "\n",
    "print(f\"Evaluation quantized model:\\n{eval_quant}\")\n",
    "print(f\"Evaluation baseline model:\\n{eval_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 09:08:50 sparseml.export.export INFO     Starting export for transformers model...\n",
      "2024-05-10 09:08:50 sparseml.export.export WARNING  Deployment directory at: ./oneshot_deployment/deployment already exists.Overwriting the existing deployment directory... \n",
      "2024-05-10 09:08:50 sparseml.transformers.integration_helper_functions INFO     Fetching default helper functions for transformers integration\n",
      "2024-05-10 09:08:50 sparseml.export.export INFO     Creating model for the export...\n",
      "2024-05-10 09:08:50 sparseml.transformers.integration_helper_functions WARNING  trust_remote_code is set to False. It is possible, that the model will not be loaded correctly.\n",
      "2024-05-10 09:08:50 sparseml.transformers.utils.sparse_model WARNING  QAT state detected, ignore any loading errors, weights will reload after SparseML recipes have been applied /root/sparseml/oneshot_output\n",
      "[WARNING|modeling_utils.py:3777] 2024-05-10 09:08:51,261 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /root/sparseml/oneshot_output and are newly initialized: ['roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'classifier.out_proj.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'classifier.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'classifier.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'classifier.out_proj.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-10 09:08:51 sparseml.transformers.utils.helpers INFO     Found recipe in the model_path: /root/sparseml/oneshot_output/recipe.yaml\n",
      "2024-05-10 09:08:51 sparseml.core.recipe.recipe INFO     Loading recipe from file /root/sparseml/oneshot_output/recipe.yaml\n",
      "manager stage: Model structure initialized\n",
      "2024-05-10 09:08:51 sparseml.pytorch.model_load.helpers INFO     Applied an unstaged recipe to the model at /root/sparseml/oneshot_output\n",
      "[WARNING|modeling_utils.py:3777] 2024-05-10 09:08:51,713 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /root/sparseml/oneshot_output and are newly initialized: ['classifier.dense.module.weight_fake_quant.observer_enabled', 'classifier.dense.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.module.weight_fake_quant.scale', 'classifier.out_proj.module.weight', 'classifier.dense.quant.activation_post_process.zero_point', 'classifier.out_proj.quant.activation_post_process.zero_point', 'classifier.dense.quant.activation_post_process.observer_enabled', 'classifier.dense.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.quant.activation_post_process.scale', 'classifier.dense.module.weight_fake_quant.activation_post_process.max_val', 'classifier.dense.module.weight_fake_quant.activation_post_process.eps', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.max_val', 'classifier.out_proj.quant.activation_post_process.scale', 'classifier.dense.module.bias', 'classifier.out_proj.quant.activation_post_process.activation_post_process.eps', 'classifier.dense.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.module.weight_fake_quant.scale', 'classifier.dense.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.module.weight', 'classifier.out_proj.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.out_proj.quant.activation_post_process.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.module.bias', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.eps']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-10 09:08:51 sparseml.pytorch.model_load.helpers WARNING  Missing keys found when reloading model state for SparseML recipe:['classifier.dense.module.weight_fake_quant.observer_enabled', 'classifier.dense.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.module.weight_fake_quant.scale', 'classifier.out_proj.module.weight', 'classifier.dense.quant.activation_post_process.zero_point', 'classifier.out_proj.quant.activation_post_process.zero_point', 'classifier.dense.quant.activation_post_process.observer_enabled', 'classifier.dense.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.min_val', 'classifier.dense.quant.activation_post_process.scale', 'classifier.dense.module.weight_fake_quant.activation_post_process.max_val', 'classifier.dense.module.weight_fake_quant.activation_post_process.eps', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.max_val', 'classifier.out_proj.quant.activation_post_process.scale', 'classifier.dense.module.bias', 'classifier.out_proj.quant.activation_post_process.activation_post_process.eps', 'classifier.dense.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.module.weight_fake_quant.scale', 'classifier.dense.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.quant.activation_post_process.activation_post_process.min_val', 'classifier.out_proj.quant.activation_post_process.activation_post_process.max_val', 'classifier.dense.module.weight', 'classifier.out_proj.quant.activation_post_process.fake_quant_enabled', 'classifier.dense.quant.activation_post_process.activation_post_process.eps', 'classifier.out_proj.quant.activation_post_process.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.observer_enabled', 'classifier.out_proj.module.weight_fake_quant.zero_point', 'classifier.out_proj.module.weight_fake_quant.fake_quant_enabled', 'classifier.out_proj.module.bias', 'classifier.out_proj.module.weight_fake_quant.activation_post_process.eps']\n",
      "2024-05-10 09:08:51 sparseml.pytorch.model_load.helpers WARNING  Unexpected keys found when reloading model state for SparseML recipe:['lm_head.decoder.quant.activation_post_process.zero_point', 'lm_head.decoder.module.weight_fake_quant.fake_quant_enabled', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.eps', 'lm_head.decoder.module.bias', 'lm_head.dense.module.weight_fake_quant.observer_enabled', 'lm_head.layer_norm.quant.activation_post_process.scale', 'lm_head.decoder.quant.activation_post_process.observer_enabled', 'lm_head.dense.quant.activation_post_process.zero_point', 'lm_head.dense.module.weight_fake_quant.activation_post_process.eps', 'lm_head.dense.module.bias', 'lm_head.decoder.module.weight_fake_quant.scale', 'lm_head.decoder.quant.activation_post_process.fake_quant_enabled', 'lm_head.layer_norm.quant.activation_post_process.observer_enabled', 'lm_head.decoder.module.weight_fake_quant.observer_enabled', 'lm_head.decoder.quant.activation_post_process.scale', 'lm_head.dense.module.weight_fake_quant.fake_quant_enabled', 'lm_head.dense.quant.activation_post_process.fake_quant_enabled', 'lm_head.layer_norm.quant.activation_post_process.zero_point', 'lm_head.decoder.quant.activation_post_process.activation_post_process.max_val', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.max_val', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.min_val', 'lm_head.decoder.quant.activation_post_process.activation_post_process.eps', 'lm_head.dense.quant.activation_post_process.activation_post_process.eps', 'lm_head.dense.module.weight_fake_quant.activation_post_process.min_val', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.min_val', 'lm_head.dense.quant.activation_post_process.activation_post_process.max_val', 'lm_head.layer_norm.module.weight', 'lm_head.dense.module.weight_fake_quant.zero_point', 'lm_head.dense.quant.activation_post_process.activation_post_process.min_val', 'lm_head.decoder.module.weight_fake_quant.zero_point', 'lm_head.layer_norm.quant.activation_post_process.activation_post_process.eps', 'lm_head.layer_norm.quant.activation_post_process.fake_quant_enabled', 'lm_head.dense.quant.activation_post_process.scale', 'lm_head.dense.quant.activation_post_process.observer_enabled', 'lm_head.dense.module.weight_fake_quant.scale', 'lm_head.decoder.module.weight', 'lm_head.decoder.module.weight_fake_quant.activation_post_process.max_val', 'lm_head.decoder.quant.activation_post_process.activation_post_process.min_val', 'lm_head.dense.module.weight', 'lm_head.bias', 'lm_head.dense.module.weight_fake_quant.activation_post_process.max_val', 'lm_head.layer_norm.module.bias']\n",
      "2024-05-10 09:08:51 sparseml.pytorch.model_load.helpers INFO     Reloaded 1485 model params for SparseML Recipe from /root/sparseml/oneshot_output\n",
      "2024-05-10 09:08:51 sparseml.pytorch.model_load.helpers INFO     Loaded student from /root/sparseml/oneshot_output with 124647939 total params. Of those there are 85526784 prunable params which have 12.627738931467364 avg sparsity.\n",
      "2024-05-10 09:08:52 sparseml.pytorch.model_load.helpers INFO     sparse model detected, all sparsification info: {\"params_summary\": {\"total\": 124647939, \"sparse\": 10801638, \"sparsity_percent\": 8.665717288755172, \"prunable\": 85526784, \"prunable_sparse\": 10800099, \"prunable_sparsity_percent\": 12.627738931467364, \"quantizable\": 85610499, \"quantized\": 85610499, \"quantized_percent\": 100.0}, \"params_info\": {\"roberta.encoder.layer.0.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0401645228266716, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.03113640658557415, \"quantized\": true}, \"roberta.encoder.layer.0.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0321282297372818, \"quantized\": true}, \"roberta.encoder.layer.0.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0824008509516716, \"quantized\": true}, \"roberta.encoder.layer.0.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.04801347479224205, \"quantized\": true}, \"roberta.encoder.layer.0.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.14651447534561157, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0421329066157341, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0525377057492733, \"quantized\": true}, \"roberta.encoder.layer.1.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0338049978017807, \"quantized\": true}, \"roberta.encoder.layer.1.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0759921595454216, \"quantized\": true}, \"roberta.encoder.layer.1.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.055060915648937225, \"quantized\": true}, \"roberta.encoder.layer.1.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.23680199682712555, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0260637067258358, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0455305315554142, \"quantized\": true}, \"roberta.encoder.layer.2.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0333896204829216, \"quantized\": true}, \"roberta.encoder.layer.2.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0655924454331398, \"quantized\": true}, \"roberta.encoder.layer.2.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.044445887207984924, \"quantized\": true}, \"roberta.encoder.layer.2.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.16232426464557648, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02632988803088665, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0442318394780159, \"quantized\": true}, \"roberta.encoder.layer.3.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0382605642080307, \"quantized\": true}, \"roberta.encoder.layer.3.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0631120502948761, \"quantized\": true}, \"roberta.encoder.layer.3.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0517289899289608, \"quantized\": true}, \"roberta.encoder.layer.3.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1978318989276886, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0352528877556324, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0445454902946949, \"quantized\": true}, \"roberta.encoder.layer.4.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0441979318857193, \"quantized\": true}, \"roberta.encoder.layer.4.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0588141530752182, \"quantized\": true}, \"roberta.encoder.layer.4.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.060012392699718475, \"quantized\": true}, \"roberta.encoder.layer.4.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.2542177736759186, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02959696389734745, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0488976389169693, \"quantized\": true}, \"roberta.encoder.layer.5.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0362684465944767, \"quantized\": true}, \"roberta.encoder.layer.5.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0561184361577034, \"quantized\": true}, \"roberta.encoder.layer.5.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.07045449316501617, \"quantized\": true}, \"roberta.encoder.layer.5.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.29985299706459045, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0362124964594841, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510389544069767, \"quantized\": true}, \"roberta.encoder.layer.6.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0326470285654068, \"quantized\": true}, \"roberta.encoder.layer.6.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.052398681640625, \"quantized\": true}, \"roberta.encoder.layer.6.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.07209396362304688, \"quantized\": true}, \"roberta.encoder.layer.6.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31009674072265625, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0373602956533432, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0535990409553051, \"quantized\": true}, \"roberta.encoder.layer.7.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0319146066904068, \"quantized\": true}, \"roberta.encoder.layer.7.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0583224818110466, \"quantized\": true}, \"roberta.encoder.layer.7.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.08474943041801453, \"quantized\": true}, \"roberta.encoder.layer.7.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.4055476784706116, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.03656005859375, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510576032102108, \"quantized\": true}, \"roberta.encoder.layer.8.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.02870686911046505, \"quantized\": true}, \"roberta.encoder.layer.8.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0551791712641716, \"quantized\": true}, \"roberta.encoder.layer.8.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.08564291894435883, \"quantized\": true}, \"roberta.encoder.layer.8.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3597395122051239, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0463528111577034, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0510457344353199, \"quantized\": true}, \"roberta.encoder.layer.9.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0348409004509449, \"quantized\": true}, \"roberta.encoder.layer.9.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0517985038459301, \"quantized\": true}, \"roberta.encoder.layer.9.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1108008474111557, \"quantized\": true}, \"roberta.encoder.layer.9.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.31593406200408936, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0291103795170784, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0565422922372818, \"quantized\": true}, \"roberta.encoder.layer.10.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0291290283203125, \"quantized\": true}, \"roberta.encoder.layer.10.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0573645681142807, \"quantized\": true}, \"roberta.encoder.layer.10.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.0795491561293602, \"quantized\": true}, \"roberta.encoder.layer.10.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.3796229958534241, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.query.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0537075474858284, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.key.module.weight\": {\"numel\": 589824, \"sparsity\": 0.057281494140625, \"quantized\": true}, \"roberta.encoder.layer.11.attention.self.value.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0306854248046875, \"quantized\": true}, \"roberta.encoder.layer.11.attention.output.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0516120046377182, \"quantized\": true}, \"roberta.encoder.layer.11.intermediate.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.05897776409983635, \"quantized\": true}, \"roberta.encoder.layer.11.output.dense.module.weight\": {\"numel\": 2359296, \"sparsity\": 0.1474219411611557, \"quantized\": true}, \"classifier.dense.module.weight\": {\"numel\": 589824, \"sparsity\": 0.0, \"quantized\": true}, \"classifier.out_proj.module.weight\": {\"numel\": 2304, \"sparsity\": 0.0, \"quantized\": true}}}\n",
      "2024-05-10 09:08:52 sparseml.pytorch.model_load.helpers INFO     Reloaded model state after SparseML recipe structure modifications from /root/sparseml/oneshot_output\n",
      "2024-05-10 09:08:52 sparseml.pytorch.model_load.helpers INFO     Delayed load of model /root/sparseml/oneshot_output detected. Will print out model information once SparseML recipes have loaded\n",
      "2024-05-10 09:08:52 sparseml.export.export INFO     Creating data loader for the export...\n",
      "2024-05-10 09:08:52 sparseml.export.export INFO     Created additional items that will be used for the export: ['tokenizer', 'sequence_length', 'config', 'input_names']\n",
      "2024-05-10 09:08:52 sparseml.export.export INFO     Exporting model.onnx to ./oneshot_deployment...\n",
      "/root/sparseml/.venv/lib/python3.10/site-packages/sparseml/modifiers/quantization/utils/fake_quant_wrapper.py:43: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.fake_quant_enabled[0] == 0:\n",
      "2024-05-10 09:09:00 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 148 matches\n",
      "2024-05-10 09:09:01 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 228 matches\n",
      "2024-05-10 09:09:01 sparseml.exporters.transforms.onnx_transform INFO     [UnwrapBatchNorms] Transformed 0 matches\n",
      "2024-05-10 09:09:02 sparseml.exporters.transforms.onnx_transform INFO     [DeleteTrivialOnnxAdds] Transformed 0 matches\n",
      "2024-05-10 09:09:03 sparseml.exporters.transforms.onnx_transform INFO     [ConstantsToInitializers] Transformed 393 matches\n",
      "2024-05-10 09:09:03 sparseml.exporters.transforms.onnx_transform INFO     [FoldIdentityInitializers] Transformed 0 matches\n",
      "2024-05-10 09:09:04 sparseml.exporters.transforms.onnx_transform INFO     [InitializersToUint8] Transformed 67 matches\n",
      "2024-05-10 09:09:05 sparseml.exporters.transforms.onnx_transform INFO     [FlattenQParams] Transformed 0 matches\n",
      "2024-05-10 09:09:05 sparseml.exporters.transforms.onnx_transform INFO     [FoldConvDivBn] Transformed 0 matches\n",
      "2024-05-10 09:09:06 sparseml.exporters.transforms.onnx_transform INFO     [DeleteRepeatedQdq] Transformed 0 matches\n",
      "2024-05-10 09:09:06 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeQATEmbedding] Transformed 3 matches\n",
      "2024-05-10 09:09:07 sparseml.exporters.transforms.onnx_transform INFO     [PropagateEmbeddingQuantization] Transformed 0 matches\n",
      "2024-05-10 09:09:07 sparseml.exporters.transforms.onnx_transform INFO     [PropagateDequantThroughSplit] Transformed 0 matches\n",
      "2024-05-10 09:09:09 sparseml.exporters.transforms.onnx_transform INFO     [MatMulAddToMatMulIntegerAddCastMul] Transformed 72 matches\n",
      "2024-05-10 09:09:09 sparseml.exporters.transforms.onnx_transform INFO     [MatMulToMatMulIntegerCastMul] Transformed 0 matches\n",
      "2024-05-10 09:09:09 sparseml.exporters.transforms.onnx_transform INFO     [FoldReLUQuants] Transformed 0 matches\n",
      "2024-05-10 09:09:09 sparseml.exporters.transforms.onnx_transform INFO     [ConvToConvIntegerAddCastMul] Transformed 0 matches\n",
      "2024-05-10 09:09:10 sparseml.exporters.transforms.onnx_transform INFO     [GemmToQLinearMatMul] Transformed 0 matches\n",
      "2024-05-10 09:09:10 sparseml.exporters.transforms.onnx_transform INFO     [GemmToMatMulIntegerAddCastMul] Transformed 2 matches\n",
      "2024-05-10 09:09:12 sparseml.exporters.transforms.onnx_transform INFO     [QuantizeResiduals] Transformed 0 matches\n",
      "2024-05-10 09:09:12 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQConvWeights] Transformed 0 matches\n",
      "2024-05-10 09:09:13 sparseml.exporters.transforms.onnx_transform INFO     [RemoveDuplicateQuantizeOps] Transformed 0 matches\n",
      "2024-05-10 09:09:13 sparseml.export.export INFO     Successfully exported model.onnx to ./oneshot_deployment/model.onnx...\n",
      "2024-05-10 09:09:13 sparseml.export.export INFO     Creating deployment folder deployment at directory: ./oneshot_deployment...\n",
      "2024-05-10 09:09:13 sparseml.export.helpers WARNING  Optional file tokenizer.model not found in source path /root/sparseml/oneshot_output\n",
      "2024-05-10 09:09:13 sparseml.export.export INFO     Applying optimizations: all to the exported model...\n",
      "2024-05-10 09:09:13 sparseml.export.export INFO     Validating model structure...\n",
      "2024-05-10 09:09:13 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-outputs is missing.\n",
      "2024-05-10 09:09:13 sparseml.export.validators WARNING  File ./oneshot_deployment/deployment/tokenizer.model is missing.\n",
      "2024-05-10 09:09:13 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-inputs is missing.\n",
      "2024-05-10 09:09:13 sparseml.export.validators WARNING  File ./oneshot_deployment/sample-labels is missing.\n",
      "2024-05-10 09:09:13 sparseml.export.export INFO     Successfully exported model from:\n",
      "./oneshot_deployment\n",
      "to\n",
      "./oneshot_deployment/deployment\n",
      "for integration: transformers\n"
     ]
    }
   ],
   "source": [
    "### Export\n",
    "export(source_path = \"./oneshot_output\", target_path = \"./oneshot_deployment\", task=\"text-classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
