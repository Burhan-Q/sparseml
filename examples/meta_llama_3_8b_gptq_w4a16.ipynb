{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ with `meta-llama/Meta-Llama-3-8B-Instruct`\n",
    "\n",
    "In this tutorial, we will demonstrate how to apply GPTQ to Llama-3-8B instruct to quantize the weights to 4 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install\n",
    "\n",
    "Get started by installing SparseML via pip. You will need a GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sparseml[transformers]==1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load Model\n",
    "\n",
    "First, load a model from the Hugging Face hub (in this case `Meta-Llama-3-8B-Instruct`) using `SparseAutoModelForCausalLM`.\n",
    "\n",
    "* `SparseAutoModelForCausalLM` is a wrapper around `AutoModelForCausalLM`, with some added utilities for saving and loading quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sparseml.transformers import SparseAutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "model = SparseAutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Dataset\n",
    "\n",
    "Next, load a dataset for calibrating the model. \n",
    "\n",
    "Best practices for calibration data:\n",
    "* Apply the model's chat template to the sample data\n",
    "* Use at least 512 samples. 1024 samples can improve the results sometimes\n",
    "* Use at least 2048 sequence length. 4096 can improve the results sometimes\n",
    "* Select a diverse, high quality dataset (ideally that is adapted to your use case)\n",
    "\n",
    "In this case, we will use the [`HuggingFaceH4/ultrachat_200k` dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k), which contains multi-turn conversations and is generally a good choice for chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 512\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "# Dataset should have \"text\" field with the data you want to use to calibrate\n",
    "ds = ds.map(lambda batch: {\n",
    "    \"text\": tokenizer.apply_chat_template(batch[\"messages\"], tokenize=False)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Recipe\n",
    "\n",
    "Next, we make a recipe to specify the quantization algorithm to apply. In this case, we will use the `GPTQModifier`, which quantizes the weights of the model using GPTQ.\n",
    "\n",
    "We will target all linear layers except the lm-head with:\n",
    "- 4 bits \n",
    "- symmetric quantization\n",
    "- groups of 128\n",
    "\n",
    "This scheme is generally a good choice for making accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe = \"\"\"\n",
    "quant_stage:\n",
    "    quant_modifiers:\n",
    "        GPTQModifier:\n",
    "            sequential_update: false\n",
    "            ignore: [\"lm_head\"]\n",
    "            config_groups:\n",
    "                group_0:\n",
    "                    weights:\n",
    "                        num_bits: 4\n",
    "                        type: \"int\"\n",
    "                        symmetric: true\n",
    "                        strategy: \"group\"\n",
    "                        group_size: 128\n",
    "                    targets: [\"Linear\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Apply The Algorithm\n",
    "\n",
    "After making the recipe, we can apply the quantization algorithm using the `oneshot` function.\n",
    "\n",
    "> WARNING: You will need about 60GB of GPU RAM to run the below. To reduce memory consumption at the expense of speed, set `sequential_update: true` in your recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparseml.transformers import oneshot\n",
    "\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm generations of the quantized model look sane\n",
    "input_ids = tokenizer(\"Hello my name is\", return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Serialize the model\n",
    "\n",
    "Save the model using `save_pretrained` using `save_compressed=True`. This will save the weights in a compressed format, compatible for loading with vLLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"llama-3-gptq-4-bit\"\n",
    "model.save_pretrained(OUTPUT_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparseml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
